<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><meta name="google-site-verification" content="-l60HPLrjDNbr3Ni1wLsNkiKiCWUAmxiC_ObB8vNMF0"><meta name="msvalidate.01" content="AF3396A141E1B198CA1BE76915B3969F"><meta name="baidu-site-verification" content="code-OBKi1CbRLy"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"vivounicorn.github.io",root:"/",scheme:"Muse",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="4. 最优化原理 4.1 泰勒定理 满足一定条件的函数可以通过泰勒展开式对其做近似： 4.1.1 泰勒展开式 泰勒展开式原理如下，主要采用分部积分推导： \[ \begin{align*} f(x+\Delta x)&amp;&#x3D;f(x)+\int_{x}^{x+\Delta x}\nabla f(t)dt\\ &amp;&#x3D;f(x)+((x+\Delta x) \nabla f(x+\Delta x"><meta property="og:type" content="article"><meta property="og:title" content="机器学习与人工智能技术分享-第四章 最优化原理"><meta property="og:url" content="https://vivounicorn.github.io/article/ad2261a2.html"><meta property="og:site_name" content="业精于勤，荒于嬉；行成于思，毁于随。"><meta property="og:description" content="4. 最优化原理 4.1 泰勒定理 满足一定条件的函数可以通过泰勒展开式对其做近似： 4.1.1 泰勒展开式 泰勒展开式原理如下，主要采用分部积分推导： \[ \begin{align*} f(x+\Delta x)&amp;&#x3D;f(x)+\int_{x}^{x+\Delta x}\nabla f(t)dt\\ &amp;&#x3D;f(x)+((x+\Delta x) \nabla f(x+\Delta x"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_4/image_1fbjaso1ni41lk7nofbbnba9m.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_4/image_1fbjbvq0t14gvsisstr1uufcpm13.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_4/image_1fbjcq0mg8c11h511kmivm11far1g.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_4/bgd.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_4/sgd.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_4/msgd.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_4/nesterov_update_vector.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_4/adadelta.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_4/compare.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_4/contours_evaluation_optimizers.gif"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_4/image_1apn008f514vrvrf12mmjo31qrrm.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_4/allreduce1.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_4/image_1app04foe100116b7vf6rblrqn9.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_4/image_1apo0jjdvq6v8h011kq1useljp15.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_4/image_1appb6oak1iu3eptmi51dvhjmhm.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_4/image_1atg85ts8kff2erd551fss2s39.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_4/image_1apq85tf71ff74affhh8uh16na2a.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_4/image_1apq8gg70htn1p57gv9qbj6k79.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_4/image_1apq53v8s1567rgu1t1v14eaqjb13.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_4/image_1apq7j7prao3qfa1k6es0l83o1g.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_4/image_1apsoja5s1iao1v9b22c17ie18hp9.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_4/image_1fbjjdgn91r7v1p7a1o1c1o2f100b1t.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_4/image_1fbjjf0o2mbd108vl2p1vhh1v5f2a.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_4/image_1fbjjijog11ju16bmmdk1rcg134d2n.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_4/image_1fbjjji7k12qhvta1gud16d21iv734.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_4/image_1fbjjqkt419971tob174m1lmo1kih3h.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_4/image_1fbjjr9sra7uhbh1hukbl01vtp3u.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_4/image_1fbjjropb8ee7tf1hnck4doaf4b.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_4/image_1fbjjubf4103g1son9t5qe61q394r.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_4/image_1fbjk825e1vldbo71as012u1gsi5o.png"><meta property="article:published_time" content="2021-09-05T08:55:13.000Z"><meta property="article:modified_time" content="2021-12-14T02:10:46.752Z"><meta property="article:author" content="张磊"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="最优化"><meta property="article:tag" content="第四章"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://vivounicorn.github.io/images/ai_chapter_4/image_1fbjaso1ni41lk7nofbbnba9m.png"><link rel="canonical" href="https://vivounicorn.github.io/article/ad2261a2.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>机器学习与人工智能技术分享-第四章 最优化原理 | 业精于勤，荒于嬉；行成于思，毁于随。</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">业精于勤，荒于嬉；行成于思，毁于随。</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">花晨月夕</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>站点地图</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div><div><img itemprop="image" src="https://vivounicorn.github.io/images/background.jpg"></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://vivounicorn.github.io/article/ad2261a2.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="https://vivounicorn.github.io/images/wali.png"><meta itemprop="name" content="张磊"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="业精于勤，荒于嬉；行成于思，毁于随。"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">机器学习与人工智能技术分享-第四章 最优化原理</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-09-05 16:55:13" itemprop="dateCreated datePublished" datetime="2021-09-05T16:55:13+08:00">2021-09-05</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2021-12-14 10:10:46" itemprop="dateModified" datetime="2021-12-14T10:10:46+08:00">2021-12-14</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a> </span></span><span id="/article/ad2261a2.html" class="post-meta-item leancloud_visitors" data-flag-title="机器学习与人工智能技术分享-第四章 最优化原理" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/article/ad2261a2.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/article/ad2261a2.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>31k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>28 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="最优化原理">4. 最优化原理</h1><h2 id="泰勒定理">4.1 泰勒定理</h2><p>满足一定条件的函数可以通过泰勒展开式对其做近似：</p><h3 id="泰勒展开式">4.1.1 泰勒展开式</h3><p>泰勒展开式原理如下，主要采用分部积分推导： <span class="math display">\[ \begin{align*} f(x+\Delta x)&amp;=f(x)+\int_{x}^{x+\Delta x}\nabla f(t)dt\\ &amp;=f(x)+((x+\Delta x) \nabla f(x+\Delta x)-xf(x))-\int_{x}^{x+\Delta x}t\nabla^2f(t)dt\\ &amp;=f(x)+(x+\Delta x)(\nabla f(x)+\int_{x}^{x+\Delta x}\nabla^2f(t)dt)-x\nabla f(x)-\int_{x}^{x+\Delta x}t\nabla^2f(t)dt\\ &amp;=f(x)+\nabla f(x)\Delta x +\int_{x}^{x+\Delta x}(x+\Delta x-t) \nabla^2f(t)dt\\ &amp;......\\ &amp;=f(x)+\nabla f(x)\Delta x+\frac{1}{2}\nabla^2 f(x)\Delta x^2+...\frac{1}{n!}\nabla^nf(x)\Delta x^n \\ &amp;+ \int_{x}^{x+\Delta x}\frac{\nabla^{n+1}f(t)}{n!}(x+\Delta x-t) dt \end{align*} \]</span></p><span id="more"></span><h3 id="泰勒中值定理">4.1.2 泰勒中值定理</h3><p>需要注意泰勒中值定理是一个严格的等式： <span class="math display">\[ \begin{array}{l} f(x+\Delta x)=f(x)+\nabla f(x)\Delta x+\frac{1}{2}\nabla^2 f(x)\Delta x^2 + ...\frac{1}{n!}\nabla^nf(x)\Delta x^n \\ +\frac{1}{(n+1)!}\nabla^{n+1}f(\xi)\Delta x^{n+1} ,\text{where } \xi \in(0,\Delta x) \end{array} \]</span></p><h2 id="梯度下降法">4.2 梯度下降法</h2><h3 id="基本原理">4.2.1 基本原理</h3><p>梯度下降是一种简单、好用、经典的使用一阶信息的最优化方法（意味着相对低廉的计算成本），其基本原理可以想象为一个下山问题，当下降方向与梯度方向一致时，目标函数的方向导数最大，即此时目标函数在当前起点位置的下降速度最快。</p><p><strong>1、一个小例子</strong></p>假设有单变量实值函数<span class="math inline">\(y=f(x)\)</span>，其图形如下：<center><img src="https://vivounicorn.github.io/images/ai_chapter_4/image_1fbjaso1ni41lk7nofbbnba9m.png" width="300"></center><p>实值函数y=f(x)在点x_0的导数f'(x)的意义是该函数在x=x_0 处的瞬时变化率，即：</p><p><span class="math inline">\(f&#39;(x_0)=\lim_{\Delta x-&gt;0}\frac{f(x_0+\Delta x)-f(x_0)}{\Delta x}\)</span></p><p>在自变量x发生微小变化时，目标函数值的变化可以这么描述：</p><p><span class="math inline">\(dy=f&#39;(x)dx\)</span></p><p>针对上图有以下三种情况：</p><p>(1)、<span class="math inline">\(x_1\)</span>点位置，此时<span class="math inline">\(f&#39;(x_1)&gt;0\)</span>，在<span class="math inline">\(x_1\)</span> 点做微小正向变化：<span class="math inline">\(dx&gt;0\)</span>，显然有<span class="math inline">\(dy&gt;0\)</span>，这说明在<span class="math inline">\(x_1\)</span>点往<span class="math inline">\(x\)</span>轴正向有可以使目标函数<span class="math inline">\(y=f(x)\)</span>值增大点存在；</p><p>(2)、<span class="math inline">\(x_2\)</span>点位置，此时<span class="math inline">\(f&#39;(x_2)&lt;0\)</span>，在<span class="math inline">\(x_2\)</span> 点做微小负向变化：<span class="math inline">\(dx&lt;0\)</span>，显然有<span class="math inline">\(dy&gt;0\)</span>，这说明在<span class="math inline">\(x_2\)</span>点往<span class="math inline">\(x\)</span>轴负向有可以使目标函数<span class="math inline">\(y=f(x)\)</span>值增大点存在；</p><p>(3)、<span class="math inline">\(x_0\)</span>点位置，此时<span class="math inline">\(f&#39;(x_0)=0\)</span>，不管在<span class="math inline">\(x_0\)</span> 点做微小负向变化还是正向变化都有<span class="math inline">\(dy=0\)</span>，这说明在<span class="math inline">\(x_0\)</span>点是一个最优解。</p><p>实际上，在一维情况下目标函数的梯度就是<span class="math inline">\(f&#39;(x)\)</span>，它表明了目标函数值变化方向。</p><p><strong>2、梯度与方向导数</strong></p><ul><li><p>方向导数 以二元函数：<span class="math inline">\(y=f(x_0,x_1)\)</span>为例，设它在点<span class="math inline">\(p(x_0,x_1)\)</span>的某个邻域<span class="math inline">\(U(p)\)</span>内有定义，以点<span class="math inline">\(p(x_0,x_1)\)</span>出发引射线l，<span class="math inline">\(p&#39;(x_0+\Delta x_0,x_1+\Delta x_1)\)</span>为<span class="math inline">\(l\)</span>上的且在邻域<span class="math inline">\(U(p)\)</span>内的任意点，则方向导数的定义为： <span class="math display">\[\frac{\partial f}{\partial l} = \lim_{\rho-&gt;0^+} {\frac{f(x_0+\Delta x_0,x_1+\Delta x_1)-f(x_0,x_1)}{\rho}} \]</span> 其中<span class="math inline">\(\rho\)</span>表示<span class="math inline">\(p\)</span>和<span class="math inline">\(p&#39;\)</span>两点之间的欧氏距离：<span class="math inline">\(\rho = \sqrt{(\Delta x_0)^2 + (\Delta x_1)^2}\)</span> 从这个式子可以看到：方向导数与某个方向<span class="math inline">\(l\)</span>有联系、方向导数是个极限值、方向导数与偏导数似乎有联系。 实际上，如果<span class="math inline">\(y=f(x_0,x_1)\)</span>在点<span class="math inline">\(p(x_0,x_1)\)</span>可微，则： <span class="math display">\[\frac{\partial f}{\partial l} = \frac{\partial f}{\partial x_0} \cos\alpha +\frac{\partial f}{\partial x_1} \cos\beta \]</span> 其中<span class="math inline">\(\alpha\)</span>和<span class="math inline">\(\beta\)</span>分别是两个维度上的方向角 这里需要注意的一个细节是：沿某个维度的方向导数存在时，其偏导数不一定存在，原因就是方向导数只要求半边极限存在(<span class="math inline">\(\rho-&gt;{0^+}\)</span>)，而偏导数则要求双边都存在。</p></li><li>梯度 把方向导数变换一下形式： <span class="math display">\[\frac{\partial f}{\partial l} = \frac{\partial f}{\partial x_0} \cos\alpha +\frac{\partial f}{\partial x_1} \cos\beta =(\frac{\partial f}{\partial x_0}, \frac{\partial f }{\partial x_1}) \cdot (\cos\alpha, \cos\beta)\]</span> 函数<span class="math inline">\(y=f(x_0,x_1)\)</span>在点<span class="math inline">\(p(x_0,x_1)\)</span>的梯度就被定义为向量： <span class="math display">\[gradf(x_0,x_1)=\frac{\partial f}{\partial x_0}\cdot i + \frac{\partial f }{\partial x_1}\cdot j\]</span> 与射线l同方向的单位向量被定义为： <span class="math inline">\(e=\cos \alpha \cdot i+ \cos \beta\cdot j\)</span> 于是方向导数变成了： <span class="math display">\[\frac{\partial f}{\partial l} = gradf(x_0,x_1)\cdot e=|gradf(x_0,x_1)|\cdot \cos&lt;gradf(x_0,x_1), e&gt;\]</span> 我的理解是：方向导数描述了由各个维度方向形成的合力方向上函数变化的程度，当这个合力方向与梯度向量的方向相同时，函数变化的最剧烈，我想这就是为什么在梯度上升算法或者梯度下降算法中选择梯度方向或者负梯度方向的原因吧。换句话说就是：函数在某点的梯度是这样一个向量，它的方向与取得最大方向导数的方向一致，而它的模为方向导数的最大值。<center><img src="https://vivounicorn.github.io/images/ai_chapter_4/image_1fbjbvq0t14gvsisstr1uufcpm13.png" width="300"></center><center>某个函数和它的等高线，图中标出了a点的梯度上升方向</center></li></ul><p><strong>3、多维无约束问题</strong></p><p>将开篇的那个小例子扩展到多维的情况，目标函数值将会成为一个向量，向任意个维度方向做微小变动都将对目标函数值产生影响，假设有n个维度，可以用下面的式子描述：</p><p><span class="math display">\[dy=(dy_0,....,dy_n)=grad f\cdot (dx_0,...,dx_n)^T=gradf\cdot dx=|grad f|\cdot |dx|\cdot \cos&lt;grad f , dx&gt;\]</span></p><p>令<span class="math inline">\(\alpha = \angle &lt;grad f , dx&gt;\)</span></p><p>(1)、当<span class="math inline">\(0\leq \alpha&lt;\frac{\pi}{2}\)</span>，此时<span class="math inline">\(dy&gt;0\)</span>，因此可以从点<span class="math inline">\(x\)</span>移动使得目标函数值增加；</p><p>(2)、当<span class="math inline">\(\frac{\pi}{2}&lt; \alpha\leq \pi\)</span>，此时<span class="math inline">\(dy&lt;0\)</span>，因此可以从点<span class="math inline">\(x\)</span>移动使得目标函数值减少；</p><p>(3)、当 <span class="math inline">\(\alpha=\frac{\pi}{2}\)</span>，梯度向量和<span class="math inline">\(dx\)</span>正交（任一向量为0也视为正交），不管从点<span class="math inline">\(x\)</span>怎样移动都找不到使目标函数值发生变化的点，于是x点就是目标函数的最优解。 由于<span class="math inline">\(dx\)</span>可以是任意方向向量，只要点x的梯度向量不为零，从点x出发总可以找到一个变化方向使得目标函数值向我们希望的方向变化（比如就找梯度方向，此时能引起目标函数值最剧烈地变化），理论上当最优解<span class="math inline">\(x^*\)</span>出现时就一定有<span class="math inline">\(gradf (x^*)=0\)</span>（实际上允许以某个误差<span class="math inline">\(\epsilon\)</span>结束），比如，对于梯度下降算法，当<span class="math inline">\(gradf (x^*)=0\)</span>时迭代结束，此时的<span class="math inline">\(x^*\)</span>为最优解（可能是全局最优解也可能是局部最优解） <span class="math display">\[x_{n+1}=x_n - \alpha \cdot gradf(x_n)\]</span></p><p>总结来说，基于梯度的优化算法通常有两个核心元素：搜索方向和搜索步长，并且一般都会和泰勒定理有某种联系，从泰勒中值定理可以得到下面的等式：</p><p><span class="math display">\[ \begin{align*} f(x_{n+1})&amp;=f(x_n)+\nabla f(x_n)(x_{n+1}-x_n)+\frac{1}{2}\nabla^2 f(\xi)(x_{n+1}-x_n)^2\\ &amp;~~~~~~~~~~~~~~~\text{where } \xi \in(x_n,x_{n+1})\\ \because &amp; \nabla f(x_{n+1})=g_n+\nabla^2\\ &amp;f(\xi)(x_{n+1}-x_n)=0\\ \therefore &amp; x_{n+1}=x_n-\underbrace{\nabla^2 f(\xi)^{-1}}_{\eta :~\text{learning rate }} \underbrace{\nabla f(x_n)}_{ d:~\text{gradient}}\\ \end{align*} \]</span></p><p>抽象出迭代框架如下：</p><p><span class="math display">\[ \begin{align*} x_{n+1}&amp;=x_n-\eta \nabla f(x_n)\\ &amp;~~~~~~~~\text{ where $f(x)$ is objective function.}\\ &amp;or\\ x_{n+1}&amp;=x_n-\Delta x_n\\ \Delta x_n&amp;=\eta \nabla f(x_n) \end{align*} \]</span></p><h3 id="拉格朗日乘数法和kkt条件">4.2.2 拉格朗日乘数法和KKT条件</h3><p>假设目标函数和约束在某点可微，用符号<span class="math inline">\(\nabla f\)</span>代替符号<span class="math inline">\(grad f\)</span>。</p><p><strong>1、等式约束</strong></p><span class="math display">\[\begin{eqnarray*} &amp;&amp;\min f(x) \\ &amp;&amp;s.t. h(x)=0, \quad \end{eqnarray*}\]</span><center><img src="https://vivounicorn.github.io/images/ai_chapter_4/image_1fbjcq0mg8c11h511kmivm11far1g.png" width="300"></center><p>在约束条件的作用下，与点<span class="math inline">\(x\)</span>（它是个向量）可移动方向相关的向量<span class="math inline">\(dx\)</span>就不像无约束问题那样随便往哪个方向都能移动了，此时<span class="math inline">\(dx\)</span>只能沿着约束曲线移动，例如，在<span class="math inline">\(x1\)</span>、<span class="math inline">\(x2\)</span>处，<span class="math inline">\(\nabla f(x)\)</span>和<span class="math inline">\(dx\)</span>不正交，说明还有使目标函数值更小的等高线存在，所以点<span class="math inline">\(x\)</span>还有移动的余地，当移动到<span class="math inline">\(x0\)</span>位置时<span class="math inline">\(\nabla f(x)\)</span>和<span class="math inline">\(dx\)</span>正交，得到最优解<span class="math inline">\(x0\)</span>。那么在最优解处<span class="math inline">\(\nabla f(x)\)</span>和约束有什么关系呢？因为此时<span class="math inline">\(h(x)=0\)</span>，<span class="math inline">\(\nabla h(x) \cdot dx=0\)</span>，显然此时有<span class="math inline">\(\lambda \cdot \nabla h(x)=\nabla f(x)\)</span>（其中<span class="math inline">\(\lambda\)</span>是常数），也就是说约束的梯度向量与目标函数的梯度向量在最优解处一定平行。 想到求解此类优化问题时最常用的方法——拉格朗日乘数法，先要构造拉格朗日函数：</p><p><span class="math display">\[L(x,\lambda) = f(x) - \lambda h(x) \]</span> 其中<span class="math inline">\(\lambda \geq0\)</span>，是常数</p><p>为什么求解拉格朗日函数得到的最优解就是原问题的最优解呢？</p><p><span class="math display">\[\frac{\partial L(x,\lambda)}{\partial x}=\nabla f(x)-\lambda \nabla h(x) \frac{\partial L(x,\lambda)}{\partial \lambda}= h(x)\]</span></p><p>假设<span class="math inline">\(x^*\)</span>、<span class="math inline">\(\lambda^*\)</span>为<span class="math inline">\(L(x,\lambda)\)</span>的最优解，那么就需要满足：</p><p><span class="math display">\[ \begin{eqnarray*} \\ \nabla f(x^*)-\lambda^* \nabla h(x^*)&amp;=&amp; 0\\ \\ h(x^*)&amp;=&amp;0\\ \end{eqnarray*} \]</span></p><p>第一个式子印证了约束的梯度向量与目标函数的梯度向量在最优解处一定平行，第二个式子就是等式约束本身。</p><p>于是：</p><p><span class="math display">\[ \begin{eqnarray*} &amp;&amp;L(x,\lambda) &amp;\geq &amp;L(x^*,\lambda^*)\\ \Rightarrow&amp;&amp; f(x)-\lambda h(x) &amp;\geq &amp;f(x^*)-\lambda^* h(x^*) \\ \Rightarrow &amp;&amp;f(x) &amp;\geq &amp; f(x^*) \end{eqnarray*} \]</span></p><p><strong>2、不等式约束</strong></p><p>实际情况中，约束条件可能是等式约束也可能是不等式约束或者同时包含这两种约束，下面描述为更一般地情况：</p><p><span class="math display">\[ \begin{eqnarray*} &amp; \min &amp;f(x)\\ &amp; s.t.&amp; h_i(x)=0 \quad (i=0 ... n)\\ &amp;&amp;g_j(x) \leq 0 \quad (j=0...m) \end{eqnarray*} \]</span></p><p>依然使用拉格朗日乘数法，构造拉格朗日函数：</p><p><span class="math display">\[L(x,\alpha ,\beta) = f(x) + \sum\limits_{i=0}^n \alpha_i \cdot h_i(x) + \sum\limits_{j=0}^m \beta_j\cdot g_j(x) \]</span> 其中<span class="math inline">\(\alpha_i \geq 0\)</span>且<span class="math inline">\(\beta_j \geq 0\)</span></p><p>在这里不得不说一下Fritz John 定理了，整个证明就不写了（用局部极小必要条件定理、Gordan 引理可以证明）。</p><p><strong>定理1：</strong></p><p>依然假设<span class="math inline">\(x^*\)</span>为上述问题的极小值点，问题中涉及到的各个函数一阶偏导都存在，则存在不全为零的_i使得下组条件成立：</p><p><span class="math display">\[ \lambda_0 \nabla f(x^*) + \sum\limits_{i=0}^n \lambda_i \cdot \nabla h_i(x^*) + \sum\limits_{j=0}^m \lambda_j\cdot \nabla g_j(x^*)=0 \lambda_j \cdot g_j(x^*) = 0 ,j=0,...m \lambda_j \geq 0,j=0,...m\]</span></p><p>这个定理第一项的形式类似于条件极值必要条件的形式，如果<span class="math inline">\(\lambda_0=0\)</span>则有效约束 <span class="math inline">\(\nabla g_j(x)\)</span>会出现正线性相关，由Gordan 引理知道此时将存在可行方向，就是<span class="math inline">\(x^*\)</span>将不是原问题的极值点，因此令 <span class="math inline">\(\nabla g_j(x)\)</span>则线性无关则<span class="math inline">\(\lambda_0&gt;0\)</span> 。</p><p><span class="math inline">\(\lambda_j \cdot g_j(x^*) = 0 ,j=1,...m\)</span>这个条件又叫互不松弛条件（Complementary Slackness），SVM里的支持向量就是从这个条件得来的。</p><p>由Fritz John 定理可知 <span class="math inline">\(\nabla g_j(x)\)</span>线性无关则<span class="math inline">\(\lambda_0&gt;0\)</span> ，让每一个拉格朗日乘子除以<span class="math inline">\(\lambda_0\)</span>，即<span class="math inline">\(\mu_i=\lambda_i/\lambda_0\)</span>，得到下面这组原问题在点<span class="math inline">\(x^*\)</span>处取得极小值一阶必要条件。</p><p><strong>定理2：</strong></p><p>假设<span class="math inline">\(x^*\)</span>为上述问题的极小值点，问题中涉及到的各个函数一阶偏导都存在，有效约束 <span class="math inline">\(\nabla g_j(x)\)</span>线性无关，则下组条件成立：</p><p><span class="math display">\[\frac{\partial L(x,\mu_i,\mu_j)}{\partial x} =\nabla f(x^*) + \sum\limits_{i=0}^n \mu^*_i \cdot \nabla h_i(x^*) + \sum\limits_{j=0}^m \mu^*_j\cdot \nabla g_j(x^*)=0 \\ \mu_j^* \cdot g_j(x^*) = 0 ,j=0,...m \\ \mu^*_j \geq 0,j=0,...m \\ h_i(x^*)=0,i=0,..,n \\ g_j(x^*) \leq 0,j=0,...m\]</span></p><p>这组条件就是Karush-Kuhn-Tucker条件，满足KKT条件的点就是KKT点，需要注意的是KKT条件是必要条件（当然在某些情况下会升级为充要条件，比如凸优化问题）。 由此也可以想到求解SVM最大分类间隔器时，不管是解决原问题还是解决对偶问题，不管是用SMO方法或其它方法，优化的过程就是找到并优化违反KKT条件的最合适的乘子。 KKT条件与对偶理论有密切的关系，依然是解决下面这个问题：</p><p><span class="math display">\[ \begin{eqnarray*} &amp; \min &amp;f(x)\\ &amp; s.t.&amp; h_i(x)=0 \quad (i=0 ... n)\\ &amp;&amp;g_j(x) \leq 0 \quad (j=0...m) \end{eqnarray*} \]</span></p><p>构造拉格朗日函数：</p><p><span class="math display">\[ L(x,\alpha ,\beta) = f(x) + \sum\limits_{i=0}^n \alpha_i \cdot h_i(x) + \sum\limits_{j=0}^m \beta_j\cdot g_j(x)\]</span> 其中<span class="math inline">\(\alpha_i \geq 0\)</span>且<span class="math inline">\(\beta_j \geq 0\)</span>，它们都是拉格朗日乘子</p><p>令<span class="math inline">\(O_p(x)=\max\limits_{\alpha ,\beta} L(x,\alpha,\beta)\)</span>，原问题可以表示为下面这个形式：</p><p><span class="math display">\[ O_p(x)= \left\{ \begin{array}{c} &amp;f(x)&amp; if \quad x \quad satisfies \quad primal \quad constraints&amp;\\ &amp; \infty&amp; otherwise.&amp;\\ \end{array} \right. \]</span></p><p>这个式子比较容易理解，当x违反原问题约束条件时有：</p><p><span class="math display">\[ O_p(x)=\max\limits_{\alpha ,\beta} L(x,\alpha ,\beta) = \max\limits_{\alpha, \beta} f(x) + \sum\limits_{i=0}^n \alpha_i \cdot h_i(x) + \sum\limits_{j=0}^m \beta_j\cdot g_j(x)=\infty \]</span></p><p>于是原问题等价为下面这个问题：</p><p><span class="math display">\[ \min\limits_x O_p(x)=\min\limits_x \max\limits_{\alpha ,\beta} L(x,\alpha,\beta) \]</span> 它的最优解记为<span class="math inline">\(p^*\)</span></p><p>令<span class="math inline">\(O_d(\alpha,\beta)=\min\limits_{x} L(x,\alpha,\beta)\)</span>，则有以下形式：</p><p><span class="math display">\[\max\limits_{\alpha,\beta} O_d(\alpha,\beta)=\max\limits_{\alpha,\beta} \min\limits_{x} L(x,\alpha,\beta)\]</span></p><p>它的最优解记为<span class="math inline">\(d^*\)</span>，上面这两个形式很像，区别只在于<span class="math inline">\(min\)</span>和<span class="math inline">\(max\)</span>的顺序，实际上<span class="math inline">\(O_p(x)\)</span>和<span class="math inline">\(O_d(\alpha,\beta)\)</span>互为对偶问题。</p><p>因为<span class="math inline">\(\max\min \leq \min \max\)</span>，打个不太恰当的比喻，这就像瘦死的骆驼比马大，具体的证明就不写了；</p><p>所以<span class="math inline">\(d^* \leq p*\)</span>，这个就是弱对偶性，此时存在对偶间隙，它被定义为：<span class="math inline">\(gap=p^*-d^*\)</span>。</p><p>有弱对偶性就有强对偶性，它指的是在某些条件下有<span class="math inline">\(d^* = p*\)</span>，比如在以下条件下满足强对偶性：</p><p>目标函数和所有不等式约束函数是凸函数，等式约束函数是仿射函数(形如<span class="math inline">\(y=w^tx+b\)</span>)，且所有不等式约束都是严格的约束(大于或小于)。</p><p>KKT条件和强对偶性的关系是：</p><blockquote><p>KKT条件是强对偶性成立的必要条件，特别的，当原问题是凸优化问题时，KKT条件就是充要条件，强对偶性存在时KKT点既是原问题的解也是对偶问题的解，这个时候对偶间隙为0。</p></blockquote><h3 id="批量梯度下降">4.2.3 批量梯度下降</h3><p>按照上面等式，每次迭代，为计算梯度值都需要把所有样本扫描一遍，收敛曲线类似下图：</p><center><img src="https://vivounicorn.github.io/images/ai_chapter_4/bgd.png" width="500"></center><center>From <a target="_blank" rel="noopener" href="https://plot.ly/~michaeljancsy/419.embed">michaeljancsy</a></center><p>它的优点如下： - 模型学习与收敛过程通常是平滑的和稳定的； - 关于收敛条件有成熟完备的理论； - 针对它有不少利用二阶信息加速收敛的技术，例如conjugate gradient； - 对样本噪声点相对不敏感。</p><p>它的缺点如下：</p><ul><li>收敛速度慢；</li><li>对初始点敏感；</li><li>数据集的变化无法被学习到； captured.</li><li>不太适用于大规模数据。</li></ul><h3 id="随机梯度下降">4.2.4 随机梯度下降</h3><p>完全随机梯度下降（Stochastic Gradient Descent，可以想想这里为什么用Stochastic而不用Random？）每次选择一个样本更新权重，这样会带来一些噪声，但可能得到更好的解，试想很多问题都有大量局部最优解，传统批量梯度下降由于每次收集所有样后更新梯度值，当初始点确定后基本会落入到离它最近的洼地，而随机梯度下降由于噪声的引入会使它有高概率跳出当前洼地，选择变多从而可能找到更好的洼地。 收敛曲线类似下图：</p><center><img src="https://vivounicorn.github.io/images/ai_chapter_4/sgd.png" width="500"></center><center>From <a target="_blank" rel="noopener" href="https://plot.ly/~michaeljancsy/419.embed">michaeljancsy</a></center><p>完全随机梯度下降和批量梯度下降的优缺点几乎可以互换： - SGD的收敛速度更快； - SGD相对来说对初始点不敏感，容易找到更优方案； - SGD相对适合于大规模训练数据； - SGD能够捕捉到样本数据的变化； - 噪声样本可能导致权重波动从而造成无法收敛到局部最优解，步长的设计对其非常重要。</p><p>实践当中，很多样本都有类似的模式，所以SGD可以使用较少的抽样样本学习得到局部最优解，当然完全的批量学习和完全的随机学习都太极端，所以往往采用对两者的折中。</p><h3 id="小批量梯度下降">4.2.5 小批量梯度下降</h3><p>小批量梯度下降（Mini-batch Gradient Descent）是对SGD和BGD的折中，采用相对小的样本集学习，样本集大小随着学习过程保持或逐步加大，这样既能有随机带来的好处，又能使用二阶优化信息加速收敛，目前主流机器学习工具几乎都支持小批量学习。 小批量学习收敛过程如下：</p><center><img src="https://vivounicorn.github.io/images/ai_chapter_4/msgd.png" width="500"></center><center>From <a target="_blank" rel="noopener" href="https://plot.ly/~michaeljancsy/419.embed">michaeljancsy</a></center><p>梯度下降的另外一个任务是寻找合适的学习率，关于它有很多方法，介绍如下：</p><h3 id="牛顿法">4.2.6 牛顿法</h3><p>从泰勒展开式可以得到带最优步长的迭代式：</p><p><span class="math display">\[ \begin{array}{l} \Delta x_n=-\nabla^2 f(\xi)^{-1}\nabla f(x_n)\\ x_{n+1}=x_n+\Delta x_n \end{array} \]</span></p><p>但最优的学习率需要计算hessian矩阵，计算复杂度为<span class="math inline">\(O(n^3)\)</span>，所以这种方法不怎么用。</p><p><strong>为方便起见，使用 <span class="math inline">\(g_n\)</span> 代替 <span class="math inline">\(\nabla f(x_n)\)</span>.</strong></p><h3 id="momentum">4.2.7 Momentum</h3><p>SGD的一大缺点是<span class="math inline">\(\Delta x_n\)</span> 只和当前样本有关系，如果样本存在噪声则会导致权重波动，一种自然的想法就是即考虑历史梯度又考虑新样本的梯度：</p><p><span class="math display">\[ \begin{array}{l} \Delta x_n=\rho \Delta x_{n-1}-\eta g_n\\ x_{n+1}=x_n+\Delta x_n\\ \rho \text{ is usually set to a small value }\le 0.9 \end{array} \]</span></p><p>对动量的运行过程说明如下:</p><ul><li><p>在初始阶段，历史梯度信息会极大加速学习过程（比如n=2时）；</p></li><li><p>当准备穿越函数波谷时，差的学习率会导致权重向相反方向更新，于是学习过程会发生回退，这时有动量项的帮助则有可能越过这个波谷；</p></li><li><p>最后在梯度几乎为0的时候，动量项的存在又可能会使它跳出当前局部最小值，于是可能找到更好的最优值点。</p></li></ul>Nesterov accelerated gradient 是对动量法的一种改进，具体做法是：首先在之前的方向上迈一大步（棕色向量），之后计算在该点的梯度（红色向量），然后计算两个向量的和，得到的向量（绿色向量）作为最终优化方向。<center><img src="https://vivounicorn.github.io/images/ai_chapter_4/nesterov_update_vector.png" width="350"></center><center>From <a target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">G. Hinton's lecture 6c</a></center><p><span class="math display">\[ \begin{array}{l} \Delta x_n=\rho \Delta x_{n-1}-\eta \nabla f(x_n-\rho \Delta x_{n-1})\\ x_{n+1}=x_n+\Delta x_n\\ \rho \text{ is usually set to a small value }\le 0.9 \end{array} \]</span></p><h3 id="adagrad">4.2.8 AdaGrad</h3><p>Adagrad同样是基于梯度的方法，对每个参数给一个学习率，因此对于常出现的权重可以给个小的更新，而不常出现的则给予大的更新，于是对于稀疏数据集就很有效，这个方法常用于大规模神经网络，Google的FTRL-Proximal也使用了类似方法，可参见：<a target="_blank" rel="noopener" href="https://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf">Google Ad Click Prediction a View from the Trenches</a>和<a target="_blank" rel="noopener" href="http://www.jmlr.org/proceedings/papers/v15/mcmahan11b/mcmahan11b.pdf">Follow-the-Regularized-Leader and Mirror Descent: Equivalence Theorems and L1 Regularization</a>。</p><p><span class="math display">\[ \begin{array}{l} \Delta x_n=-\frac{\eta}{\sqrt{\sum_{i=1}^{n}g_i^2}+\beta}g_n\\ x_{n+1}=x_n+\Delta x_n\\ \beta \text{ is usually set to a small value.} \end{array} \]</span></p><p>这个方法有点像L2正则，其运作原理如下：</p><ul><li><p>在学习前期，梯度比较小regularizer比较大，所以梯度会被放大；</p></li><li><p>在学习后期，梯度比较大regularizer比较小，所以梯度会被缩小。</p></li></ul><p>但它的缺点是，当初始权重过大或经过几轮训练后会导致正则化太小，所以训练将被提前终止。</p><h3 id="adadelta">4.2.9 AdaDelta</h3><p>Adadelta是对Adagrad的改进，解决了以下短板：</p><ul><li><p>经过几轮的训练会导致正则化太小；</p></li><li><p>需要设置一个全局学习率；</p></li><li><p>当我们更新<span class="math inline">\(\Delta x\)</span>,等式左边和右边的单位不一致。</p></li></ul><p>对于第一个短板，设置一个窗口<span class="math inline">\(w\)</span>，仅使用最近几轮的梯度值去更新正则项但计算<span class="math inline">\(E[\nabla f(x)_{1\sim n}]\)</span> 太复杂，所以使用类似动量法的策略：</p><p><span class="math display">\[ \begin{array}{l} E[g^{2}]_{n}=\rho E[g^{2}]_{n-1}+(1-\rho )g_{n}^{2}\\ \Delta x_n=-\frac{\eta}{\sqrt{E[g^2]_n+\beta}}g_n\\ x_{n+1}=x_n+\Delta x_n\\ \beta \text{ is usually set to a small value.}\\ \rho \text{ is decay coefficient.} \end{array} \]</span></p><p>对其他短板，AdaDelta通过以下方法解决。</p><p>对SGD与Momentum(里面的注释是理解这个变换的关键)：</p><p><span class="math display">\[ \text{unit of }\Delta x \propto \text{unit of }g \propto \frac{\partial f}{\partial x} \propto \frac{1}{\text{unit of }x}\\ (\text{When $f$ is negative log likelihood function }\frac{\partial f}{\partial x} \propto \frac{\partial logf}{\partial x} =\frac{\frac{\partial f}{\partial x}}{f}) \]</span></p><p>对牛顿法： <span class="math display">\[ \text{unit of }\Delta x \propto \text{unit of }H^{-1}g\propto \frac{\frac{\partial f}{\partial x}}{\frac{\partial^{2}f}{\partial x^{2}}}\propto \frac{\frac{1}{x}}{\frac{1}{x}.\frac{1}{x}}\propto \text{unit of }x \]</span></p><p>所以二阶方法有正确的单位且快于一阶方法。</p><p>来源于Becker 和 LeCuns' 的hessian估计法：</p><p><span class="math display">\[ \begin{array}{l} \Delta x_{n} \approx \frac{\frac{\partial f}{\partial x}}{\frac{\partial^{2}f}{\partial x^{2}}}=\frac{1}{\frac{\partial^{2}f}{\partial x^{2}}}\cdot \frac{\partial f}{\partial x}=\frac{1}{\frac{\partial^{2}f}{\partial x^{2}}}\cdot g_n\\ define:RMS[g]_{n}=\sqrt{E[g^{2}]_{n}+\epsilon }\\ \because \frac{1}{\frac{\partial^{2}f}{\partial x^{2}}}=\frac{\Delta x}{\frac{\partial f}{\partial x}}\approx -\frac{RMS[\Delta x]_{n-1}}{RMS[g]_{n}}\\ \therefore \Delta x_{n}=-\frac{RMS[\Delta x]_{n-1}}{RMS[g_n]}\cdot g_n \end{array} \]</span></p>完整的算法描述如下：<center><img src="https://vivounicorn.github.io/images/ai_chapter_4/adadelta.png" width="400"></center><center>From <a target="_blank" rel="noopener" href="http://www.matthewzeiler.com/pubs/googleTR2012/googleTR2012.pdf">Zeiler</a></center><p>对以上算法的比较如下：</p><center><img src="https://vivounicorn.github.io/images/ai_chapter_4/compare.png" width="600"></center><center>From <a target="_blank" rel="noopener" href="https://cs.stanford.edu/people/karpathy/convnetjs/demo/trainers.html">Karpathy</a></center><center><img src="https://vivounicorn.github.io/images/ai_chapter_4/contours_evaluation_optimizers.gif" width="400"></center><center>From <a target="_blank" rel="noopener" href="http://sebastianruder.com/optimizing-gradient-descent/">SGD optimization on loss surface contours</a></center><h3 id="adam">4.2.10 Adam</h3><p>Adam是对Adadelta的改进，原理如下：</p><p><span class="math display">\[ \begin{align*} &amp;Recall: \Delta x_n=-\frac{\eta}{\sqrt{\sum_{i=1}^{n}g_i^2}+\epsilon}g_n\\ &amp;\text{Keeping an exponentially decaying average of past gradients:}\\ &amp;m_n = \beta_1 m_{n-1} + (1 - \beta_1) g_n\\ &amp;v_n = \beta_2 v_{n-1} + (1 - \beta_2) g_n^2\\ &amp;\because m_n=(1-\beta_1)\sum_{i=1}^n\beta_1^{n-i}g_i\\ &amp;~~~~v_n=(1-\beta_2)\sum_{i=1}^n\beta_2^{n-i}g_i^2\\ &amp;\therefore E[m_n]=E[(1-\beta_1)\sum_{i=1}^n\beta_1^{n-i}g_i]=E[g_n](1-\beta_1^n)\\ &amp;~~~~E[v_n]=E[g_n^2](1-\beta_2^n)\\ &amp;\\ &amp;\text{if set: }\\ &amp;\hat{m}_n = \dfrac{m_n}{1 - \beta^n_1}\\ &amp;\hat{v}_n = \dfrac{v_n}{1 - \beta^n_2}\\ &amp;\text{then we get the bias-corrected first and second moment estimates: }\\ &amp;E[\hat{m_n}]=m_n \text{ and } E[\hat{v_n}]=v_n\\ &amp;\\ &amp;\text{So the equation is:}\\ &amp;\Delta x_n=- \dfrac{\eta}{\sqrt{\hat{v}_n} + \epsilon} \hat{m}_n\\ &amp;x_{n+1} = x_{n} +\Delta x_n\\ &amp;\text{The authors propose default values of 0.9 for $\beta_1$, 0.999 for $\beta_2$, and $10^{-8}$ for $\epsilon$.} \end{align*} \]</span></p>算法伪代码如下：<center><img src="https://vivounicorn.github.io/images/ai_chapter_4/image_1apn008f514vrvrf12mmjo31qrrm.png" alt="image_1apn008f514vrvrf12mmjo31qrrm.png-222.1kB"></center><h2 id="并行sgd">4.3 并行SGD</h2><p>SGD相对简单并且被证明有较好的收敛性质和精度，所以自然而然就想到将其扩展到大规模数据集上，就像Hadoop/Spark的基本框架是MapReduce，并行机器学习的常见框架有两种： AllReduce 和 Parameter Server（PS）。</p><h3 id="allreduce">4.3.1 AllReduce</h3><p>AllReduce的思想来源于MPI，它可以被看做Reduce操作+Broadcast操作，例如：</p><center><img src="https://vivounicorn.github.io/images/ai_chapter_4/allreduce1.png" alt="allreduce1.png-15.8kB"></center><center>From <a target="_blank" rel="noopener" href="http://mpitutorial.com/tutorials/mpi-reduce-and-allreduce/">MPI Tutorials</a></center><p>其他AllReduce的拓扑结构如下：</p><center><img src="https://vivounicorn.github.io/images/ai_chapter_4/image_1app04foe100116b7vf6rblrqn9.png" alt="image_1app04foe100116b7vf6rblrqn9.png-74kB"></center><center>From <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1312.3020.pdf">Huasha Zhao &amp; John Canny</a></center>非常好的开源实现有<a target="_blank" rel="noopener" href="http://hunch.net/~jl/"><strong>John Langford</strong></a>的<a target="_blank" rel="noopener" href="https://github.com/JohnLangford/vowpal_wabbit/"><strong>vowpal wabbit</strong></a>和<a target="_blank" rel="noopener" href="http://homes.cs.washington.edu/~tqchen/"><strong>陈天奇</strong></a>的<a target="_blank" rel="noopener" href="https://github.com/dmlc/rabit"><strong>Rabit</strong></a>（轻量级、可容错）。并行计算的关键之一是如何在大规模数据集下计算目标函数的梯度值，AllReduce框架很适合这种任务，比如：vw通过构建一个二叉树来管理机器节点，其中一个节点会被当做master，其他节点作为slave，master管理着slave并定期接受它们的心跳，每个子节点的计算结果会被其父节点收集，到达根节点后累加并广播到其所有子节点，一个简单的例子如下：<center><img src="https://vivounicorn.github.io/images/ai_chapter_4/image_1apo0jjdvq6v8h011kq1useljp15.png" alt="image_1apo0jjdvq6v8h011kq1useljp15.png-11.6kB"></center><p>使用mini-batch的并行SGD算法伪代码如下：</p><p><span class="math display">\[ \begin{array}{l} \text{Algorithm 2: parallelizing SGD with mini-batch}\\ \text{If we have examples $X=\{x_0,x_1,...x_m\}$, machines $n$, threads of each machine $p$,} \\ \text{iterations T, batch b, local iterations t.}\\ \\ \textbf{Require}:\eta&gt;0,m&gt;1,n&gt;1,p&gt;1,T&gt;0,b&gt;1,t&gt;0\\ \quad1.\;\;\;\;\textbf{define}~H=\lfloor \frac{m}{n} \rfloor,h=\lfloor \frac{H}{p} \rfloor\\ \quad2.\;\;\;\;\text{randomly partition examples $X$,giving $H$ examples to each machine}\\ \quad3.\;\;\;\;w=0\\ \quad4.\;\;\;\;\textbf{for all }i = 1, ... T \text{ and $w$ not convenged} \textbf{ do}\\ \quad5.\;\;\quad\quad\textbf{for all }j \in \{1, ..., n\} \textbf{ parallel do}\\ \quad6.\;\;\quad\quad\quad\quad\text{randomly partition examples $h$ on machine j to each thread.}\\ \quad7.\;\;\quad\quad\quad\quad\textbf{for all }k \in \{1, ..., p\} \textbf{ parallel do}\\ \quad8.\;\;\quad\quad\quad\quad\quad\quad\text{randomly shuffle examples on thread k}\\ \quad9.\;\;\quad\quad\quad\quad\quad\quad w_0^k=0\\ \quad10.\quad\quad\quad\quad\quad\quad\textbf{for all }q = 1, ... t \textbf{ do}\\ \quad11.\quad\quad\quad\quad\quad\quad\quad\quad \textbf{choose }\text{examples $b$ uniformly at random}\\ \quad12.\quad\quad\quad\quad\quad\quad\quad\quad \textbf{update }w_{q+1}^k=w_{q}^k+\eta g_q^k\\ \quad13.\quad\quad\quad\quad\textbf{reduce }w_j=\frac{1}{p}\sum_{k=1}^p w_q^k\\ \quad14.\quad\quad\textbf{AllReduce (reduce $w_j$)}\\ \quad15.\quad\quad w=\frac{1}{n}\sum_{i=1}^n w_j^i\\ \quad16.\quad\quad\textbf{AllReduce (broadcast $w$)}\\ \quad17.\quad\textbf{return }w \end{array} \]</span></p><h3 id="参数服务器parameter-server">4.3.2 参数服务器(Parameter Server)</h3>参数服务器强调模型训练时参数的并行异步更新，最早是由Google的Jeffrey Dean团队提出，为了解决深度学习的参数学习问题，其基本思想是：将数据集划分为若干子数据集，每个子数据集所在的节点都运行着一个模型的副本，通过独立 部署的参数服务器组织模型的所有权重，其基本操作有：Fatching：每隔n次迭代，从参数服务器获取参数权重，Pushing：每隔m次迭代，向参数服务器推送本地梯度更新值，之后参数服务器会更新相关参数权重，其基本架构如下：<center><img src="https://vivounicorn.github.io/images/ai_chapter_4/image_1appb6oak1iu3eptmi51dvhjmhm.png" alt="image_1appb6oak1iu3eptmi51dvhjmhm.png-33.9kB"></center><center>From <a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf">Jeffrey Dean: Large Scale Distributed Deep Networks</a></center><p>每个模型的副本都是，为减少通信开销，每个模型副本在迭代<span class="math inline">\(n_{fetch}\)</span>次后向参数服务器请求参数跟新，反过来本地模型每迭代<span class="math inline">\(n_{push}\)</span>次后向参数服务器推送一次梯度更新值，当然，为了折中速度和效果，梯度的更新可以选择异步也可以是同。 参数服务器是一个非常好的机器学习框架，尤其在深度学习的应用场景中，有篇不错的文章： <a target="_blank" rel="noopener" href="http://chuansong.me/n/2161528">参数服务器——分布式机器学习的新杀器</a>。开源的实现中比较好的是<a target="_blank" rel="noopener" href="https://github.com/petuum/bosen"><strong>bosen</strong></a>项目和<a target="_blank" rel="noopener" href="http://www.cs.cmu.edu/~muli/"><strong>李沐</strong></a>的<a target="_blank" rel="noopener" href="https://github.com/dmlc/ps-lite"><strong>ps-lite</strong></a>（现已集成到<a target="_blank" rel="noopener" href="https://github.com/dmlc">DMLC</a>项目中）。</p><p>下面是一个Go语言实现的多线程版本的参数服务器（用于Ftrl算法的优化），源码位置：<a target="_blank" rel="noopener" href="https://github.com/vivounicorn/goline"><strong>Goline</strong></a>：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// data structure of ftrl solver.</span></span><br><span class="line"><span class="keyword">type</span> FtrlSolver <span class="keyword">struct</span> &#123;</span><br><span class="line">	Alpha   <span class="keyword">float64</span> <span class="string">`json:&quot;Alpha&quot;`</span></span><br><span class="line">	Beta    <span class="keyword">float64</span> <span class="string">`json:&quot;Beta&quot;`</span></span><br><span class="line">	L1      <span class="keyword">float64</span> <span class="string">`json:&quot;L1&quot;`</span></span><br><span class="line">	L2      <span class="keyword">float64</span> <span class="string">`json:&quot;L2&quot;`</span></span><br><span class="line">	Featnum <span class="keyword">int</span>     <span class="string">`json:&quot;Featnum&quot;`</span></span><br><span class="line">	Dropout <span class="keyword">float64</span> <span class="string">`json:&quot;Dropout&quot;`</span></span><br><span class="line">	N []<span class="keyword">float64</span> <span class="string">`json:&quot;N&quot;`</span></span><br><span class="line">	Z []<span class="keyword">float64</span> <span class="string">`json:&quot;Z&quot;`</span></span><br><span class="line">	Weights util.Pvector <span class="string">`json:&quot;Weights&quot;`</span></span><br><span class="line">	Init <span class="keyword">bool</span> <span class="string">`json:&quot;Init&quot;`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// data structure of parameter server.</span></span><br><span class="line"><span class="keyword">type</span> FtrlParamServer <span class="keyword">struct</span> &#123;</span><br><span class="line">	FtrlSolver</span><br><span class="line">	ParamGroupNum <span class="keyword">int</span></span><br><span class="line">	LockSlots     []sync.Mutex</span><br><span class="line">	log           log4go.Logger</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// fetch parameter group for update n and z value.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(fps *FtrlParamServer)</span> <span class="title">FetchParamGroup</span><span class="params">(n []<span class="keyword">float64</span>, z []<span class="keyword">float64</span>, group <span class="keyword">int</span>)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">	<span class="keyword">if</span> !fps.FtrlSolver.Init &#123;</span><br><span class="line">		fps.log.Error(<span class="string">&quot;[FtrlParamServer-FetchParamGroup] Initialize fast ftrl solver error.&quot;</span>)</span><br><span class="line">		<span class="keyword">return</span> errors.New(<span class="string">&quot;[FtrlParamServer-FetchParamGroup] Initialize fast ftrl solver error.&quot;</span>)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">var</span> start <span class="keyword">int</span> = group * ParamGroupSize</span><br><span class="line">	<span class="keyword">var</span> end <span class="keyword">int</span> = util.MinInt((group+<span class="number">1</span>)*ParamGroupSize, fps.FtrlSolver.Featnum)</span><br><span class="line"></span><br><span class="line">	fps.LockSlots[group].Lock()</span><br><span class="line">	<span class="keyword">for</span> i := start; i &lt; end; i++ &#123;</span><br><span class="line">		n[i] = fps.FtrlSolver.N[i]</span><br><span class="line">		z[i] = fps.FtrlSolver.Z[i]</span><br><span class="line">	&#125;</span><br><span class="line">	fps.LockSlots[group].Unlock()</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// fetch parameter from server.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(fps *FtrlParamServer)</span> <span class="title">FetchParam</span><span class="params">(n []<span class="keyword">float64</span>, z []<span class="keyword">float64</span>)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">	<span class="keyword">if</span> !fps.FtrlSolver.Init &#123;</span><br><span class="line">		fps.log.Error(<span class="string">&quot;[FtrlParamServer-FetchParam] Initialize fast ftrl solver error.&quot;</span>)</span><br><span class="line">		<span class="keyword">return</span> errors.New(<span class="string">&quot;[FtrlParamServer-FetchParam] Initialize fast ftrl solver error.&quot;</span>)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; fps.ParamGroupNum; i++ &#123;</span><br><span class="line">		err := fps.FetchParamGroup(n, z, i)</span><br><span class="line">		<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">			fps.log.Error(fmt.Sprintf(<span class="string">&quot;[FtrlParamServer-FetchParam] Initialize fast ftrl solver error.&quot;</span>, err.Error()))</span><br><span class="line">			<span class="keyword">return</span> errors.New(fmt.Sprintf(<span class="string">&quot;[FtrlParamServer-FetchParam] Initialize fast ftrl solver error.&quot;</span>, err.Error()))</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// push parameter group for upload n and z value.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(fps *FtrlParamServer)</span> <span class="title">PushParamGroup</span><span class="params">(n []<span class="keyword">float64</span>, z []<span class="keyword">float64</span>, group <span class="keyword">int</span>)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">	<span class="keyword">if</span> !fps.FtrlSolver.Init &#123;</span><br><span class="line">		fps.log.Error(<span class="string">&quot;[FtrlParamServer-PushParamGroup] Initialize fast ftrl solver error.&quot;</span>)</span><br><span class="line">		<span class="keyword">return</span> errors.New(<span class="string">&quot;[FtrlParamServer-PushParamGroup] Initialize fast ftrl solver error.&quot;</span>)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">var</span> start <span class="keyword">int</span> = group * ParamGroupSize</span><br><span class="line">	<span class="keyword">var</span> end <span class="keyword">int</span> = util.MinInt((group+<span class="number">1</span>)*ParamGroupSize, fps.FtrlSolver.Featnum)</span><br><span class="line"></span><br><span class="line">	fps.LockSlots[group].Lock()</span><br><span class="line">	<span class="keyword">for</span> i := start; i &lt; end; i++ &#123;</span><br><span class="line">		fps.FtrlSolver.N[i] += n[i]</span><br><span class="line">		fps.FtrlSolver.Z[i] += z[i]</span><br><span class="line">		n[i] = <span class="number">0</span></span><br><span class="line">		z[i] = <span class="number">0</span></span><br><span class="line">	&#125;</span><br><span class="line">	fps.LockSlots[group].Unlock()</span><br><span class="line">	<span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// push weight update to parameter server.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(fw *FtrlWorker)</span> <span class="title">PushParam</span><span class="params">(param_server *FtrlParamServer)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">	<span class="keyword">if</span> !fw.FtrlSolver.Init &#123;</span><br><span class="line">		fw.log.Error(<span class="string">&quot;[FtrlWorker-PushParam] Initialize fast ftrl solver error.&quot;</span>)</span><br><span class="line">		<span class="keyword">return</span> errors.New(<span class="string">&quot;[FtrlWorker-PushParam] Initialize fast ftrl solver error.&quot;</span>)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; fw.ParamGroupNum; i++ &#123;</span><br><span class="line">		err := param_server.PushParamGroup(fw.NUpdate, fw.ZUpdate, i)</span><br><span class="line">		<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">			fw.log.Error(fmt.Sprintf(<span class="string">&quot;[FtrlWorker-PushParam] Initialize fast ftrl solver error.&quot;</span>, err.Error()))</span><br><span class="line">			<span class="keyword">return</span> errors.New(fmt.Sprintf(<span class="string">&quot;[FtrlWorker-PushParam] Initialize fast ftrl solver error.&quot;</span>, err.Error()))</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// to do update for all weights.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(fw *FtrlWorker)</span> <span class="title">Update</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">	x util.Pvector,</span></span></span><br><span class="line"><span class="params"><span class="function">	y <span class="keyword">float64</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">	param_server *FtrlParamServer)</span> <span class="title">float64</span></span> &#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> !fw.FtrlSolver.Init &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="number">0.</span></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">var</span> weights util.Pvector = <span class="built_in">make</span>(util.Pvector, fw.FtrlSolver.Featnum)</span><br><span class="line">	<span class="keyword">var</span> gradients []<span class="keyword">float64</span> = <span class="built_in">make</span>([]<span class="keyword">float64</span>, fw.FtrlSolver.Featnum)</span><br><span class="line">	<span class="keyword">var</span> wTx <span class="keyword">float64</span> = <span class="number">0.</span></span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="built_in">len</span>(x); i++ &#123;</span><br><span class="line">		item := x[i]</span><br><span class="line">		<span class="keyword">if</span> util.UtilGreater(fw.FtrlSolver.Dropout, <span class="number">0.0</span>) &#123;</span><br><span class="line">			rand_prob := util.UniformDistribution()</span><br><span class="line">			<span class="keyword">if</span> rand_prob &lt; fw.FtrlSolver.Dropout &#123;</span><br><span class="line">				<span class="keyword">continue</span></span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">var</span> idx <span class="keyword">int</span> = item.Index</span><br><span class="line">		<span class="keyword">if</span> idx &gt;= fw.FtrlSolver.Featnum &#123;</span><br><span class="line">			<span class="keyword">continue</span></span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">var</span> val <span class="keyword">float64</span> = fw.FtrlSolver.GetWeight(idx)</span><br><span class="line">		weights = <span class="built_in">append</span>(weights, util.Pair&#123;idx, val&#125;)</span><br><span class="line">		gradients = <span class="built_in">append</span>(gradients, item.Value)</span><br><span class="line">		wTx += val * item.Value</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">var</span> pred <span class="keyword">float64</span> = util.Sigmoid(wTx)</span><br><span class="line">	<span class="keyword">var</span> grad <span class="keyword">float64</span> = pred - y</span><br><span class="line">	util.VectorMultiplies(gradients, grad)</span><br><span class="line">	<span class="keyword">for</span> k := <span class="number">0</span>; k &lt; <span class="built_in">len</span>(weights); k++ &#123;</span><br><span class="line">		<span class="keyword">var</span> i <span class="keyword">int</span> = weights[k].Index</span><br><span class="line">		<span class="keyword">var</span> g <span class="keyword">int</span> = i / ParamGroupSize</span><br><span class="line">		<span class="keyword">if</span> fw.ParamGroupStep[g]%fw.FetchStep == <span class="number">0</span> &#123;</span><br><span class="line">			param_server.FetchParamGroup(</span><br><span class="line">				fw.FtrlSolver.N,</span><br><span class="line">				fw.FtrlSolver.Z,</span><br><span class="line">				g)</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">var</span> w_i <span class="keyword">float64</span> = weights[k].Value</span><br><span class="line">		<span class="keyword">var</span> grad_i <span class="keyword">float64</span> = gradients[k]</span><br><span class="line">		<span class="keyword">var</span> sigma <span class="keyword">float64</span> = (math.Sqrt(fw.FtrlSolver.N[i]+grad_i*grad_i) - math.Sqrt(fw.FtrlSolver.N[i])) / fw.FtrlSolver.Alpha</span><br><span class="line">		fw.FtrlSolver.Z[i] += grad_i - sigma*w_i</span><br><span class="line">		fw.FtrlSolver.N[i] += grad_i * grad_i</span><br><span class="line">		fw.ZUpdate[i] += grad_i - sigma*w_i</span><br><span class="line">		fw.NUpdate[i] += grad_i * grad_i</span><br><span class="line">		<span class="keyword">if</span> fw.ParamGroupStep[g]%fw.PushStep == <span class="number">0</span> &#123;</span><br><span class="line">			param_server.PushParamGroup(fw.NUpdate, fw.ZUpdate, g)</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		fw.ParamGroupStep[g] += <span class="number">1</span></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> pred</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h2 id="二阶优化方法">4.4 二阶优化方法</h2><h3 id="概览">4.4.1 概览</h3><p>大部分的优化算法都是基于梯度的迭代方法，其迭代式来源为泰勒展开式，迭代的一般式为：</p><p><span class="math display">\[ \begin{array}{l} x_{k+1}=x_{k}+\alpha_kp_k \end{array} \]</span></p><p>其中<span class="math inline">\(\alpha_k&gt;0\)</span> 被称作步长，向量<span class="math inline">\(p_k\)</span> 被称作搜索方向，它一般要求是一个能使目标函数值（最小化问题）下降的方向，即满足：</p><p><span class="math display">\[ \begin{array}{l} p_{k}^T\nabla f(x_k)&lt;0 \end{array} \]</span></p><p>进一步说，<span class="math inline">\(p_k\)</span> 的通项式有以下形式：</p><p><span class="math display">\[ \begin{array}{l} p_{k}=-B_{k}^{-1}\nabla f(x_k) \end{array} \]</span></p><p><span class="math inline">\(B_k\)</span> 是一个对称非奇异矩阵（大家请问为什么？）。</p><ul><li><p>在 Steepest Descent 法中 <span class="math inline">\(B_k\)</span> 是一个单位矩阵；</p></li><li><p>在 Newton 法中，<span class="math inline">\(B_k\)</span> 是一个精确的Hessian 矩阵 <span class="math inline">\(\nabla^2 f(x_k)\)</span>；</p></li><li><p>在 Quasi-Newton 法中， <span class="math inline">\(B_k\)</span> 是对Hessian矩阵的估计。</p></li></ul><p><span class="math display">\[ \begin{array}{l} \because p_{k}^T\nabla f(x_k)=-\nabla f(x_k)^TB_{k}^{-1}\nabla f(x_k)&lt;0\\ \therefore \text{$B_k$ is must positive definite.} \end{array} \]</span></p><p>这类优化方法大体分两种，要么是先确定优化方向后确定步长（line search），要么是先确定步长后确定优化方向（trust region）。</p><center><img src="https://vivounicorn.github.io/images/ai_chapter_4/image_1atg85ts8kff2erd551fss2s39.png" width="500"></center><p>以常用的line search为例，如何找到较好的步长 <span class="math inline">\(\alpha\)</span>呢？好的步长它需要满足以下条件：</p><ul><li>Armijo 条件</li></ul><p>充分下降条件，即要使步长<span class="math inline">\(\alpha_k\)</span>在非精确一维搜索中能保证目标函数 <span class="math inline">\(f\)</span>下降，则它需要满足以下不等式：</p><p><span class="math inline">\(f(x_k+\alpha p_k) \le f(x_k) + c_1\alpha \nabla f_k^Tp_k\)</span></p><p><span class="math inline">\(c_1\)</span> 一般选取一个较小的值，例如：<span class="math inline">\(c_1=10^{−4}\)</span>。</p><p>Armijo 条件的几何解释如下：</p><center><img src="https://vivounicorn.github.io/images/ai_chapter_4/image_1apq85tf71ff74affhh8uh16na2a.png" width="400"></center><p>常用求解方法如下：</p><p><span class="math display">\[ \begin{array}{l} \text{Algorithm 3.Backtracking Line search}\\ \textbf{Require:}\;\; \rho \in (0,1), c \in (0,1)\\ \quad1.\;\;\textbf{choose }\hat{\alpha}&gt;0,\text{set $\alpha=\hat{\alpha}$}\\ \quad2.\;\;\textbf{repeat }\text{until }f(x_k+\alpha p_k) \le f(x_k) + c\alpha \nabla f_k^Tp_k\\ \quad3.\quad\quad\alpha=\rho\alpha\\ \quad4.\;\;\textbf{end(repeat)}\\ \quad5.\;\;\text{return }\alpha \end{array} \]</span></p><ul><li>Curvature 条件</li></ul><p>不只要求步长能使目标函数下降，还要求其程度，这个要求有点严格，一般只要做到Armijo条件就好了，不等式如下：</p><p><span class="math inline">\(\nabla f(x_k+\alpha p_k)^Tp_k \ge c_2 \nabla f_k^Tp_k, c_2 \in(c_1,1)\)</span></p><center><img src="https://vivounicorn.github.io/images/ai_chapter_4/image_1apq8gg70htn1p57gv9qbj6k79.png" width="450"></center><ul><li>Wolfe 条件</li></ul><p>步长同时满足Armijo 条件和Curvature 条件则被称为其满足Wolfe 条件。</p><h3 id="牛顿法newton-method">4.4.2 牛顿法(Newton Method)</h3><ul><li><p>以<span class="math inline">\(x_0\)</span>点开始寻找<span class="math inline">\(f(x)=0\)</span>的解，在该点做切线，得到新的起点： <span class="math inline">\(x_1=x_0-\frac{f(x_0)}{f&#39;(x_0)}\)</span></p></li><li><p>迭代，直到满足精度条件得到<span class="math inline">\(f(x)=0\)</span>的最优解.</p></li></ul><center><img src="https://vivounicorn.github.io/images/ai_chapter_4/image_1apq53v8s1567rgu1t1v14eaqjb13.png" alt="image_1apq53v8s1567rgu1t1v14eaqjb13.png-21.7kB"></center><p>从泰勒展开式得到牛顿法的基本迭代式：</p><p><span class="math display">\[ \begin{array}{l} f(x_{n+1})=f(x_n)+\nabla f(x_n)(x_{n+1}-x_n)+\frac{1}{2}\nabla^2 f(x_n)(x_{n+1}-x_n)^2\\ \because \nabla f(x_{n+1})=\nabla f(x_n)+\nabla^2 f(x_n)(x_{n+1}-x_n)=0\\ \therefore x_{n+1}=x_n-\nabla^2 f(x_n)^{-1}\nabla f(x_n)\\ \end{array} \]</span></p><p>对牛顿法的改进之一是使用自适应步长 <span class="math inline">\(\alpha\)</span>：</p><center><img src="https://vivounicorn.github.io/images/ai_chapter_4/image_1apq7j7prao3qfa1k6es0l83o1g.png" width="800"></center><p>但总的来说牛顿法由于需要求解Hessian 矩阵，所以计算代价过大，对问题规模较大的优化问题力不从心。</p><h3 id="拟牛顿法quasi-newton-method">4.4.3 拟牛顿法(Quasi-Newton Method)</h3><p>为解决Hessian 矩阵计算代价的问题，想到通过一阶信息去估计它的办法，于是涌现出一类方法，其中最有代表性的是DFP和BFGS(L-BFGS)，其原理如下：</p><p><span class="math display">\[ \begin{array}{l} \text{We set $f(x_k)=f_k$ and $\nabla f(x_k)$=$\nabla f_k$}\\ \because \nabla f_{k+1}\approx\nabla f_{k}+\nabla^2 f_{k}(x_{k+1}-x_k)\\ \therefore \nabla^2 f_{k}(x_{k+1}-x_k)\approx\nabla f_{k+1}-\nabla f_{k}\\ \text{our task is to approximate hessian matrix $\nabla^2 f_{k}$}\\ \textbf{set } s_k=x_{k+1}-x_k,y_k=\nabla f_{k+1}-\nabla f_k\\ \text{and the low-rank approximating of hessian matrix is $B_{k+1}$ then}\\ 1.B_{k+1}s_k=y_k \quad \quad \\ 2.\text{$B_{k+1}$must be symmetric.}\\ 3.\text{We hope the new matrix can be stable and does not change wildly from iteration to iteration.}\\ \text{so we have a optimization problem:}\\ * \textbf{DFP}.\\ min ||B-B_{k}||_W\\ s.t.\;B=B^T\\ \quad \quad Bs_k=y_k\\ where\; ||B-B_{k}||_W=||W^{1/2}(B-B_k)W^{1/2}||\\ \text{W is any matrix satisfying } Wy_k=s_k\\ \text{The solution of this problem is:}\\ B_{k+1}=(I-(y_k^Ts_k)^{-1}y_ks_k^T)B_k(I-(y_k^Ts_k)^{-1}s_ky_k^T)+(y_k^Ts_k)^{-1}y_ky_k^T\\ \text{Note that if the initial Hessian approximation B_0 is positive definite then B_k will be positive definite}\\ \text{This algorithm is called DFP, named after Davidson, who discovered it in 1959, and Fletcher and Powell.}\\ \\ * \textbf{BFGS}.\\ \text{If we directly approximate Hessian&#39;s inverse $H_k=B_k^{-1}$ then we have a optimization problem:}\\ min ||H-H_{k}||_W\\ s.t.\;H=H^T\\ \quad \quad Hy_k=s_k\\ where\; ||H-H_{k}||_W=||W^{1/2}(H-H_k)W^{1/2}||\\ \text{W is any matrix satisfying } Ws_k=y_k\\ \text{The solution of this problem is:}\\ H_{k+1}=(I-(y_k^Ts_k)^{-1}s_ky_k^T)H_k(I-(y_k^Ts_k)^{-1}y_ky_s^T)+(y_k^Ts_k)^{-1}s_ks_k^T\\ \end{array} \]</span></p><p>一些有用的资料：</p><ul><li>最优化相关书籍首推：《Numerical Optimization 2nd ed (Jorge Nocedal, Stephen J.Wright)》</li><li>vw源码：<a target="_blank" rel="noopener" href="https://github.com/JohnLangford/vowpal_wabbit/wiki">vowpal_wabbit</a></li><li><a target="_blank" rel="noopener" href="http://hunch.net/~jl/">John Langford</a>的<a target="_blank" rel="noopener" href="http://hunch.net/">博客</a></li></ul>思考一个问题：为什么通常二阶优化方法收敛速度快于一阶方法？<center><img src="https://vivounicorn.github.io/images/ai_chapter_4/image_1apsoja5s1iao1v9b22c17ie18hp9.png" width="400"></center><h2 id="owl-qn算法">4.5 OWL-QN算法</h2><h3 id="bfgs算法回顾">4.5.1 BFGS算法回顾</h3><p>算法思想如下：</p><p><strong>Step1:</strong> 取初始点<span class="math inline">\(x^{(0)}\)</span> ，初始正定矩阵<span class="math inline">\(H_0\)</span>，允许误差<span class="math inline">\(\epsilon&gt;0\)</span>，令<span class="math inline">\(k=0\)</span></p><p><strong>Step2:</strong> 计算: <span class="math display">\[p^{(k)}=-H_k \nabla f(x^{(k)})\]</span></p><p><strong>Step3:</strong> 计算<span class="math inline">\(\alpha_k&gt;0\)</span>，使得: <span class="math display">\[f(x^{(k)}+\alpha_kp^{(k)})=\min\limits_{\alpha \geq 0} f(x^{(k)}+\alpha p^{(k)})\]</span></p><p><strong>Step4:</strong> 令: <span class="math display">\[x^{(k+1)}=x^{(k)}+\alpha_k p^{(k)}\]</span></p><p><strong>Step5:</strong> 如果<span class="math display">\[||\nabla f(x^{(k+1)})|| \leq \epsilon\]</span>，则取<span class="math inline">\(x^{(k+1)}\)</span>为近似最优解；否则转下一步</p><p><strong>Step6:</strong> 计算：</p><p><span class="math display">\[s_k=x^{(k+1)}-x^{(k)}\]</span></p><p><span class="math display">\[y_k=\nabla f(x^{(k+1)})-\nabla f(x^{(k)})\]</span></p><p><span class="math display">\[H_{k+1}=H_k+\frac{1}{s_k^Ty_k}(1+\frac{y_k^TH_ky_k}{s_k^Ty_k})s_ks_k^T-\frac{1}{s_k^Ty_k}(s_ky_k^TH_k+H_ky_ks_k)\]</span></p><p>令<span class="math inline">\(k=k+1\)</span>，转<strong>Step2</strong>.</p><p>优点：</p><p>1、不用直接计算Hessian矩阵；</p><p>2、通过迭代的方式用一个近似矩阵代替Hessian矩阵的逆矩阵。</p><p>缺点：</p><p>1、矩阵存储量为<span class="math inline">\(n^2\)</span>，因此维度很大时内存不可接受；</p><p>2、矩阵非稀疏会导致训练速度慢。</p><h3 id="l-bfgs算法">4.5.2 L-BFGS算法</h3><p>针对BFGS的缺点，主要在于如何合理的估计出一个Hessian矩阵的逆矩阵，L-BFGS的基本思想是只保存最近的m次迭代信息，从而大大降低数据存储空间。对照BFGS，我重新整理一下用到的公式：</p><p><span class="math display">\[\rho_k=\frac{1}{y_{k}^T s_k} s_k=x_k-x_{k-1} y_k\\ =\nabla{f(x_k)}-\nabla{f(x_{k-1})} V_k\\ =I-\rho_{k}y_{k}s_{k}^T \]</span></p><p>于是估计的Hessian矩阵逆矩阵如下：</p><p><span class="math display">\[ H_k=(I-\rho_{k-1}s_{k-1}y_{k-1}^T)H_{k-1}(I-\rho_{k-1}y_{k-1}s_{k-1}^T)+s_{k-1}\rho_{k-1}s_{k-1}^T \\ =V_{k-1}^TH_{k-1}V_{k-1}+ s_{k-1}\rho_{k-1}s_{k-1}^T \]</span></p><p>把 <span class="math display">\[ H_{k-1}=V_{k-2}^TH_{k-2}V_{k-2}+ s_{k-2}\rho_{k-2}s_{k-2}^T\]</span></p><p>带入上式，得：</p><p><span class="math display">\[H_k=V_{k-1}^TV_{k-2}^TH_{k-2}V_{k-2}V_{k-1}+ V_{k-1}^Ts_{k-2}\rho_{k-2}s_{k-2}^T V_{k-1}+s_{k-1}\rho_{k-1}s_{k-1}^T \]</span></p><p>假设当前迭代为<span class="math inline">\(k\)</span>，只保存最近的<span class="math inline">\(m\)</span>次迭代信息，（即：从<span class="math inline">\(k-m~k-1\)</span>），依次带入<span class="math inline">\(H\)</span>，得到：</p><p><strong>公式1：</strong></p><p><span class="math display">\[H_k=(V_{k-1}^TV_{k-2}^T\ldots V_{k-m}^T) H_k^{0}(V_{k-m}\ldots V_{k-2}V_{k-1}) + (V_{k-1}^TV_{k-2}^T\ldots V_{k-m+1}^T) S_{k-m}\rho_{k-m}S_{k-m}^T (V_{k-m}\ldots V_{k-2}V_{k-1})\\ + (V_{k-1}^TV_{k-2}^T\ldots V_{k-m+2}^T) S_{k-m+1}\rho_{k-m+1}S_{k-m+1}^T (V_{k-m+1}\ldots V_{k-2}V_{k-1})\\ +\ldots +V_{k-1}^T s_{k-2}\rho_{k-2} s_{k-2}^TV_{k-1} +s_{k-1}\rho_{k-1}s_{k-1}^T \]</span></p><p>算法第二步表明了上面推导的最终目的：找到第k次迭代的可行方向，满足：</p><p><span class="math display">\[ p_k=-H_k\nabla f(x_k)\]</span></p><p>为了求可行方向<span class="math inline">\(p\)</span>，有下面的:</p><p>two-loop recursion算法：</p><p><span class="math display">\[ \begin{align*} &amp;q=\nabla f(x_k) \\ &amp;for (i=1 \ldots m) \quad do \\ &amp;\quad \alpha_i=\rho_{k-i}s_{k-i}^Tq\\ &amp;\quad q=q-\alpha_iy_{k-i}\\ &amp;end \quad for\\ &amp;r=H_k^{0}q \\ &amp;for (i=m \ldots 1) \quad do\\ &amp;\quad \beta=\rho_{k-i}y_{k-i}^Tr \\ &amp;\quad r=r+s_{k-i}(\alpha_i-\beta)\\ &amp;end \quad for\\ &amp;return \quad r \end{align*} \]</span></p><p>该算法的正确性推导：</p><p>1、令: $ q_0=f(x_k)$ ，递归带入<span class="math inline">\(q\)</span>：</p><p><span class="math display">\[ \begin{align*} q_i&amp;=q_{i-1}-\rho_{k-i}y_{k-i}s_{ki}^Tq_{i-1}\\ &amp;=(I-\rho_{k-i}y_{k-i}s_{k-i}^T)q_{i-1} \\ &amp;=V_{k-i}q_{i-1}\\ &amp;=V_{k-i}V_{k-i+1}q_{i-2}\\ &amp;=\ldots \\ &amp; =V_{k-i}V_{k-i+1} \ldots V_{k-1} q_0\\ &amp;=V_{k-i}V_{k-i+1} \ldots V_{k-1} \nabla f(x_k) \end{align*} \]</span></p><p>相应的：</p><p><span class="math display">\[ \begin{align*} \alpha_i&amp;=\rho_{k-i}s_{k-i}^Tq_{i-1}\\ &amp;=\rho_{k-i}s_{k-i}^T V_{k-i+1}V_{k-i+2} \ldots V_{k-1} \nabla f(x_k) \end{align*} \]</span></p><p>2、令：<span class="math inline">\(r_{m+1}=H_{k-m}q=H_{k-m}V_{k-i}V_{k-i+1} \ldots V_{k-1} \nabla f(x_k)\)</span></p><p><span class="math display">\[ \begin{align*} r_i&amp;=r_{i+1}+s_{k-i}(\alpha_i-\beta)\\ &amp;=r_{i+1}+s_{k-i}(\alpha_i-\rho_{k-i}y_{k-i}^Tr_{i+1}) \\ &amp;=s_{k-i}\alpha_i+(I-s_{k-i}\rho_{k-i}y_{k-i}^T)r_{i+1}\\ &amp;=s_{k-i}\alpha_{i}+V_{k-i}^Tr_{i+1} \end{align*} \]</span></p><p>于是:</p><p><span class="math display">\[ \begin{align*} r_1&amp;=s_{k-1}\alpha_1+V_{k-1}^Tr_2 =s_{k-1}\rho_{k-1}s_{k-1}^T \nabla f(x_k)+V_{k-1}^Tr_2\\ &amp;=s_{k-1}\rho_{k-1}s_{k-1}^T \nabla f(x_k)+V_{k-1}^T(s_{k-2}\alpha_2+V_{k-2}^Tr_3)\\ &amp;=s_{k-1}\rho_{k-1}s_{k-1}^T \nabla f(x_k)+V_{k-1}^Ts_{k-2}\rho_{k-2}s_{k-2}^TV_{k-1}\nabla f(x_k)+V_{k-1}^T V_{k-2}^T r_3\\ &amp;=\ldots \\ &amp;=s_{k-1}\rho_{k-1}s_{k-1}^T \nabla f(x_k)\\ &amp;+V_{k-1}^T s_{k-2}\rho_{k-2} s_{k-2}^TV_{k-1} \nabla f(x_k)\\ &amp;+\ldots\\ &amp;+ (V_{k-1}^TV_{k-2}^T\ldots V_{k-m+2}^T) S_{k-m+1}\rho_{k-m+1}S_{k-m+1}^T (V_{k-m+1}\ldots V_{k-2}V_{k-1}) \nabla f(x_k)\\ &amp;+ (V_{k-1}^TV_{k-2}^T\ldots V_{k-m+1}^T) S_{k-m}\rho_{k-m}S_{k-m}^T (V_{k-m}\ldots V_{k-2}V_{k-1}) \nabla f(x_k)\\ &amp;+(V_{k-1}^TV_{k-2}^T\ldots V_{k-m}^T) H_{k-m}(V_{k-m}\ldots V_{k-2}V_{k-1}) \nabla f(x_k) \end{align*} \]</span></p><p>这个two-loop recursion算法的结果和**公式1*初始梯度**的形式完全一样，这么做的好处是：</p><p>1、只需要存储<span class="math inline">\(s_{k-i}、y_{k-i} （i=1~m）\)</span>；</p><p>2、计算可行方向的时间复杂度从<span class="math inline">\(O(n*n)\)</span>降低到了<span class="math inline">\(O(n*m)\)</span>，当<span class="math inline">\(m\)</span>远小于<span class="math inline">\(n\)</span>时为线性复杂度。</p><p>总结L-BFGS算法的步骤如下：</p><p><strong>Step1</strong>:选初始点<span class="math inline">\(x_0\)</span>，允许误差<span class="math inline">\(\epsilon &gt;0\)</span>，存储最近迭代次数<span class="math inline">\(m\)</span>（一般取6）；</p><p><strong>Step2</strong>:<span class="math inline">\(k=0, \quad \ H_0=I , \quad r=\nabla f(x_{0})\)</span>；</p><p><strong>Step3</strong>:如果 <span class="math inline">\(||\nabla f(x_{k+1})||\leq \epsilon\)</span> 则返回最优解<span class="math inline">\(x_{k+1}\)</span>，否则转<strong>Step4</strong>；</p><p><strong>Step4</strong>:计算本次迭代的可行方向：<span class="math inline">\(p_k=-r _k\)</span>；</p><p><strong>Step5</strong>: 计算步长<span class="math inline">\(\alpha_k&gt;0\)</span>，对下面式子进行一维搜索：</p><p><span class="math display">\[f(x_k+\alpha_kp_k)=\min\limits_{\alpha \geq 0} \quad f(x_k+\alpha p_k)\]</span></p><p><strong>Step6</strong>:更新权重<span class="math inline">\(x\)</span>：</p><p><span class="math display">\[ x_{k+1}=x_k+\alpha_kp_k\]</span></p><p><strong>Step7</strong>: 如果 $ k &gt; m$ 只保留最近m次的向量对，需删除(<span class="math inline">\(s_{k-m},y_{k-m}\)</span>)；</p><p><strong>Step8</strong>:计算并保存： <span class="math display">\[ \begin{align*} s_k&amp;=x_{k+1}-x_k\\ y_k&amp;=\nabla f(x_{k+1})-\nabla f(x_k) \end{align*} \]</span></p><p><strong>Step9</strong>:用two-loop recursion算法求得： <span class="math display">\[r_k=H_k\nabla f(x_k)\]</span></p><p><span class="math inline">\(k=k+1\)</span>，转<strong>Step3</strong>。</p><p>需要注意的地方，每次迭代都需要一个<span class="math inline">\(H_{k-m}\)</span> ，实践当中被证明比较有效的取法为：</p><p><span class="math display">\[ \begin{align*} H_k^0&amp;=\gamma_k I\\ \gamma_k&amp;=\frac{s_{k-1}^Ty_{k-1}}{y_{k-1}^Ty_{k-1}} \end{align*} \]</span></p><h3 id="owl-qn算法原理">4.5.3 OWL-QN算法原理</h3><p><strong>1、问题描述</strong></p><p>对于类似于Logistic Regression这样的Log-Linear模型，一般可以归结为最小化下面这个问题：</p><p><span class="math display">\[ J(x)=l(x)+r(x) \]</span> 其中，第一项为loss function，用来衡量当训练出现偏差时的损失，可以是任意可微凸函数（如果是非凸函数该算法只保证找到局部最优解），后者为regularization term，用来对模型空间进行限制，从而得到一个更“简单”的模型。 根据对模型参数所服从的概率分布的假设的不同，regularization term一般有：L2-norm（模型参数服从Gaussian分布）；L1-norm（模型参数服从Laplace分布）；以及其他分布或组合形式。</p><p>L2-norm的形式类似于：</p><p><span class="math display">\[ J(x)=l(x)+C\sum\limits_i{x_i^2} \]</span> L1-norm的形式类似于：</p><p><span class="math display">\[ J(x)=l(x)+C\sum\limits_i{|x_i|} \]</span></p><p>L1-norm和L2-norm之间的一个最大区别在于前者可以产生稀疏解，这使它同时具有了特征选择的能力，此外，稀疏的特征权重更具有解释意义。</p>对于损失函数的选取就不在赘述，看两幅图：<center><img src="https://vivounicorn.github.io/images/ai_chapter_4/image_1fbjjdgn91r7v1p7a1o1c1o2f100b1t.png" width="400"> 图1 - 红色为Laplace Prior，黑色为Gaussian Prior</center><center><img src="https://vivounicorn.github.io/images/ai_chapter_4/image_1fbjjf0o2mbd108vl2p1vhh1v5f2a.png" width="400"> 图2 直观解释稀疏性的产生</center><p>对LR模型来说损失函数选取凸函数，那么L2-norm的形式也是的凸函数，根据最优化理论，最优解满足KKT条件，即有：<span class="math inline">\(\nabla J(x^*)=0\)</span> ，但是L1-norm的regularization term显然不可微，怎么办呢？</p><p>2、Orthant-Wise Limited-memory Quasi-Newton</p>OWL-QN主要是针对L1-norm不可微提出的，它是基于这样一个事实：任意给定一个维度象限，L1-norm 都是可微的，因为此时它是一个线性函数：<center><img src="https://vivounicorn.github.io/images/ai_chapter_4/image_1fbjjijog11ju16bmmdk1rcg134d2n.png" width="300"> 图3 任意给定一个象限后的L1-norm</center><p>OWL-QN中使用了次梯度决定搜索方向，凸函数不一定是光滑而处处可导的，但是它又符合类似梯度下降的性质，在多元函数中把这种梯度叫做次梯度，见维基百科http://en.wikipedia.org/wiki/Subderivative</p>举个例子：<center><img src="https://vivounicorn.github.io/images/ai_chapter_4/image_1fbjjji7k12qhvta1gud16d21iv734.png" width="300"> 图4 次导数</center><p>对于定义域中的任何<span class="math inline">\(x_0\)</span>，我们总可以作出一条直线，它通过点<span class="math inline">\((x_0, f(x_0))\)</span>，并且要么接触f的图像，要么在它的下方。这条直线的斜率称为函数的次导数，推广到多元函数就叫做次梯度。</p><p>次导数及次微分： 凸函数<span class="math inline">\(f:I→R\)</span>在点<span class="math inline">\(x_0\)</span>的次导数，是实数c使得：</p><p><span class="math display">\[ f(x)-f(x_0)\ge c(x-x_0) \]</span> 对于所有I内的x。可以证明，在点x0的次导数的集合是一个非空闭区间[a, b]，其中a和b是单侧极限</p><p><span class="math display">\[ a=\lim_{x\to x_0^-}\frac{f(x)-f(x_0)}{x-x_0}\\ b=\lim_{x\to x_0^+}\frac{f(x)-f(x_0)}{x-x_0} \]</span></p><p>它们一定存在，且满足<span class="math inline">\(a ≤ b\)</span>。所有次导数的集合<span class="math inline">\([a, b]\)</span>称为函数<span class="math inline">\(f\)</span>在<span class="math inline">\(x_0\)</span>的次微分。</p><p><strong>OWL-QN和传统L-BFGS的不同之处在于：</strong></p><ul><li>利用次梯度的概念推广了梯度 定义了一个符合上述原则的虚梯度，求一维搜索的可行方向时用虚梯度来代替L-BFGS中的梯度： <span class="math display">\[ \begin{align*} \Diamond_i f(x) &amp;=\left\{ \begin{array}{**lr**} \partial_i^{-}f(x) &amp; if &amp;\partial_i^{-}f(x)&gt;0 \\ \partial_i^{+}f(x)&amp; if &amp;\partial_i^{+}f(x)&lt;0\\ 0 &amp; &amp;otherwise \end{array} \right.\\ \partial_i^{\pm}f(x)&amp;=\frac{\partial}{\partial x_i}l(x)+\left\{ \begin{array}{**lr**} C\sigma (x_i) &amp; if &amp;x_i\neq 0\\ \pm C&amp; if &amp; x_i=0 \end{array} \right.\\ \partial_i^{-}f(x) &amp;\leq \partial_i^{+}f(x) \end{align*} \]</span> 怎么理解这个虚梯度呢？见下图： 对于非光滑凸函数，那么有这么几种情况：<center><img src="https://vivounicorn.github.io/images/ai_chapter_4/image_1fbjjqkt419971tob174m1lmo1kih3h.png" width="300"> 图5 <span class="math inline">\(\partial_i^-f(x)&gt;0\)</span></center><center><img src="https://vivounicorn.github.io/images/ai_chapter_4/image_1fbjjr9sra7uhbh1hukbl01vtp3u.png" width="300"> 图6 <span class="math inline">\(\partial_i^+f(x)&lt;0\)</span></center><center><img src="https://vivounicorn.github.io/images/ai_chapter_4/image_1fbjjropb8ee7tf1hnck4doaf4b.png" width="400"> 图7 otherwise</center></li><li>一维搜索要求不跨越象限 要求更新前权重与更新后权重同方向：</li></ul><center><img src="https://vivounicorn.github.io/images/ai_chapter_4/image_1fbjjubf4103g1son9t5qe61q394r.png" width="300"> 图8 OWL-QN的一次迭代</center><p>总结OWL-QN的一次迭代过程：</p><ul><li><p>Find vector of steepest descent</p></li><li><p>Choose sectant</p></li><li><p>Find L-BFGS quadratic approximation</p></li><li><p>Jump to minimum</p></li><li><p>Project back onto sectant</p></li><li><p>Update Hessian approximation using gradient of loss alone</p></li></ul><p>最后OWL-QN算法框架如下：</p><center><img src="https://vivounicorn.github.io/images/ai_chapter_4/image_1fbjk825e1vldbo71as012u1gsi5o.png" width="400"></center><p>与L-BFGS相比，第一步用虚梯度代替梯度，第二、三步要求一维搜索不跨象限，也就是迭代前的点与迭代后的点处于同一象限，第四步要求估计Hessian矩阵时依然使用loss function的梯度（因为L1-norm的存在与否不影响Hessian矩阵的估计）。</p></div><div class="popular-posts-header">相关文章推荐</div><ul class="popular-posts"><li class="popular-posts-item"><div class="popular-posts-title"><a href="/article/d677b2e0.html" rel="bookmark">机器学习与人工智能技术分享-第三章 机器学习中的统一框架</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/article/307f39c.html" rel="bookmark">机器学习与人工智能技术分享-第二章 建模方法回顾</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/article/b12a240.html" rel="bookmark">机器学习与人工智能技术分享-第九章 语义分割</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/article/301f2134.html" rel="bookmark">机器学习与人工智能技术分享-第十一章 OCR</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/article/b539fa9.html" rel="bookmark">机器学习与人工智能技术分享-第十二章 机器学习框架</a></div></li></ul><footer class="post-footer"><div class="post-tags"><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a> <a href="/tags/%E6%9C%80%E4%BC%98%E5%8C%96/" rel="tag"># 最优化</a> <a href="/tags/%E7%AC%AC%E5%9B%9B%E7%AB%A0/" rel="tag"># 第四章</a></div><div class="post-nav"><div class="post-nav-item"><a href="/article/d677b2e0.html" rel="prev" title="机器学习与人工智能技术分享-第三章 机器学习中的统一框架"><i class="fa fa-chevron-left"></i> 机器学习与人工智能技术分享-第三章 机器学习中的统一框架</a></div><div class="post-nav-item"><a href="/article/783e74f9.html" rel="next" title="机器学习与人工智能技术分享-第五章 深度神经网络">机器学习与人工智能技术分享-第五章 深度神经网络 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener("tabs:register",()=>{let e=CONFIG.comments["activeClass"];if(CONFIG.comments.storage&&(e=localStorage.getItem("comments_active")||e),e){let t=document.querySelector(`a[href="#comment-${e}"]`);t&&t.click()}}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{t.target.matches(".tabs-comment .tab-content .tab-pane")&&(t=t.target.classList[1],localStorage.setItem("comments_active",t))})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9C%80%E4%BC%98%E5%8C%96%E5%8E%9F%E7%90%86"><span class="nav-text">4. 最优化原理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B3%B0%E5%8B%92%E5%AE%9A%E7%90%86"><span class="nav-text">4.1 泰勒定理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B3%B0%E5%8B%92%E5%B1%95%E5%BC%80%E5%BC%8F"><span class="nav-text">4.1.1 泰勒展开式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B3%B0%E5%8B%92%E4%B8%AD%E5%80%BC%E5%AE%9A%E7%90%86"><span class="nav-text">4.1.2 泰勒中值定理</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="nav-text">4.2 梯度下降法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86"><span class="nav-text">4.2.1 基本原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0%E6%B3%95%E5%92%8Ckkt%E6%9D%A1%E4%BB%B6"><span class="nav-text">4.2.2 拉格朗日乘数法和KKT条件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-text">4.2.3 批量梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-text">4.2.4 随机梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B0%8F%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-text">4.2.5 小批量梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%9B%E9%A1%BF%E6%B3%95"><span class="nav-text">4.2.6 牛顿法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#momentum"><span class="nav-text">4.2.7 Momentum</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#adagrad"><span class="nav-text">4.2.8 AdaGrad</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#adadelta"><span class="nav-text">4.2.9 AdaDelta</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#adam"><span class="nav-text">4.2.10 Adam</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B9%B6%E8%A1%8Csgd"><span class="nav-text">4.3 并行SGD</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#allreduce"><span class="nav-text">4.3.1 AllReduce</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E6%9C%8D%E5%8A%A1%E5%99%A8parameter-server"><span class="nav-text">4.3.2 参数服务器(Parameter Server)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E9%98%B6%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95"><span class="nav-text">4.4 二阶优化方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A6%82%E8%A7%88"><span class="nav-text">4.4.1 概览</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%9B%E9%A1%BF%E6%B3%95newton-method"><span class="nav-text">4.4.2 牛顿法(Newton Method)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95quasi-newton-method"><span class="nav-text">4.4.3 拟牛顿法(Quasi-Newton Method)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#owl-qn%E7%AE%97%E6%B3%95"><span class="nav-text">4.5 OWL-QN算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#bfgs%E7%AE%97%E6%B3%95%E5%9B%9E%E9%A1%BE"><span class="nav-text">4.5.1 BFGS算法回顾</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#l-bfgs%E7%AE%97%E6%B3%95"><span class="nav-text">4.5.2 L-BFGS算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#owl-qn%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86"><span class="nav-text">4.5.3 OWL-QN算法原理</span></a></li></ol></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="张磊" src="https://vivounicorn.github.io/images/wali.png"><p class="site-author-name" itemprop="name">张磊</p><div class="site-description" itemprop="description"></div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">13</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">2</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">41</span> <span class="site-state-item-name">标签</span></a></div></nav></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2021</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">张磊</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span title="站点总字数">476k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="站点阅读时长">7:12</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动</div></div></footer></div><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script>!function(){var e,t,o,n,r=document.getElementsByTagName("link");if(0<r.length)for(i=0;i<r.length;i++)"canonical"==r[i].rel.toLowerCase()&&r[i].href&&(e=r[i].href);t=(e||window.location.protocol).split(":")[0],e=e||window.location.href,window,o=e,n=document.referrer,/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(o)||(t="https"===String(t).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif",n?(t+="?r="+encodeURIComponent(document.referrer),o&&(t+="&l="+o)):o&&(t+="?l="+o),(new Image).src=t)}()</script><script src="/js/local-search.js"></script><script>"undefined"==typeof MathJax?(window.MathJax={loader:{load:["[tex]/mhchem"],source:{"[tex]/amsCd":"[tex]/amscd","[tex]/AMScd":"[tex]/amscd"}},tex:{inlineMath:{"[+]":[["$","$"]]},packages:{"[+]":["mhchem"]},tags:"ams"},options:{renderActions:{findScript:[10,n=>{document.querySelectorAll('script[type^="math/tex"]').forEach(e=>{var t=!!e.type.match(/; *mode=display/);const a=new n.options.MathItem(e.textContent,n.inputJax[0],t);t=document.createTextNode("");e.parentNode.replaceChild(t,e),a.start={node:t,delim:"",n:0},a.end={node:t,delim:"",n:0},n.math.push(a)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}},function(){var e=document.createElement("script");e.src="//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js",e.defer=!0,document.head.appendChild(e)}()):(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset())</script><script>NexT.utils.loadComments(document.querySelector("#valine-comments"),()=>{NexT.utils.getScript("//unpkg.com/valine/dist/Valine.min.js",()=>{var i=["nick","mail","link"],e="nick,mail,link".split(",").filter(e=>i.includes(e));new Valine({el:"#valine-comments",verify:!1,notify:!0,appId:"m8FPP0CqMpxyTuUvaVOX9qVV-gzGzoHsz",appKey:"Ori6X9PXqQyURvwgl7HT5TJj",placeholder:"赠人玫瑰，手有余香",avatar:"mm",meta:e,pageSize:"10",visitor:!0,lang:"zh-cn",path:location.pathname,recordIP:!1,serverURLs:""})},window.Valine)})</script></body></html>