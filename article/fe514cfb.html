<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><meta name="google-site-verification" content="csXtmAKXrGoQr2GkIa90aLM9urmzXCZXGcXWMwI-kj0"><meta name="msvalidate.01" content="AF3396A141E1B198CA1BE76915B3969F"><meta name="yandex-verification" content="ee8492bd2e7708db"><meta name="baidu-site-verification" content="code-h6vqPQbqvn"><meta name="sogou_site_verification" content="33FOy4QSNu"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css"><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"www.vivounicorn.xyz",root:"/",scheme:"Muse",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"always",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:{enable:!0,onlypost:!1,loadingImg:"./images/loading.gif",isSPA:!1,preloadRatio:3},pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="本文对BERTopic主题模型原理、思路和应用做了简单介绍。"><meta property="og:type" content="article"><meta property="og:title" content="BERTopic主题模型详解"><meta property="og:url" content="http://www.vivounicorn.xyz/article/fe514cfb.html"><meta property="og:site_name" content="业精于勤，荒于嬉；行成于思，毁于随。"><meta property="og:description" content="本文对BERTopic主题模型原理、思路和应用做了简单介绍。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://vivounicorn.github.io/images/bertopic/BERTopic.png"><meta property="og:image" content="https://vivounicorn.github.io/images/bertopic/BERTopic.png"><meta property="og:image" content="https://vivounicorn.github.io/images/bertopic/huggingface.png"><meta property="og:image" content="https://vivounicorn.github.io/images/bertopic/kl.png"><meta property="og:image" content="https://vivounicorn.github.io/images/bertopic/com.png"><meta property="og:image" content="https://vivounicorn.github.io/images/bertopic/digits.png"><meta property="og:image" content="https://vivounicorn.github.io/images/bertopic/umap.png"><meta property="og:image" content="https://vivounicorn.github.io/images/bertopic/umap_g.png"><meta property="og:image" content="https://vivounicorn.github.io/images/bertopic/cluster.png"><meta property="og:image" content="https://vivounicorn.github.io/images/bertopic/h.png"><meta property="og:image" content="https://vivounicorn.github.io/images/bertopic/gd.png"><meta property="og:image" content="https://vivounicorn.github.io/images/bertopic/gd1.png"><meta property="og:image" content="https://vivounicorn.github.io/images/bertopic/d1.png"><meta property="og:image" content="https://vivounicorn.github.io/images/bertopic/d2.png"><meta property="og:image" content="https://vivounicorn.github.io/images/bertopic/d3.png"><meta property="og:image" content="https://vivounicorn.github.io/images/bertopic/gd5.png"><meta property="og:image" content="https://vivounicorn.github.io/images/bertopic/de1.png"><meta property="og:image" content="https://vivounicorn.github.io/images/bertopic/de2.png"><meta property="og:image" content="https://vivounicorn.github.io/images/bertopic/de3.png"><meta property="article:published_time" content="2022-08-13T07:28:34.000Z"><meta property="article:modified_time" content="2022-09-23T05:43:40.378Z"><meta property="article:author" content="张磊"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="主题模型"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://vivounicorn.github.io/images/bertopic/BERTopic.png"><link rel="canonical" href="http://www.vivounicorn.xyz/article/fe514cfb.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>BERTopic主题模型详解 | 业精于勤，荒于嬉；行成于思，毁于随。</title><script async src="https://www.googletagmanager.com/gtag/js?id=G-P394BZNEMZ"></script><script>function gtag(){dataLayer.push(arguments)}CONFIG.hostname===location.hostname&&(window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-P394BZNEMZ"))</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"><a target="_blank" rel="noopener" href="https://github.com/vivounicorn/" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513;color:#fff;position:absolute;top:0;border:0;right:0" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">业精于勤，荒于嬉；行成于思，毁于随。</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">花晨月夕</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>站点地图</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://www.vivounicorn.xyz/article/fe514cfb.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="https://vivounicorn.github.io/images/wali.png"><meta itemprop="name" content="张磊"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="业精于勤，荒于嬉；行成于思，毁于随。"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">BERTopic主题模型详解</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2022-08-13 15:28:34" itemprop="dateCreated datePublished" datetime="2022-08-13T15:28:34+08:00">2022-08-13</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a> </span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">自然语言处理</span></a> </span></span><span id="/article/fe514cfb.html" class="post-meta-item leancloud_visitors" data-flag-title="BERTopic主题模型详解" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/article/fe514cfb.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/article/fe514cfb.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>17k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>15 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p><img data-src="https://vivounicorn.github.io/images/bertopic/BERTopic.png" width="266"> 本文对BERTopic主题模型原理、思路和应用做了简单介绍。 <span id="more"></span></p><h1 id="bertopic主题模型详解">BERTopic主题模型详解</h1><p>主题模型是用来在非结构数据中无监督的发现隐含主题信息的一类重要工具，比较成熟和常用的算法有基于矩阵分解（如：SVD分解）的LSA（Latent Semantic Analysis）, 引入概率方法代替SVD的pLSA（Probabilistic Latent Semantic Analysis）, 贝叶斯版本的pLSA——LDA（Latent Dirichlet Allocation）。 但这类方法的本质是通过最小化误差从而找到能够复现原有文档分布的主题，对隐含语义的捕获能力非常有限（本质是对信息的distributional representation）， 而随着神经网络和Embedding技术（本质是对信息的distributed representation）的发展以及Transformer在NLP领域的爆发，出现了更加有趣的主题模型，其中Maarten Grootendorst提出的BERTopic是一个简洁而强大的算法，论文地址：《<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.05794">BERTopic: Neural topic modeling with a class-based TF-IDF procedure</a>》。 本文代码示例可在<a target="_blank" rel="noopener" href="https://github.com/vivounicorn/BERTopic">这里</a>访问。</p><h2 id="bertopic基本过程">BERTopic基本过程</h2><p>BERTopic由四大部分组成，如下图：</p><center><img data-src="https://vivounicorn.github.io/images/bertopic/BERTopic.png" width="600"></center><ul><li><p>语义Embedding表示</p></li><li><p>UMAP降维</p></li><li><p>HDBSCAN文本聚类</p></li><li><p>c-TF-IDF主题生成</p></li></ul><h3 id="语义embedding">语义Embedding</h3><p>记得当初word2vec刚出现时，我对其惊艳的表现印象很深刻（经典例子：king-man+women=queen），随后出现了类似的sentence2vec、doc2vec应用于不同粒度，不过本质都是通过distributed representation生成信息的语义空间，在这个空间中，可以用“距离”来衡量任意两个稠密向量的“语义”相似度， 生成Embedding向量又有几类方法，例如：基于当前段落向量和上下文向量去预测目标词或基于段落向量去预测上下文词 和 基于Transformer结构的算法（如：Bert），而后者的效果是非常明显的，相比传统类似BOW（Bag of words）的方法，基于Transformer结构的算法同时考虑了上下文信息和词序以及句子中所有词之间的相关性。 对于BERTopic，语义Embedding是第一步也是最重要的一步，语义表达的效果直接决定了后续主题生成的效果。</p><p>BERTopic支持以下几种embedding model的库：</p><ul><li><p><a target="_blank" rel="noopener" href="https://www.sbert.net/">Sentence Transformers</a></p><p>基于PyTorch和HugingFace Transformers项目的Python库，使用非常方便，只需要在 <strong><em>https://huggingface.co</em></strong> 上选择合适的模型就可开箱即用，例如：</p><center><img data-src="https://vivounicorn.github.io/images/bertopic/huggingface.png" width="600"></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> SentenceTransformer</span><br><span class="line"></span><br><span class="line">sentences = [<span class="string">&quot;我这边是易鑫集团的工作人员&quot;</span>, <span class="string">&quot;办理流程就比较简单了?&quot;</span>, <span class="string">&quot;裸车价多少钱呢？&quot;</span>, <span class="string">&quot;加一下您微信&quot;</span>]</span><br><span class="line"></span><br><span class="line">model = SentenceTransformer(<span class="string">&#x27;shibing624/text2vec-base-chinese&#x27;</span>)</span><br><span class="line">embeddings = torch.from_numpy(model.encode(sentences))</span><br><span class="line"></span><br><span class="line">cand = embeddings[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(sentences[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(embeddings)):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;——&#x27;</span>, sentences[i], <span class="string">&#x27;相关度:&#x27;</span>, F.cosine_similarity(cand, embeddings[i], dim=<span class="number">0</span>))</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin/python /home/dell/PycharmProjects/BERTopic/test.py</span><br><span class="line">2022-08-16 12:25:46.384364: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.</span><br><span class="line">Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.</span><br><span class="line">tensor([[-0.1795, -0.7887,  1.3699,  ..., -0.3907, -0.1538,  0.1391],</span><br><span class="line">        [-0.7942, -0.1645,  0.8701,  ..., -0.9783, -0.2415,  0.0278],</span><br><span class="line">        [-0.7666,  0.4602, -0.0598,  ..., -0.6351, -0.8215,  0.2701],</span><br><span class="line">        [ 0.5862,  0.3013,  0.7182,  ..., -0.3796, -0.0490, -0.5745]])</span><br><span class="line">我这边是易鑫集团的工作人员</span><br><span class="line">—— 办理流程就比较简单了? 相似度: tensor(0.5054)</span><br><span class="line">—— 裸车价多少钱呢？ 相似度: tensor(0.3580)</span><br><span class="line">—— 加一下您微信 相似度: tensor(0.4324)</span><br><span class="line"></span><br><span class="line">Process finished with exit code 0</span><br></pre></td></tr></table></figure></li><li><p><a target="_blank" rel="noopener" href="https://github.com/flairNLP/flair">Flair</a></p></li><li><p><a target="_blank" rel="noopener" href="https://spacy.io/">SpaCy</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">import</span> spacy</span><br><span class="line"></span><br><span class="line">  sentences = [<span class="string">&quot;我这边是易鑫集团的工作人员&quot;</span>, <span class="string">&quot;办理流程就比较简单了?&quot;</span>, <span class="string">&quot;裸车价多少钱呢？&quot;</span>, <span class="string">&quot;加一下您微信&quot;</span>]</span><br><span class="line">  <span class="built_in">print</span>(sentences[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">  nlp = spacy.load(<span class="string">&quot;zh_core_web_sm&quot;</span>)</span><br><span class="line">  doc = [nlp(s) <span class="keyword">for</span> s <span class="keyword">in</span> sentences]With what we have so far, <span class="keyword">if</span> we want to correctly project close distances between points, the moderate distances are distorted <span class="keyword">and</span> appear <span class="keyword">as</span> huge distances <span class="keyword">in</span> the low dimensional space. According to the authors of t-SNE, this <span class="keyword">is</span> because “the area of the two-dimensional <span class="built_in">map</span> that <span class="keyword">is</span> available to accommodate moderately distant data points will <span class="keyword">not</span> be nearly large enough compared <span class="keyword">with</span> the area available to accommodate nearby data points.”</span><br><span class="line"></span><br><span class="line">To solve this, the second main modification introduced <span class="keyword">is</span> the use of the Student t-Distribution (which <span class="keyword">is</span> what gives the ‘t’ to t-SNE) <span class="keyword">with</span> one degree of freedom <span class="keyword">for</span> the low-dimensional probabilities,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(doc)):</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">&#x27;——&#x27;</span>, sentences[i], <span class="string">&#x27;相关度:&#x27;</span>, doc[<span class="number">0</span>].similarity(doc[i]))</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin/python /home/dell/PycharmProjects/BERTopic/test.py</span><br><span class="line">2022-08-16 16:24:57.076178: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.</span><br><span class="line">我这边是易鑫集团的工作人员</span><br><span class="line">—— 办理流程就比较简单了? 相关度: -0.023583169329516124</span><br><span class="line">—— 裸车价多少钱呢？ 相关度: 0.19299579633898184</span><br><span class="line">—— 加一下您微信 相关度: -0.01393847136393582</span><br><span class="line"></span><br><span class="line">Process finished with exit code 0</span><br></pre></td></tr></table></figure></li><li><p><a target="_blank" rel="noopener" href="https://radimrehurek.com/gensim/">Gensim</a></p></li><li><p><a target="_blank" rel="noopener" href="https://tfhub.dev/google/universal-sentence-encoder/4">USE (TF Hub)</a></p></li></ul><h3 id="降维">降维</h3><p>当文本信息被embedding后，每个文档会成为一个高维（通常是几百维以上）稠密向量，但整个语义空间会很稀疏，逻辑上语义相关的文档会因为“距离”接近在空间内“聚集”，而为了识别主题向量，首先想到的是基于密度的聚类算法，但这类算法对向量维度很敏感，维度越高速度越慢，所以有必要把原来几百维的向量压缩降维到个位数维度，比如2~3维，同时也方便数据可视化。 降维方法主要有三类：特征选择法（ Feature Selection），基于矩阵分解的方法如<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Principal_component_analysis">PCA</a>（Principal Component Analysis）、<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Singular_value_decomposition">SVD</a>（Singular Value Decomposition）等以及基于邻居图（Neighbor Graphs）的方法<a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2002/file/6150ccc6069bea6b5716254057a194ef-Paper.pdf">SNE</a>（Stochastic Neighbor Embedding）、<a target="_blank" rel="noopener" href="https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf?fbcl">t-SNE</a>（t-Distributed Stochastic Neighbourh Embedding）、<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1802.03426">UMAP</a>（Uniform Manifold Approximation and Projection）等，其中UMAP更是速度和效果方面的佼佼者。</p><p>邻居图方法大致可以看做以下几个步骤：</p><ul><li><p>定义和计算输入空间数据概率分布<span class="math inline">\(p\)</span></p></li><li><p>定义和计算降维空间数据概率分布<span class="math inline">\(q\)</span></p></li><li><p>定义损失函数，描述以上两个概率分布的差异</p></li><li><p>由于这种映射会损失信息，所以最小化求解损失函数，找到让两个分布最接近的参数</p></li><li><p>多次实验找到较优的困惑度（Perplexity）参数 困惑度指的是：已知当前点为<span class="math inline">\(x_i\)</span>，计算概率分布时需要选取的领域内包含的节点个数，显然其物理含义是：取值越大，领域越大，越可能将远处的点作为邻居节点，越能具有“全局”视野但越容易误判，反之越能保留更“准确”的局部信息但容易切得太细，“全局”视野差，困惑度选多少没有理论最优值，但可以通过自己或别人的实验找到比较好的经验值。</p></li></ul><p>比较下几种邻居图方法：</p><ul><li><p><strong>SNE</strong> <strong><em>1、定义和计算输入空间数据概率分布<span class="math inline">\(p\)</span></em></strong></p><p>假设当前点为<span class="math inline">\(x_i=x_0\)</span>，困惑度取值为<span class="math inline">\(n\)</span>，则其邻居节点为<span class="math inline">\(x_j \in \{x_1,...,x_n\}\)</span>，节点<span class="math inline">\(i\)</span>与其任一邻居节点的距离采用缩放的欧氏距离：<span class="math inline">\(d_{j|i}=\sqrt{\frac{||x_i-x_j||^2}{2\delta_i^2}}\)</span>（其中<span class="math inline">\(\delta_i\)</span>为调节参数），任一邻居节点<span class="math inline">\(j\)</span>的分布为：<span class="math display">\[p_{j|i}=\frac{e^{-d_{j|i}^2}}{\sum\limits_{k=1,k\neq i}^ne^{-d_{k|i}^2}}\]</span> 注意，为了满足用户设定的困惑度参数<span class="math inline">\(n\)</span>，算法需要通过调节参数<span class="math inline">\(\delta_i\)</span>（假设以<span class="math inline">\(x_i\)</span>为中心的高斯分布的方差）来实现，这样使得<span class="math inline">\(p_{j|i}\neq p_{i|j}\)</span>，即不满足对称性。 另外由于没法用唯一的<span class="math inline">\(\delta_i\)</span>值应用于所有<span class="math inline">\(x_i\)</span>，所以用户在指定困惑度后，SNE采用binary search确定，因此困惑度定义为：<span class="math display">\[Perp(P_i)=2^{H(P_i)}=2^{-\sum\limits_jp_{j|i}log_2{p(j|i)}}\]</span> 于是算法通过自动调整<span class="math inline">\(\delta_i\)</span>参数使得困惑度等于用户设定的值。 ps：困惑度的经验值为5~50。</p><p><strong><em>2.定义和计算降维空间数据概率分布<span class="math inline">\(q\)</span></em></strong></p><p>假设输入空间中的点<span class="math inline">\(x_i\)</span>和<span class="math inline">\(x_j\)</span>映射到降维空间中的点为<span class="math inline">\(y_i\)</span>和<span class="math inline">\(y_j\)</span>，则任一邻居节点<span class="math inline">\(j\)</span>的分布为：<span class="math display">\[q_{j|i}=\frac{e^{-||y_i-y_j||^2}}{\sum\limits_{k=1,k\neq i}^ne^{-||y_i-y_k||^2}}\]</span></p><p><strong><em>3.定义损失函数</em></strong></p><p>直观上看，如果数据从高维输入空间映射到降维空间后的信息损失很小，那么两个概率分布应该在“距离”上很接近，一个自然的选择是KL散度（KL Divergence）：<span class="math display">\[D_{​KL}(p∣∣q)=\sum\limits_{i=1}^m\sum\limits_{j=1}^n p(j|i)⋅log​\frac{p(j|i)}{q(j​|i)}\]</span> 不过这个损失函数也有明显缺点，如下图：</p><center><img data-src="https://vivounicorn.github.io/images/bertopic/kl.png" width="600"></center><p>当高维空间中两个距离相近的点降维映射后距离变远了，此时损失函数会做比较大的惩罚，但当高维空间中两个距离较远的点降维映射后距离变近了，则此时并没有多大惩罚。</p><p><strong><em>4.求解</em></strong></p><p><span class="math inline">\(min D_{​KL}(p∣∣q)\)</span>，采用梯度下降求解，损失函数梯度如下： <span class="math display">\[ \frac{\partial D_{​KL}(p∣∣q)}{\partial y_i}=2\sum\limits_{j=1}^{n}(p(j|i)-q(j|i)+p(i|j)-q(i|j))(y_i-y_j) \]</span> 采用动量梯度下降，迭代式如下： <span class="math display">\[ y_t=y_{t-1}+\eta\frac{\partial D_{​KL}(p∣∣q)}{\partial y}+\alpha(t)(y_{t-1}-y_{t-2}) \]</span> 其中<span class="math inline">\(\eta\)</span>为学习率，<span class="math inline">\(\alpha\)</span>为第<span class="math inline">\(t\)</span>次迭代时的动量值（用来尽可能不陷入到局部最小值），优化求解过程比较费劲，对初始值很敏感，且需要在数据集上多次训练才有可能找到合适的超参数。</p></li><li><p><strong>t-SNE</strong> 从SNE原理上看，除了损失函数优化求解费劲外，还有个缺陷是降维后的数据拥挤问题（Crowding Problem），为了解决这两个问题，做了以下改进： 1、输入空间的条件概率分布改为联合概率分布，使其具有对称性</p><p><span class="math inline">\(i\)</span>是<span class="math inline">\(j\)</span>的邻居，反过来也成立，即<span class="math display">\[p(ij) =\frac{p(j|i)+p(i|j)}{2n}\]</span></p><p>2、解决降维后的数据拥挤问题 简单说，高维空间中的点尤其是互相之间距离中等或较远的点，在降维后出现的聚集现象叫做数据拥挤问题。 t-SNE另外一个改进是对降维后空间数据做自由度为1的t分布假设：<span class="math inline">\(f(x)=\frac{1}{\pi(1+x^2)}\)</span>，这样联合概率分布<span class="math inline">\(q(ij)\)</span>就变成： <span class="math display">\[q(ij)=\frac{(1+||y_i-y_j||^2)^{-1}}{\sum\limits_{k\neq l}(1+||y_k-y_l||^2)^{-1}}\]</span> 因为<span class="math inline">\(p(ij)=p(ji),q(ij)=q(ji)\)</span>，所以损失函数变成了： <span class="math display">\[D_{​KL}(p∣∣q)=\sum\limits_{i=1}^m\sum\limits_{j=1}^n p(ij)⋅log​\frac{p(ij)}{q(ij)}\]</span> 求梯度时就简单多了，变成： <span class="math display">\[ \frac{\partial D_{​KL}(p∣∣q)}{\partial y_i}=4\sum\limits_{j=1}^{n}(p(ij)-q(ij))(y_i-y_j) \]</span></p><p>直观比较如图：</p><center><img data-src="https://vivounicorn.github.io/images/bertopic/com.png" width="600"></center><p>横坐标是降维后两点间距离，纵坐标是<span class="math inline">\(q\)</span>值，显然以高斯分布为假设的数据，对距离远的映射后<span class="math inline">\(p\)</span>值都聚集在一起了，反观以t分布为假设的数据则更有区分度。</p></li><li><p><strong>UMAP</strong> UMAP的理论基础基于黎曼几何和代数拓扑的框架，理论基础很完备。相比t-SNE，它并未做概率分布假设，而是直接定义相似度，而且保留了更多的全局结构信息，对embedding的维度没有计算上的限制，内存占用更小、运行速度更快，是降维领域的SOTA算法。</p><p><strong>什么是“流形”（manifold）</strong>： 流形是一个数学上抽象的拓扑空间，这个空间是n维的，其中的每个点局部来看都有一个类似欧几里得空间的邻域（例如：一维流形中每个点都有一个类似一条线段的邻域，二维流形中每个点都有一个类似圆盘的邻域，三维流形中，每个点都有一个类似单位圆球<span class="math inline">\(x^2+y^2=1\)</span>的邻域），但全局结构又可能很复杂（典型的例子：地球的结构，局部看是平的，但全局看是个球体），流形的定义使得人们可以用更容易理解的简单空间的属性来表达和理解更复杂的结构（换个角度看，流形可以视做欧式空间的非线性推广）。 流形还常常被赋予一些附加结构，例如它可以是平滑的（Smooth Manifolds）的可微流形、可以定义距离和角度的黎曼流形（Riemannian Manifold）等等。</p><p>UMAP的算法设计在数据上做了以下<strong>假设</strong>：</p><p>1、数据在黎曼流形上服从均匀分布，黎曼流形的特点有：可定义距离和角度、流形是可微的</p><p>2、流形是局部连接的(locally connected)，这样拓扑空间就能保证将局部计算结果合理地、光滑地拼接起来，从而揭示出整体结构</p><p>3、黎曼度量（在流形上定义距离的方法，由流形的切空间中所有点的內积组成，是一个对称正定矩阵）是局部恒定（locally constant，上述矩阵是常数对称矩阵）的</p><p>以流形中的任何一个输入数据点<span class="math inline">\(X_i\)</span>为中心，在给定体积的球形邻域内包含的输入数据点个数大致相当，也就是说，以流形中的任何一个输入数据点<span class="math inline">\(X_i\)</span>为中心的球形邻域内，如果都包含<span class="math inline">\(k\)</span>个邻居节点，那么这些球形邻域的体积应该大致相同，因而可以利用<span class="math inline">\(X_i\)</span>到其第<span class="math inline">\(k\)</span>个最近邻的距离对<span class="math inline">\(X_i\)</span>到其邻居们的距离做归一化从而近似得到<span class="math inline">\(X_i\)</span>到其邻居的测地线距离。 <span class="math inline">\(k\)</span>的大小决定了我们希望在多大程度上局部地估计黎曼度量，即<span class="math inline">\(k\)</span>越小代表我们越想捕捉到局部结构信息，反之亦然。 基于以上假设的流形学习，就是要根据已知的数据样本通过学习高维流形的内在结构或规律，找出嵌入在高维空间中的低维光滑流形，从而揭示隐含在高维数据空间中的内在的低维空间结构，整体是一个非线性降维过程，但带来的问题是可解释性会比较差，另外由于UMAP更关注对局部信息的准确表达，所以如果全局结构更重要那么就要谨慎选择了。</p><p>原理说明：</p><p><strong><em>1、定义和计算输入空间数据概率分布p</em></strong></p><p>条件概率分布定义为： <span class="math display">\[p(i|j)=e^{\frac{-\max(0,d(x_i,x_j)-\rho_i)}{\sigma_i}}\]</span> 以上的计算只需要输入节点的<span class="math inline">\(n\)</span>个邻居节点（这也是UMAP比t-SNE高效的地方之一），其中: <span class="math inline">\(d(x_i,x_j)\)</span>为两点间距离且不一定得是欧氏距离， <span class="math inline">\(\rho_i\)</span>是与<span class="math inline">\(x_i\)</span>距离最近的邻居节点的距离，即： <span class="math display">\[\rho_i=\min(d(x_i,x_j)|1\leq j\leq k,d(x_i,x_j)&gt;0)\]</span> <span class="math inline">\(\sigma_i\)</span>是与t-SNE中<span class="math inline">\(\delta_i\)</span>有类似作用的调节参数，为了满足用户设定的困惑度，由以下式子得到： <span class="math display">\[ \sum\limits_{j=1}^k e^{\frac{-\max(0,d(x_i,x_j)-\rho_i)}{\sigma_i}}=log_2k \]</span></p><p>联合概率分布定义为： <span class="math display">\[p(ij)=p(i|j)+p(j|i)-p(i|j)p(j|i)\]</span></p><p><strong><em>2.定义和计算降维空间数据概率分布q</em></strong></p><p>联合概率分布定义为： <span class="math display">\[q(ij)=(1+a||y_i-y_j||_2^{2b})^{-1}\]</span> 其中a、b为大于0的超参数，默认可取：<span class="math inline">\(a\approx1.93,b\approx0.79\)</span></p><p><strong><em>3.定义损失函数</em></strong> 使用交叉熵为损失函数，定义如下： <span class="math display">\[ C_{UMAP} =\sum\limits_{i\neq j}p(ij)log\frac{p(ij)}{q(ij)}+(1-p(ij))log\frac{1-p(ij)}{1-q(ij)} \]</span></p><p><strong>实现标准库</strong></p><p>UMAP有官方实现的标准库，说明如下：https://umap-learn.readthedocs.io/en/latest/，几个核心超参数含义：</p><p><strong><em>1、n_neighbors</em></strong></p><p>上面公式中的参数<span class="math inline">\(k\)</span>，用来tradeoff局部结构信息和全局结构信息，取值越小，越注重局部结构信息，取值范围[2,200]，默认最佳实践为15。</p><p><strong><em>2、min_dist</em></strong></p><p>控制降维后空间中点之间的紧密程度，取值越大，越保留数据原有拓扑结构，它的取值会影响上述公式中<span class="math inline">\(a,b\)</span>的值，取值范围[0.0,0.99]，默认最佳实践为0.1。</p><p><strong><em>3、n_components</em></strong> 整数值，用于设定降维空间的维度，取值范围[2,100]，为方便可视化，可以取2或3。</p><p>此外metric参数用于指定距离公式，支持23种距离，为了让每次降维后结果都一样，需要给参数random_state赋值。</p><p>一个手写体数字识别的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"><span class="keyword">import</span> umap.plot</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用sklearn直接导入手写体数据集.</span></span><br><span class="line">digits = load_digits()</span><br><span class="line"><span class="comment"># 打印部分数据集以便直观查看.</span></span><br><span class="line">f, a = plt.subplots(<span class="number">15</span>, <span class="number">15</span>)</span><br><span class="line">axes = a.flatten()</span><br><span class="line"><span class="keyword">for</span> i, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(axes):</span><br><span class="line">    item.imshow(digits.images[i], cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.setp(axes, xticks=[], yticks=[], frame_on=<span class="literal">False</span>)</span><br><span class="line">plt.tight_layout(h_pad=<span class="number">0.05</span>, w_pad=<span class="number">0.02</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># UMAP降维.</span></span><br><span class="line">mapper = umap.UMAP(n_neighbors=<span class="number">15</span>,</span><br><span class="line">                   n_components=<span class="number">2</span>,</span><br><span class="line">                   min_dist=<span class="number">0.1</span>,</span><br><span class="line">                   random_state=<span class="number">2022</span>).fit(digits.data)</span><br><span class="line"><span class="comment"># 可视化降维结果.</span></span><br><span class="line">q = umap.plot.points(mapper, labels=digits.target)</span><br><span class="line">umap.plot.show(q)</span><br></pre></td></tr></table></figure><p></p><p>原始数据集如下，包含0~9的10种手写体数据：</p><center><img data-src="https://vivounicorn.github.io/images/bertopic/digits.png" width="600"></center><p>降维结果如下：</p><center><img data-src="https://vivounicorn.github.io/images/bertopic/umap.png" width="600"></center><p>可以看到，相同的手写体数字在降维后大部分会聚在一起。</p></li></ul><h3 id="hdbscan聚类">HDBSCAN聚类</h3><p>降维算法是无监督学习，所以通过UMAP实际得到的应该如下图：</p><center><img data-src="https://vivounicorn.github.io/images/bertopic/umap_g.png" width="600"></center><p>接下来就需要用聚类算法尽可能的把数据中高内聚低耦合的规律找出来，聚类算法大致分为基于扁平或层次结构的聚类和基于质心或密度的聚类，当然两类也可以交叉。扁平聚类可以看做层次聚类的特例，它们有点像决策树，更关注层次关系，典型的例子是：“国家-&gt;省-&gt;市-&gt;县...”这种层次关系的发现， 质心聚类更关注找到每个以质心为中心的“球状”的聚类，密度聚类则可以处理不规则形状的聚类。如果结合密度聚类和层次聚类，就可以得到既能处理不规则形状又能得到解释性比较好还能方便识别异常点的聚类算法，HDBSCAN（Hierarchical Density-Based Spatial Clustering of Applications with Noise）就是一个这类算法。</p><p>还以digits数据集为例，UMAP降维后接着做HDBSCAN聚类，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"><span class="keyword">import</span> umap.plot</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> hdbscan</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line">digits = load_digits()</span><br><span class="line"></span><br><span class="line">mapper = umap.UMAP(n_neighbors=<span class="number">15</span>,</span><br><span class="line">                   n_components=<span class="number">2</span>,</span><br><span class="line">                   min_dist=<span class="number">0.1</span>,</span><br><span class="line">                   random_state=<span class="number">2022</span>).fit_transform(digits.data)</span><br><span class="line"></span><br><span class="line">clusterer = hdbscan.HDBSCAN(min_cluster_size=<span class="number">15</span>,</span><br><span class="line">                            metric=<span class="string">&#x27;manhattan&#x27;</span>,</span><br><span class="line">                            cluster_selection_method=<span class="string">&#x27;eom&#x27;</span>,</span><br><span class="line">                            prediction_data=<span class="literal">True</span>)</span><br><span class="line">clusterer.fit(mapper)</span><br><span class="line"></span><br><span class="line">ids = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> clusterer.labels_:</span><br><span class="line">    <span class="keyword">if</span> x <span class="keyword">not</span> <span class="keyword">in</span> ids.keys():</span><br><span class="line">        ids[x] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">color_palette = sns.color_palette(<span class="string">&#x27;bright&#x27;</span>, <span class="built_in">len</span>(ids))</span><br><span class="line"></span><br><span class="line">cluster_colors = [color_palette[x] <span class="keyword">if</span> x &gt;= <span class="number">0</span></span><br><span class="line">                  <span class="keyword">else</span> (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>)</span><br><span class="line">                  <span class="keyword">for</span> x <span class="keyword">in</span> clusterer.labels_]</span><br><span class="line">cluster_member_colors = [sns.desaturate(x, p) <span class="keyword">for</span> x, p <span class="keyword">in</span></span><br><span class="line">                         <span class="built_in">zip</span>(cluster_colors, clusterer.probabilities_)]</span><br><span class="line"></span><br><span class="line">plt.scatter(mapper[:, <span class="number">0</span>], mapper[:, <span class="number">1</span>], s=<span class="number">50</span>, linewidth=<span class="number">0</span>, c=cluster_member_colors, alpha=<span class="number">0.25</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>按照不同聚类id着色后如下：</p><center><img data-src="https://vivounicorn.github.io/images/bertopic/cluster.png" width="800"></center><p>层次结构如下：</p><center><img data-src="https://vivounicorn.github.io/images/bertopic/h.png" width="800"></center><p><strong><em>HDBSCAN的数据假设</em></strong></p><p>所有机器学习算法背后都有假设，甚至所有方法背后都有假设，基于这些假设方法才有效，HDBSCAN对数据做了以下假设：</p><p>1、数据聚集簇（cluster）的形状不规则，有扁的、有长条的（如前面的digits数据集）</p><p>2、簇的大小不一，有的胖有的瘦</p><p>3、簇的密度不一，有的稠密，有的稀疏</p><p>4、存在噪声点</p><p>5、数据分布可以看做是多个不同概率分布的叠加</p><p><strong><em>基于密度的聚类</em></strong></p><p>怎么理解基于密度的聚类呢？举个例子：假设数据服从某个二元高斯分布<span class="math inline">\(f\)</span>，它的数据散点图和高斯分布的pdf关系如图：</p><center><img data-src="https://vivounicorn.github.io/images/bertopic/gd.png" width="800"></center><p>如果把每个维度投影，可以看到边缘分布是高斯分布，它的“山峰”处的数据比较稠密，反之“山谷”处的数据比较稀疏，同时pdf中概率越大的地方对应的散点图区域越稠密。 可以推广到数据有多个“山峰”和多个“山谷”，例如以下数据集<span class="math inline">\(A\)</span>，由三个高斯分布叠加生成：</p><center><img data-src="https://vivounicorn.github.io/images/bertopic/gd1.png" width="800"></center><p>基于密度的聚类就是把这些“山谷”找出来作为边界，从而得到若干个相对独立的“山头”。</p><p><strong><em>层次密度聚类</em></strong></p><p>假设数据分布为<span class="math inline">\(f\)</span>，我们需要一个阈值<span class="math inline">\(\lambda\)</span>来确定到底应该聚几类，如图：</p><center><img data-src="https://vivounicorn.github.io/images/bertopic/d1.png" width="800"></center><p>当<span class="math inline">\(\lambda=\lambda_0\)</span>时，数据被聚成2类，当<span class="math inline">\(\lambda=\lambda_1\)</span>时则是3类，<span class="math inline">\(\lambda\)</span>是聚类的超参数，那么怎么确定这个值呢？《<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1705.07321.pdf">Accelerated Hierarchical Density Clustering</a>》一文介绍了一种叫<strong>eom</strong>（excess of mass）的算法，大致原理如下：</p><p>1、excess of mass值定义为：<span class="math display">\[E(C,\lambda) = \int_{C_λ}(f(x) − \lambda)dx\]</span> 其中<span class="math inline">\(C\)</span>为<span class="math inline">\(f\)</span>域的子集：$ C_{} = {x C | f(x) λ}$。</p><p>2、当以<span class="math inline">\(λ_{C_i}\)</span>作为阈值划分出类簇<span class="math inline">\(C_i\)</span>时，如果<span class="math inline">\(λ_{min}(C_i)\)</span>为能够划分出<span class="math inline">\(C_i\)</span>这个类簇的最小阈值时，它的eom定义为： <span class="math display">\[ E(C_i) = \int_{C_i}(f(x) − \lambda_{min}(C_i))dx \]</span></p><p>3、如果<span class="math inline">\(λ_{max}(C_i)\)</span>为能够划分出<span class="math inline">\(C_i\)</span>这个类簇的最大阈值，则定义其reom（relative excess of mass）值为： <span class="math display">\[ E_R(C_i) = \int_{C_i}(min(f(x),λ_{max}(C_i)) − λ_{min}(C_i))dx \]</span></p><p>如果<span class="math inline">\(C_i\)</span>有多个子树<span class="math inline">\(C_{i_1},C_{i_2},C_{i_3},...,C_{i_k}\)</span>，则reom值为： <span class="math display">\[ E_R(C_i) = E(C_i) −\sum\limits_{j=1}^kE(C_{i_j}) \]</span></p><center><img data-src="https://vivounicorn.github.io/images/bertopic/d2.png" width="800"></center><p>由于真实数据的pdf函数<span class="math inline">\(f\)</span>是未知的，所以需要通过已有数据做经验密度估计，方法简单描述如下： 对任何一个点<span class="math inline">\(j\)</span>，以<span class="math inline">\(\epsilon\)</span>为半径画“圆/球”邻域，数下里面包含的邻居节点个数，个数越多，意味着此处越稠密，对应的pdf值越大，反之亦然。 反过来换个角度，假设指定<span class="math inline">\(k\)</span>近邻的参数<span class="math inline">\(k=k_0\)</span>，找到某点<span class="math inline">\(j\)</span>可以包含<span class="math inline">\(k_0\)</span>个邻居节点的最小的“圆/球”邻域，那么这个半径<span class="math inline">\(\epsilon_{x_j}\)</span>就可以作为pdf的估计，所以可以定义<span class="math inline">\(f(x_j)\)</span>的经验pdf为： <span class="math display">\[ \hat{f}(x_j)=\frac{1}{\epsilon_{x_j}} \]</span></p><p>由于此时经验分布是离散的，所以上述eom改写为： <span class="math display">\[ E(C_i) = \sum\limits_{x_j\in C_i}(\hat{f}(x_j) − \lambda_{min}(C_i)) \]</span></p><p>reom为： <span class="math display">\[ \sigma(C_i) = E(C_i) −\sum\limits_{j=1}^kE(C_{i_j}) \]</span></p><p>最后求解一个让所有类簇的reom之和最大的约束最优化问题即可，即： <span class="math display">\[ \begin{eqnarray} max&amp;\sum\limits_{i\in \{1,2,...,n\}} \sigma(C_i) &amp; \\ s.t.&amp;C_i \cap C_j=\emptyset &amp;\quad where \quad i\neq j \end{eqnarray} \]</span></p><p>详情可以参考论文《<a target="_blank" rel="noopener" href="https://repositorio.usp.br/bitstream/handle/BDPI/51005/2709770.pdf?sequence=1">Hierarchical density estimates for data clustering, visualization, and outlier detection</a>》。</p><p>层次结构示意：</p><center><img data-src="https://vivounicorn.github.io/images/bertopic/d3.png" width="800"></center><p>使用HDBSCAN的库可以方便的完成以上过程，数据集<span class="math inline">\(A\)</span>的层次聚类如下：</p><center><img data-src="https://vivounicorn.github.io/images/bertopic/gd5.png" width="600"></center><h3 id="主题标题生成">主题标题生成</h3><p>主题标题生成采用改进的TF-IDF算法（c-TF-IDF），作为比较，原本的TF-IDF算法如下： <span class="math display">\[ W_{t,d}=tf_{t,d}\cdot \log(\frac{N}{df_t}) \]</span> 其中，<span class="math inline">\(tf_{t,d}\)</span>是词<span class="math inline">\(t\)</span>在文档<span class="math inline">\(d\)</span>中的词频，<span class="math inline">\(df_t\)</span>是词<span class="math inline">\(t\)</span>出现在多少的文档中，<span class="math inline">\(N\)</span>是总文档数。</p><p>c-TF-IDF算法如下： <span class="math display">\[ W_{t,c}=tf_{t,c}\cdot \log(1+\frac{A}{tf_t}) \]</span> 其中，所有属于类簇<span class="math inline">\(c\)</span>的文档被看做一个大文档，<span class="math inline">\(tf_{t,c}\)</span>是词<span class="math inline">\(t\)</span>在这个大文档中的词频，<span class="math inline">\(tf_t\)</span>是词<span class="math inline">\(t\)</span>出现在多少个类簇中，<span class="math inline">\(A\)</span>是所有类簇的平均单词数量（即单词总数/类簇个数，注意，为了让<span class="math inline">\(\log\)</span>函数结果是正数，所以需要加个1），最后，对每个类簇，选出权重最高的<strong>top n</strong>个词作为该主题的标题。</p><h2 id="bertopic代码实践">BERTopic代码实践</h2><p>以搜狐新闻为语料做简单演示，核心类如下，完整代码在<a target="_blank" rel="noopener" href="https://github.com/vivounicorn/BERTopic">这里</a>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CCTopic</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                 embedding_model_name: <span class="built_in">str</span> = <span class="string">&#x27;DMetaSoul/sbert-chinese-general-v2-distill&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 backend: <span class="built_in">str</span> = <span class="string">&#x27;sentence_transformers&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 min_topic_size=<span class="number">10</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 nr_topics=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 threshold: <span class="built_in">float</span> = <span class="number">0.015</span></span></span></span><br><span class="line"><span class="params"><span class="function">                 </span>):</span></span><br><span class="line">        <span class="comment"># &#x27;paraphrase-distilroberta-base-v1&#x27;</span></span><br><span class="line">        <span class="comment"># &#x27;DMetaSoul/sbert-chinese-general-v2-distill&#x27;</span></span><br><span class="line">        <span class="comment"># 使用开箱即用的预训练模型，也可根据自有数据做fine-tuning.</span></span><br><span class="line">        <span class="keyword">if</span> backend == <span class="string">&#x27;sentence_transformers&#x27;</span>:</span><br><span class="line">            self.embedding_model = SentenceTransformer(embedding_model_name)</span><br><span class="line">        <span class="keyword">elif</span> backend == <span class="string">&#x27;pretrained&#x27;</span>:</span><br><span class="line">            self.tokenizer = BertTokenizer.from_pretrained(embedding_model_name)</span><br><span class="line">            self.embedding_model = BertModel.from_pretrained(embedding_model_name)</span><br><span class="line"></span><br><span class="line">        self.min_topic_size = min_topic_size</span><br><span class="line">        self.topic_threshold = threshold</span><br><span class="line">        self.documents = <span class="literal">None</span></span><br><span class="line">        self.topic_model = <span class="literal">None</span></span><br><span class="line">        self.topic_probs = <span class="literal">None</span></span><br><span class="line">        self.topics = <span class="literal">None</span></span><br><span class="line">        self.nr_topics = nr_topics</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit_model</span>(<span class="params">self, doc_path=<span class="string">&#x27;data/dx_txt/all1_cut.dat&#x27;</span>, save_path=<span class="string">&#x27;models/distill.dat&#x27;</span></span>):</span></span><br><span class="line">        self.documents = fetch_corpus(doc_path)</span><br><span class="line">        stable_umap = UMAP(n_neighbors=<span class="number">15</span>,</span><br><span class="line">                           n_components=<span class="number">5</span>,</span><br><span class="line">                           min_dist=<span class="number">0.0</span>,</span><br><span class="line">                           metric=<span class="string">&#x27;cosine&#x27;</span>,</span><br><span class="line">                           random_state=<span class="number">2022</span>)</span><br><span class="line"></span><br><span class="line">        self.topic_model = BERTopic(language=<span class="string">&quot;chinese (simplified)&quot;</span>,</span><br><span class="line">                                    min_topic_size=self.min_topic_size,</span><br><span class="line">                                    umap_model=stable_umap,</span><br><span class="line">                                    calculate_probabilities=<span class="literal">True</span>,</span><br><span class="line">                                    nr_topics=self.nr_topics,</span><br><span class="line">                                    verbose=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        s_embeddings = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            s_embeddings = self.embedding_model.encode(self.documents, show_progress_bar=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="built_in">print</span>(self.documents)</span><br><span class="line"></span><br><span class="line">        logger.info(<span class="string">&#x27;Begin training...&#x27;</span>)</span><br><span class="line">        self.topics, self.topic_probs = self.topic_model.fit_transform(self.documents, s_embeddings)</span><br><span class="line">        logger.info(<span class="string">&#x27;End training.&#x27;</span>)</span><br><span class="line">        logger.info(self.topic_model.get_topic_info())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成主题对应的标题文档</span></span><br><span class="line">        topics_docs = pd.DataFrame(&#123;<span class="string">&#x27;topic&#x27;</span>: self.topics, <span class="string">&#x27;doc&#x27;</span>: self.documents&#125;)</span><br><span class="line">        topics_docs.to_excel(<span class="string">&#x27;topic.xlsx&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        self.topic_model.save(save_path)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">visualize</span>(<span class="params">self, n</span>):</span></span><br><span class="line">        bar_chart = self.topic_model.visualize_barchart(top_n_topics=<span class="number">300</span>)</span><br><span class="line">        bar_chart.write_html(<span class="string">&#x27;figures/bar_chart.html&#x27;</span>)</span><br><span class="line">        logger.info(self.topic_model.get_topic(<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">        first_new_topic_probs = self.topic_model.visualize_distribution(self.topic_probs[n], min_probability=<span class="number">0</span>)</span><br><span class="line">        logger.info(<span class="string">&#x27;document content &#x27;</span> + self.documents[n])</span><br><span class="line">        first_new_topic_probs.write_html(<span class="string">&#x27;figures/first_new_topic_probs.html&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        visualize_topics1 = self.topic_model.visualize_topics()</span><br><span class="line">        <span class="comment"># 可视化结果保存至html中，可以动态显示信息</span></span><br><span class="line">        visualize_topics1.write_html(<span class="string">&#x27;figures/distance.html&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        visualize_hierarchy = self.topic_model.visualize_hierarchy()</span><br><span class="line">        visualize_hierarchy.write_html(<span class="string">&#x27;figures/hierarchy.html&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inference</span>(<span class="params">self, docs, topn=<span class="number">5</span>, min_probability=<span class="number">0.00</span>, model_path=<span class="string">&#x27;models/distill.dat&#x27;</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.topic_model <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.topic_model = BERTopic.load(model_path)</span><br><span class="line"></span><br><span class="line">        can_embeddings = self.embedding_model.encode(docs)</span><br><span class="line">        topics, probs = self.topic_model.transform(docs, can_embeddings)</span><br><span class="line">        vis_probs = self.topic_model.visualize_distribution(probs[<span class="number">0</span>], min_probability=min_probability)</span><br><span class="line">        vis_probs.write_html(<span class="string">&#x27;figures/vis_probs.html&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        self.topic_threshold = <span class="number">0</span></span><br><span class="line">        selected_topics = np.argwhere(probs[<span class="number">0</span>] &gt;= self.topic_threshold).flatten()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> topn &gt;= <span class="built_in">len</span>(selected_topics) &gt; <span class="number">0</span>:</span><br><span class="line">            cut_probs = probs[<span class="number">0</span>][selected_topics]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            selected_topics = np.argpartition(probs[<span class="number">0</span>], -topn, axis=<span class="literal">None</span>)[-topn:]</span><br><span class="line">            cut_probs = probs[<span class="number">0</span>][selected_topics]</span><br><span class="line"></span><br><span class="line">        selected_topics = selected_topics[np.argsort(-cut_probs)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> topics, [(self.topic_model.topic_names[t], probs[<span class="number">0</span>][t]) <span class="keyword">for</span> t <span class="keyword">in</span> selected_topics]</span><br></pre></td></tr></table></figure><p></p><p>BERTopic库可视化方面很丰富，展示几个常用的：</p><p>1、展示每个topic里权重最高的前五个词</p><center><img data-src="https://vivounicorn.github.io/images/bertopic/de1.png" width="600"></center><p>2、可视化展示topic的类簇分布</p><center><img data-src="https://vivounicorn.github.io/images/bertopic/de2.png" width="600"></center><p>3、可视化层次结构</p><center><img data-src="https://vivounicorn.github.io/images/bertopic/de3.png" width="800"></center><p>Topic的生成结果还是很赞的。</p><h2 id="总结">总结</h2><p>BERTopic的组件化结构设计非常好，不管是用于文档嵌入的Transformers、降维的UMAP、聚类的HDBSCAN还是主题标题提取的c-TF-IDF，每个组件都可以用这个领域的STOA算法，未来升级也很方便，深入了解它的各个组件，也会帮助你在应用时更有感觉。</p></div><div class="popular-posts-header">相关文章推荐</div><ul class="popular-posts"><li class="popular-posts-item"><div class="popular-posts-title"><a href="/article/d677b2e0.html" rel="bookmark">机器学习与人工智能技术分享-第三章 机器学习中的统一框架</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/article/ad2261a2.html" rel="bookmark">机器学习与人工智能技术分享-第四章 最优化原理</a></div></li></ul><div class="followme"><p>欢迎关注我的其它发布渠道</p><div class="social-list"><div class="social-item"><a target="_blank" class="social-link" href="https://vivounicorn.github.io/images/wx.jpeg"><span class="icon"><i class="fab fa-weixin"></i> </span><span class="label">WeChat</span></a></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a> <a href="/tags/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/" rel="tag"># 主题模型</a></div><div class="post-nav"><div class="post-nav-item"><a href="/article/d0a635cc.html" rel="prev" title="0-1规划及其现实应用(0-1 Programming)"><i class="fa fa-chevron-left"></i> 0-1规划及其现实应用(0-1 Programming)</a></div><div class="post-nav-item"><a href="/article/2675d8.html" rel="next" title="非线性最小二乘(Nonlinear Least Squares)">非线性最小二乘(Nonlinear Least Squares) <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener("tabs:register",()=>{let e=CONFIG.comments["activeClass"];if(CONFIG.comments.storage&&(e=localStorage.getItem("comments_active")||e),e){let t=document.querySelector(`a[href="#comment-${e}"]`);t&&t.click()}}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{t.target.matches(".tabs-comment .tab-content .tab-pane")&&(t=t.target.classList[1],localStorage.setItem("comments_active",t))})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#bertopic%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3"><span class="nav-text">BERTopic主题模型详解</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#bertopic%E5%9F%BA%E6%9C%AC%E8%BF%87%E7%A8%8B"><span class="nav-text">BERTopic基本过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%AD%E4%B9%89embedding"><span class="nav-text">语义Embedding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%99%8D%E7%BB%B4"><span class="nav-text">降维</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hdbscan%E8%81%9A%E7%B1%BB"><span class="nav-text">HDBSCAN聚类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BB%E9%A2%98%E6%A0%87%E9%A2%98%E7%94%9F%E6%88%90"><span class="nav-text">主题标题生成</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bertopic%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5"><span class="nav-text">BERTopic代码实践</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-text">总结</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="张磊" src="https://vivounicorn.github.io/images/wali.png"><p class="site-author-name" itemprop="name">张磊</p><div class="site-description" itemprop="description"></div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">18</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">13</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">51</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="sidebar-button motion-element"><i class="fa fa-comment"></i> Chat</div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/vivounicorn" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;vivounicorn" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:zhangleisuper@gmail.com" title="E-Mail → mailto:zhangleisuper@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a> </span><span class="links-of-author-item"><a href="https://weibo.com/vivounicorn" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;vivounicorn" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-block"><div class="links-of-blogroll-title recent-posts-title"><i class="fa fa-history" aria-hidden="true"></i> 近期文章</div><ul class="links-of-blogroll-list recent-posts-list"><li class="my-links-of-blogroll-item"><a href="/article/2675d8.html" title="非线性最小二乘(Nonlinear Least Squares)" target="" style="display:block;text-align:left">☺ 非线性最小二乘(Nonlinear Least Squares)</a></li><li class="my-links-of-blogroll-item"><a href="/article/fe514cfb.html" title="BERTopic主题模型详解" target="" style="display:block;text-align:left">☺ BERTopic主题模型详解</a></li><li class="my-links-of-blogroll-item"><a href="/article/d0a635cc.html" title="0-1规划及其现实应用(0-1 Programming)" target="" style="display:block;text-align:left">☺ 0-1规划及其现实应用(0-1 Programming)</a></li><li class="my-links-of-blogroll-item"><a href="/article/1b1480ce.html" title="机器学习中的自动微分(Automatic Differentiation)" target="" style="display:block;text-align:left">☺ 机器学习中的自动微分(Automatic Differentiation)</a></li><li class="my-links-of-blogroll-item"><a href="/article/c0a9d987.html" title="机器学习与人工智能技术分享-第十章-Vision Transformers (ViT)" target="" style="display:block;text-align:left">☺ 机器学习与人工智能技术分享-第十章-Vision Transformers (ViT)</a></li></ul></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">京ICP备2022027092号-1 </a><img src="https://beian.miit.gov.cn/#/Integrated/index" style="display:inline-block"></div><div class="copyright">&copy; <span itemprop="copyrightYear">2022</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">张磊</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span title="站点总字数">584k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="站点阅读时长">8:51</span></div></div></footer></div><script src="//cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js"></script><script src="//cdn.bootcdn.net/ajax/libs/jquery/3.6.0/jquery.min.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js"></script><script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script>!function(){var e,t,o,n,r=document.getElementsByTagName("link");if(0<r.length)for(i=0;i<r.length;i++)"canonical"==r[i].rel.toLowerCase()&&r[i].href&&(e=r[i].href);t=(e||window.location.protocol).split(":")[0],e=e||window.location.href,window,o=e,n=document.referrer,/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(o)||(t="https"===String(t).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif",n?(t+="?r="+encodeURIComponent(document.referrer),o&&(t+="&l="+o)):o&&(t+="?l="+o),(new Image).src=t)}()</script><script src="/js/local-search.js"></script><script>"undefined"==typeof MathJax?(window.MathJax={loader:{load:["[tex]/mhchem"],source:{"[tex]/amsCd":"[tex]/amscd","[tex]/AMScd":"[tex]/amscd"}},tex:{inlineMath:{"[+]":[["$","$"]]},packages:{"[+]":["mhchem"]},tags:"ams"},options:{renderActions:{findScript:[10,d=>{document.querySelectorAll('script[type^="math/tex"]').forEach(e=>{var t=!!e.type.match(/; *mode=display/);const a=new d.options.MathItem(e.textContent,d.inputJax[0],t);t=document.createTextNode("");e.parentNode.replaceChild(t,e),a.start={node:t,delim:"",n:0},a.end={node:t,delim:"",n:0},d.math.push(a)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}},function(){var e=document.createElement("script");e.src="//cdnjs.cloudflare.com/ajax/libs/mathjax/3.0.5/es5/tex-mml-chtml.js",e.defer=!0,document.head.appendChild(e)}()):(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset())</script><script>NexT.utils.loadComments(document.querySelector("#valine-comments"),()=>{NexT.utils.getScript("//cdnjs.cloudflare.com/ajax/libs/valine/1.3.10/Valine.min.js",()=>{var i=["nick","mail","link"],e="nick,mail,link".split(",").filter(e=>i.includes(e));new Valine({el:"#valine-comments",verify:!1,notify:!0,appId:"m8FPP0CqMpxyTuUvaVOX9qVV-gzGzoHsz",appKey:"Ori6X9PXqQyURvwgl7HT5TJj",placeholder:"赠人玫瑰，手有余香",avatar:"mm",meta:e,pageSize:"10",visitor:!0,lang:"zh-cn",path:location.pathname,recordIP:!1,serverURLs:""})},window.Valine)})</script></body></html>