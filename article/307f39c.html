<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><meta name="google-site-verification" content="-l60HPLrjDNbr3Ni1wLsNkiKiCWUAmxiC_ObB8vNMF0"><meta name="msvalidate.01" content="AF3396A141E1B198CA1BE76915B3969F"><meta name="yandex-verification" content="ee8492bd2e7708db"><meta name="baidu-site-verification" content="code-OBKi1CbRLy"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css"><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"vivounicorn.github.io",root:"/",scheme:"Muse",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"always",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:{enable:!0,onlypost:!1,loadingImg:"./images/loading.gif",isSPA:!1,preloadRatio:3},pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="本章对方差和偏差、损失函数、常用的机器学习模型（包括：线性回归、支持向量机、逻辑回归、GBDT、CatBoost、RF）等做了回顾。"><meta property="og:type" content="article"><meta property="og:title" content="机器学习与人工智能技术分享-第二章 建模方法回顾"><meta property="og:url" content="https://vivounicorn.github.io/article/307f39c.html"><meta property="og:site_name" content="业精于勤，荒于嬉；行成于思，毁于随。"><meta property="og:description" content="本章对方差和偏差、损失函数、常用的机器学习模型（包括：线性回归、支持向量机、逻辑回归、GBDT、CatBoost、RF）等做了回顾。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1aoqq68m81tb5rsd1k71llid5k13.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1aoqq68m81tb5rsd1k71llid5k13.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1aoqrsu9k1u936ff1gps16anac61g.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1aoif9nr11ft1j95hca71a1a9dm.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1aonhhgoo83ei2ca021k5vfc11j.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1aont3ghm1g1e18qk18b492k1nd20.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1asr9qkai1mseu8n13i8ele1sf89.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1astp4htfueu10v51ciq1ab32sr9.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1ap1pq34s184n18stpt413bq12at9.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1ap2hfjae8q6nta1ppd10l1fkv1g.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1ap2e2geqo3a11ur18od2b413ek13.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1ap2m8teo1kqg1up8mtvccpfns2a.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1ap2oflv078q1h631peataq13ih2n.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/gbdt.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1ap5297q1nte1tkpb3u1lfr1pk45c.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1b0s1udkl1att1v8o1hfs11im1eepm.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1apkqel8gmtlervoud3k1ad2m.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1apme7kvi5klumr1cig181c2vm9.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1e366blci7k512pq1vduq2v6519.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1e364p1q9rkocvq1juo1cs96jq9.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1e36utda26u3k4712t5d8d10001g.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1e3er90lfo882vu1v234415gu1j.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/a2.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1e3jn8bq91vlofsk3tc1lb516h22c.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1e3jnra3u19v71ec21djqff31mtt2p.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1ap79lfscsq013g11c4g1aqu1jb4m.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1ap6vqg8k1nursgn1kv541nevd9.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1ap79nd0j1m5cuhbv0ltog1ao313.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1ap7b1i9s156h8n91noe1v0b1kk41g.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1ap7hp4el195d12al1d2119h3r8e46.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1asuka9jl1u479sd1k7l64l11s2m.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1ap7k2i4u1qeq14871ge613np5r5d.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1ap7i6rfq1ua917bf12llgol1vcs50.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1ap7kf21k8c2me14dr1sd1t5d67.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1ap7kod2dpkn1cqu1oksvmd3ms6t.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1ap87o3no141a1tnvid984h1vnv7n.png"><meta property="article:published_time" content="2021-09-05T06:44:00.000Z"><meta property="article:modified_time" content="2022-01-04T01:54:11.190Z"><meta property="article:author" content="张磊"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="Bagging"><meta property="article:tag" content="Boosting"><meta property="article:tag" content="第二章"><meta property="article:tag" content="LR"><meta property="article:tag" content="SVM"><meta property="article:tag" content="ATM"><meta property="article:tag" content="RF"><meta property="article:tag" content="GBDT"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://vivounicorn.github.io/images/ai_chapter_2/image_1aoqq68m81tb5rsd1k71llid5k13.png"><link rel="canonical" href="https://vivounicorn.github.io/article/307f39c.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>机器学习与人工智能技术分享-第二章 建模方法回顾 | 业精于勤，荒于嬉；行成于思，毁于随。</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">业精于勤，荒于嬉；行成于思，毁于随。</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">花晨月夕</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>站点地图</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div><div><img itemprop="image" src="https://vivounicorn.github.io/images/background.jpg"></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://vivounicorn.github.io/article/307f39c.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="https://vivounicorn.github.io/images/wali.png"><meta itemprop="name" content="张磊"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="业精于勤，荒于嬉；行成于思，毁于随。"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">机器学习与人工智能技术分享-第二章 建模方法回顾</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-09-05 14:44:00" itemprop="dateCreated datePublished" datetime="2021-09-05T14:44:00+08:00">2021-09-05</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a> </span></span><span id="/article/307f39c.html" class="post-meta-item leancloud_visitors" data-flag-title="机器学习与人工智能技术分享-第二章 建模方法回顾" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/article/307f39c.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/article/307f39c.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>59k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>53 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1aoqq68m81tb5rsd1k71llid5k13.png" width="266"> 本章对方差和偏差、损失函数、常用的机器学习模型（包括：线性回归、支持向量机、逻辑回归、GBDT、CatBoost、RF）等做了回顾。 <span id="more"></span></p><h1 id="建模方法回顾">2. 建模方法回顾</h1><p>以通用的监督学习为例，基本包含4个部分:</p><p><span class="math display">\[ \begin{array}{l} \text{1. Prediction: }y_i=f(x_i|w),~i=1,2,.....\\ \text{2. Parameters: }w=\{w_i|i=1,2,...,dim\}\\ \text{3. Objective function: }obj(w)=loss(w)+reg(w)\\ \text{4. Optimization: }min~obj(w) \text{ with(out) constraint.}\\ \end{array} \]</span></p><h2 id="偏差与方差">2.0 偏差与方差</h2><blockquote><ul><li>在机器学习算法中，偏差是由先验假设的不合理带来的模型误差，高偏差会导致<strong>欠拟合</strong>： 所谓欠拟合是指对特征和标注之间的因果关系学习不到位，导致模型本身没有较好的学到历史经验的现象；</li><li>方差表征的是模型误差对样本发生一定变化时的敏感度，高方差会导致<strong>过拟合</strong>：模型对训练样本中的随机噪声也做了拟合学习，导致在未知样本上应用时出现效果较差的现象；</li><li>机器学习模型的核心之一在于其推广能力，即在未知样本上的表现。</li></ul></blockquote>对方差和偏差的一种直观解释:<center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1aoqq68m81tb5rsd1k71llid5k13.png" width="400"></center><p>一个例子，假如我们有预测模型:</p><p><span class="math display">\[ \begin{array}{l} y=f(x)+\epsilon\\ \epsilon \sim N(0,\sigma) \end{array} \]</span></p><p>我们希望用 <span class="math inline">\(f^{e}(x)\)</span> 估计 <span class="math inline">\(f(x)\)</span>，如果使用基于square loss 的线性回归，则误差分析如下: <span class="math display">\[ \begin{align*} Err(x)&amp;=E[(y-f^{e}(x))^2]\\ &amp;=E[(f(x)-f^{e}(x))^2]+\sigma_e^2\\ &amp;=[f(x)]^2-2f(x)E[f^{e}(x)]+E[f^{e}(x)^2]+\sigma_e^2\\ &amp;=E[f^{e}(x)]^2-2f(x)E[f^{e}(x)]+[f(x)]^2\\ &amp;+E[f^{e}(x)^2]-2E[f^{e}(x)]^2+E[f^{e}(x)]^2+\sigma_e^2\\ &amp;=E[f^{e}(x)]^2-2f(x)E[f^{e}(x)]+[f(x)]^2\\ &amp;+E[f^{e}(x)^2-2f^{e}(x)E[f^{e}(x)]+E[f^{e}(x)]^2]+\sigma_e^2\\ &amp;=\underbrace{(E[f^{e}(x)]-f(x))^2}_{Bias^2}+\underbrace{E[(f^{e}(x)-E[f^{e}(x)])^2]}_{Variance}+\sigma_e^2\\ \end{align*} \]</span></p><center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1aoqrsu9k1u936ff1gps16anac61g.png" width="400"></center><p>所以大家可以清楚的看到模型学习过程其实就是对偏差和方差的折中过程。</p><h2 id="线性回归-linear-regression">2.1 线性回归-Linear Regression</h2><center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1aoif9nr11ft1j95hca71a1a9dm.png" width="400"></center><center>简单线性回归</center><h3 id="模型原理">2.1.1 模型原理</h3><p>标准线性回归通过对自变量的线性组合来预测因变量，组合自变量的权重通过最小化训练集中所有样本的预测平方误差和来得到，原理如下。</p><ul><li>预测函数</li></ul><p><span class="math display">\[ \tilde y_i=\sum_{i=1}^N w^Tx_i\]</span></p><ul><li>参数学习－采用最小二乘法</li></ul><p><span class="math display">\[min~\frac{1}{2}\sum_{i=1}^N(y_i-\tilde y_i)^2\]</span></p><p>所有机器学习模型的成立都会有一定的先验假设，线性回归也不例外，它对数据做了以下强假设:</p><ul><li><p>自变量相互独立，无多重共线性</p></li><li><p>因变量是自变量的线性加权组合：</p></li></ul><p><span class="math display">\[y=w^Tx+\epsilon\]</span></p><ul><li>所有样本独立同分布(iid)，且误差项服从以下分布：</li></ul><p><span class="math display">\[\epsilon \sim N(0,\sigma^2)\]</span></p><p>最小二乘法与以上假设的关系推导如下: <span class="math display">\[ \begin{array}{l} \because y=w^Tx+\epsilon,~~~~~\epsilon \sim N(0,\sigma^2)\\ \therefore p(y|x) = N(w^Tx,\sigma^2)\\ \Rightarrow p(y|x) = \frac{1}{\sqrt {2\pi}\sigma} e^{-\frac{(y-w^Tx)^2}{2\sigma^2}} \end{array} \]</span></p><p>使用MLE(极大似然法)估计参数如下: <span class="math display">\[ \begin{array}{l} w=arg~max_w\sum_{i=1}^Nlog~p(y_i|x_i)\\ \Leftrightarrow w=arg~min_w\frac{1}{2}\sum_{i=1}^N{(y_i-w^Tx_i)^2} \end{array} \]</span></p><p>线性回归有两个重要变体：</p><ul><li>Lasso Regression:采用L1正则并使用MAP做参数估计</li><li>Ridge Regression:采用L2正则并使用MAP做参数估计</li></ul><p>关于正则化及最优化后续会做介绍。</p><h3 id="损失函数">2.1.2 损失函数</h3><p><strong>损失函数1 —— Least Square Loss</strong></p><p><span class="math display">\[loss(x)=\frac{1}{2}\sum_{i=1}^N(y_i-\tilde y_i)^2\]</span></p><p>进一步阅读可参考：<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Least_squares">Least Squares</a></p><p>Q: 模型和损失的关系是什么?</p><h2 id="支持向量机-support-vector-machine">2.2 支持向量机-Support Vector Machine</h2><p>支持向量机通过寻找一个分类超平面使得(相对于其它超平面)它与训练集中任何一类样本中最接近于超平面的样本的距离最大。虽然从实用角度讲(尤其是针对大规模数据和使用核函数)并非最优选择，但它是大家理解机器学习的最好模型之一，涵盖了类似偏差和方差关系的泛化理论、最优化原理、核方法原理、正则化等方面知识。</p><h3 id="模型原理-1">2.2.1 模型原理</h3><p>SVM原理可以从最简单的解析几何问题中得到：</p><center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1aonhhgoo83ei2ca021k5vfc11j.png" width="400"></center><p>超平面的定义如下: <span class="math display">\[ \begin{array}{l} y=f(x)=w^Tx+b\\ f(x)=0 \end{array} \]</span></p><p>从几何关系上来看，超平面与数据点的关系如下(以正样本点为例)： <span class="math display">\[ \begin{array}{l} x_i=p_i+\gamma_i\frac{w}{\Arrowvert w\Arrowvert}\\ \text{where the p is point x&#39;s projection on the hyperplane.}\\ \Leftrightarrow w^Tx_i+b=w^Tp_i+b+\gamma_i\frac{w^Tw}{\Arrowvert w\Arrowvert}\\ \because f(p_i)=0\\ \therefore f(x_i)=\gamma_i\Arrowvert w\Arrowvert\\ \Rightarrow \gamma_i=\frac{f(x_i)}{\Arrowvert w\Arrowvert}\\ \text{consider two cases(label=}\pm1\text{)}\\ \Rightarrow \gamma_i=\frac{y_if(x_i)}{\Arrowvert w\Arrowvert}\\ \text{set}~~\tilde{\gamma_i}=y_if(x_i)\\ \Rightarrow \gamma_i=\frac{\tilde{\gamma_i}}{\Arrowvert w\Arrowvert} \end{array} \]</span></p><p>定义几何距离和函数距离分别如下： <span class="math display">\[ \begin{array}{l} \text{relative geometric margin:}~~~~\tilde{\gamma}=min_1^N\tilde{\gamma_i}\\ \text{relative functional margin:}~~~~\gamma=min_1^N\gamma_i \end{array} \]</span></p><p>由于超平面的大小对于SVM求解并不重要，重要的是其方向，所以根据SVM的定义,得到约束最优化问题： <span class="math display">\[ \begin{array}{l} max~~\gamma\\ ~~~st.~y_i\frac{f(x_i)}{\Arrowvert w\Arrowvert}\ge \gamma,~~i=1,2....N\\ \Leftrightarrow \\ max~\frac{\tilde{\gamma}}{\Arrowvert w\Arrowvert}\\ ~~~st.~y_if(x_i)\ge \tilde{\gamma},~~i=1,2....N\\ \text{the value of } \tilde{\gamma}\text{ does not affect the solution of the problem.}\\ \text{to set } \tilde{\gamma}=1.\\ \Leftrightarrow \\ min~\frac{1}{2}\Arrowvert w\Arrowvert^2\\ ~~~st.~y_if(x_i)-1\ge 0,~~i=1,2....N\\ \end{array} \]</span></p>现实当中我们无法保证数据是线性可分的，强制要求所有样本能正确分类是不太可能的，即使做了核变换也只是增加了这种可能性，因此我们又需要做折中，允许误分的情况出现，对误分的样本根据其严重性做惩罚，所以引入松弛变量，将上述问题变成软间隔优化问题。<center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1aont3ghm1g1e18qk18b492k1nd20.png" width="400"></center><p>新的优化问题： <span class="math display">\[ \begin{align*} min&amp;~\frac{1}{2}\Arrowvert w\Arrowvert^2+C\sum_{i=1}^Ng(\xi_i)\\ st.&amp;~y_if(x_i)\ge 1-\xi_i,~~i=1,2....N\\ &amp;\xi_i\ge 0,~~i=1,2....N\\ &amp;g(\xi)=\xi \text{ or } g(\xi)=\xi^2.....\\ \end{align*} \]</span></p><p>如果选择： <span class="math display">\[g(\xi)=\xi\]</span></p><p>那么优化问题变成： <span class="math display">\[ \begin{array}{l} min~\frac{1}{2}\Arrowvert w\Arrowvert^2+C\sum_{i=1}^N\xi_i\\ ~~~st.~y_if(x_i)\ge 1-\xi_i,~~i=1,2....N\\ ~~~~~~~~~\xi_i\ge 0,~~i=1,2....N\\ \end{array} \]</span></p><h3 id="损失函数-1">2.2.2 损失函数</h3><p><strong>损失函数2 —— Hinge Loss</strong></p><p><span class="math display">\[loss(x)=\sum_{i=1}^N[1-y_if(x_i)]_+\]</span></p><p>使用hinge loss将SVM套入机器学习框架，让它更容易理解。此时原始约束最优化问题变成损失函数是hinge loss且正则项是L2正则的无约束最优化问题：</p><p><span class="math display">\[ \begin{array}{l} (1)min~\frac{1}{2}\Arrowvert w\Arrowvert^2+C\sum_{i=1}^N\xi_i\\ ~~~st.~y_if(x_i)\ge 1-\xi_i,~~i=1,2....N\\ ~~~~~~~~~\xi_i\ge 0,~~i=1,2....N\\ \Leftrightarrow \\ (2)min~\sum_{i=1}^N[1-y_if(x_i)]_++\lambda \Arrowvert w\Arrowvert^2 \end{array} \]</span></p><p>下面我证明以上问题(1)和问题(2)是等价的(反之亦然)：</p><p><span class="math display">\[ \begin{array}{l} &amp;&amp; \because 1-y_if(x_i)\leq \xi_i \text{ and }0 \leq \xi_i\\ &amp;&amp; \text{if }1-y_if(x_i)\geq 0 \text{ then}\\ &amp;&amp; ~~~~min~\frac{1}{2}\Arrowvert w\Arrowvert^2+C\sum_{i=1}^N\xi_i\\ &amp;&amp; ~~~~\Leftrightarrow \\ &amp;&amp; ~~~~min~\frac{1}{2}\Arrowvert w\Arrowvert^2+C\sum_{i=1}^N1-y_if(x_i)\\ &amp;&amp; \text{if }1-y_if(x_i)&lt;0\text{ then}\\ &amp;&amp; ~~~~min~\frac{1}{2}\Arrowvert w\Arrowvert^2+C\sum_{i=1}^N\xi_i\\ &amp;&amp; ~~~~\Leftrightarrow \\ &amp;&amp; ~~~~min~\frac{1}{2}\Arrowvert w\Arrowvert^2\\ &amp;&amp; \therefore\\ &amp;&amp; min~\frac{1}{2}\Arrowvert w\Arrowvert^2+C\sum_{i=1}^N\xi_i\\ &amp;&amp; ~~~st.~y_if(x_i)\ge 1-\xi_i,~~i=1,2....N\\ &amp;&amp; ~~~~~~~~~\xi_i\ge 0,~~i=1,2....N\\ \Leftrightarrow \\ &amp;&amp; min~\sum_{i=1}^N[1-y_if(x_i)]_++\lambda \Arrowvert w\Arrowvert^2\\ \end{array} \]</span></p><p>到此为止，SVM和普通的判别模型没什么两样，也没有support vector的概念，它之所以叫SVM就得说它的对偶形式了，通过拉格朗日乘数法对原始问题做对偶变换：</p><p><span class="math display">\[ \begin{align*} min&amp;~\frac{1}{2}\Arrowvert w\Arrowvert^2+C\sum_{i=1}^N\xi_i\\ st.&amp;~y_if(x_i)\ge 1-\xi_i,~~i=1,2....N\\ &amp;~\xi_i\ge 0,~~i=1,2....N\\ \Rightarrow&amp; \\ &amp;L(w,b,\xi,\alpha,\mu)=\frac{1}{2}\Arrowvert w\Arrowvert^2+C\sum\limits_{i=1}^{N}\xi_i\\ &amp;-\sum\limits_{i=1}^{N}\alpha_i(y_i(w^Tx_i+b)-1+\xi_i)-\sum\limits_{i=1}^{N}\mu_i\xi_i\\ &amp;\text{(KKT conditions)}\\ &amp;\frac{\partial{L}}{\partial{w}}=w-\sum\limits_{i=1}^{n}y_i\alpha_i x_i=0\\ &amp;\frac{\partial{L}}{\partial{b}}=\sum\limits_{i=1}^{n}y_i\alpha_i=0\\ &amp;\frac{\partial L}{\partial \xi}=C-\alpha-\mu=0\\ &amp;\text{(Complementary Slackness condition)}\\ &amp;\alpha_i(y_i(w^Tx_i+b)- 1+\xi_i)=0\\ &amp;\mu_i\xi_i=(\alpha_i-C)\xi_i=0\\ &amp;\alpha_i\geq 0\\ &amp;\xi_i\geq 0\\ &amp;\mu_i\geq 0\\ &amp;\text{(Replace inner product with the kernel function)}\\ \\ &amp;\Rightarrow \\ max&amp;~\sum\limits_{i=1}^{N}\alpha_i-\frac{1}{2}\sum\limits_{i,j=1}^{N}{y_iy_j\alpha_i\alpha_j(K(x_i,x_j))}\\ st.&amp;~\sum\limits_{i=1}^{n}y_i\alpha_i=0\\ &amp;0 \leq \alpha_i \leq C\\ \end{align*} \]</span></p><p>从互补松弛条件可以得到以下信息：</p><p>当<span class="math inline">\(\alpha_i=C\)</span>时，松弛变量<span class="math inline">\(\xi_i\)</span>不为零，此时其几何间隔小于<span class="math inline">\(1/\Arrowvert w\Arrowvert\)</span>，对应样本点就是误分点；当<span class="math inline">\(\alpha_i=0\)</span>时，松弛变量<span class="math inline">\(\xi_i\)</span>为零，此时其几何间隔大于<span class="math inline">\(1\Arrowvert w\Arrowvert\)</span>，对应样本点就是内部点，即分类正确而又远离最大间隔分类超平面的那些样本点；而<span class="math inline">\(0 &lt; \alpha_i &lt;C\)</span>时，松弛变量<span class="math inline">\(\xi_i\)</span>为零，此时其几何间隔等于<span class="math inline">\(1/ \Arrowvert w\Arrowvert\)</span>，对应<strong>样本点</strong>就是<strong>支持向量</strong>。<span class="math inline">\(\alpha_i\)</span>的取值一定是<span class="math inline">\([0,C]\)</span>，这意味着向量<span class="math inline">\(\alpha\)</span>被限制在了一个边长为<span class="math inline">\(C\)</span>的盒子里。 详细说明可参考:<a target="_blank" rel="noopener" href="http://www.cnblogs.com/vivounicorn/archive/2010/12/22/1913538.html29le/image_1asr9qkai1mseu8n13i8ele1sf89.png">SVM学习——软间隔优化</a>。</p><center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1asr9qkai1mseu8n13i8ele1sf89.png" width="500"></center><center><span class="math inline">\(C\)</span>越大表明你越不想放弃离群点，分类超平面越向离群点移动</center><p>当以上问题求得最优解<span class="math inline">\(\alpha^*\)</span>后，几何间隔变成如下形式： <span class="math display">\[\gamma=(\sum\limits_{i,j \in \{support~vectors\}}y_iy_j\alpha_i^*\alpha_j^*K(x_i,x_j))^{-1/2}\]</span> 它只与有限个样本有关系，这些样本被称作支持向量，从这儿也能看出此时模型参数个数与样本个数有关系，这是典型的非参学习过程。</p><h3 id="核方法">2.2.3 核方法</h3><p>上面对将内积<span class="math inline">\(x_i^Tx_j\)</span>用一个核函数<span class="math inline">\(K(x_i,x_j)\)</span>做了代替，实际上这种替换不限于SVM，所有出现样本间内积的地方都可以考虑这种核变换，本质上它就是通过某种隐式的空间变换在新空间(有限维或无限维兼可)做样本相似度衡量，采用核方法后的模型都可以看做是无固定参数的基于样本的学习器，属于非参学习，核方法与SVM这类模型的发展是互相独立的。</p><center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1astp4htfueu10v51ciq1ab32sr9.png" width="500"></center><center>from <a target="_blank" rel="noopener" href="http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html">Kernel Trick</a></center><p>这里不对原理做展开，可参考：</p><p>1、<a target="_blank" rel="noopener" href="https://www.amazon.com/gp/product/0521813972/qid=1137139342/sr=11-1/ref=sr_11_1/002-7679689-7393625?n=283155">Kernel Methods for Pattern Analysis</a></p><p>2、<a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/1862-the-kernel-trick-for-distances.pdf">the kernel trick for distances</a></p><p>一些可以应用核方法的模型：</p><blockquote><ul><li>SVM</li><li>Perceptron</li><li>PCA</li><li>Gaussian processes</li><li>Canonical correlation analysis</li><li>Ridge regression</li><li>Spectral clustering</li></ul></blockquote><p>在我看来核方法的意义在于： 1、对样本进行空间映射，以较低成本隐式的表达样本之间的相似度，改善样本线性可分的状况，但不能保证线性可分； 2、将线性模型变成非线性模型从而提升其表达能力，但这种升维的方式往往会造成计算复杂度的上升。</p><p>一些关于SVM的参考资料:</p><p><a target="_blank" rel="noopener" href="http://www.cnblogs.com/vivounicorn/archive/2010/12/02/1894311.html">SVM学习——线性学习器</a></p><p><a target="_blank" rel="noopener" href="http://www.cnblogs.com/vivounicorn/archive/2010/12/06/1897702.html">SVM学习——求解二次规划问题</a></p><p><a target="_blank" rel="noopener" href="http://www.cnblogs.com/vivounicorn/archive/2010/12/13/1904720.html">SVM学习——核函数</a></p><p><a target="_blank" rel="noopener" href="http://www.cnblogs.com/vivounicorn/archive/2010/12/18/1909709.html">SVM学习——统计学习理论</a></p><p><a target="_blank" rel="noopener" href="http://www.cnblogs.com/vivounicorn/archive/2010/12/22/1913538.html">SVM学习——软间隔优化</a></p><p><a target="_blank" rel="noopener" href="http://www.cnblogs.com/vivounicorn/archive/2011/01/13/1934296.html">SVM学习——Coordinate Desent Method</a></p><p><a target="_blank" rel="noopener" href="http://www.cnblogs.com/vivounicorn/archive/2011/06/01/2067496.html">SVM学习——Sequential Minimal Optimization</a></p><p><a target="_blank" rel="noopener" href="http://www.cnblogs.com/vivounicorn/archive/2011/08/25/2152824.html">SVM学习——Improvements to Platt’s SMO Algorithm</a></p><h2 id="逻辑回归-logistic-regression">2.3 逻辑回归-Logistic Regression</h2><p>逻辑回归恐怕是互联网领域用的最多的模型之一了，很多公司做算法的同学都会拿它做为算法系统进入模型阶段的baseline。</p><h3 id="模型原理-2">2.3.1 模型原理</h3><p>逻辑回归是一种判别模型，与线性回归类似，它有比较强的先验假设 :</p><ul><li>假设因变量服从贝努利分布</li></ul><p><span class="math display">\[ \begin{align*} p(y|x)&amp;=Bernoulli(\pi)\\ &amp;=p(y=1|x)^y(1-p(y=1|x))^{1-y},y\in\{0,1\} \end{align*} \]</span></p><ul><li>假设训练样本服从<strong>钟形分布</strong>，例如高斯分布：</li></ul><p><span class="math display">\[p(x_i|y=y_k)=Gaussian(\mu_{ik},\sigma_i)\]</span></p><ul><li><p><span class="math inline">\(y\)</span> 是样本标注，布尔类型，取值为0或1；</p></li><li><p><span class="math inline">\(x\)</span> 是样本的特征向量。</p></li></ul><p>逻辑回归是判别模型，所以我们直接学习<span class="math inline">\(p(y|x)\)</span>，以高斯分布为例:</p><p><span class="math display">\[p(y=1|x)=\frac{1}{1+e^{-(w^Tx+b)}}\]</span></p><p><span class="math display">\[p(y=0|x)=\frac{1}{1+e^{(w^Tx+b)}}\]</span></p><center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1ap1pq34s184n18stpt413bq12at9.png" width="400"></center><p>整个原理部分的推导过程如下：</p><p><span class="math display">\[ \begin{align*} p(y=1|x)&amp;=\frac{p(x|y=1)p(y=1)}{p(x)}\\ &amp;=\frac{p(x|y=1)p(y=1)}{p(x|y=1)p(y=1)+p(x|y=0)p(y=0)}\\ &amp;=\frac{1}{1+\frac{p(x|y=0)p(y=0)}{p(x|y=1)p(y=1)}}\\ &amp;=\frac{1}{1+\frac{p(x|y=0)(1-p(y=1))}{p(x|y=1)p(y=1)}}~~\text {to set p(y=1)=}\pi\\ &amp;=\frac{1}{1+\frac{p(x|y=0)(1-\pi)}{p(x|y=1)\pi}}\\ &amp;=\frac{1}{1+e^{ln\frac{1-\pi}{\pi}+ln\frac{p(x|y=0)}{p(x|y=1)}}}\\ &amp;=\frac{1}{1+e^{ln\frac{1-\pi}{\pi}+\sum_iln\frac{p(x_i|y=0)}{p(x_i|y=1)}}}\\ \end{align*} \]</span> <span class="math display">\[ \begin{align*} &amp;\because p(x_i|y_k)=\frac{1}{\sigma_{ik}\sqrt{2\pi}}e^{-\frac{(x_i-\mu_{ik})^2}{2\sigma_{ik}^2}}\\ &amp;\therefore p(y=1|x)=\frac{1}{1+e^{-(ln\frac{\pi-1}{\pi}+\sum_i(\frac{\mu_{i1}-\mu_{i0}}{\sigma_i^2}x_i+\frac{\mu_{i0}^2-\mu_{i1}^2}{2\sigma_i^2}))}}\\ &amp;\qquad\qquad\qquad =\frac{1}{1+e^{-(\sum_iw_ix_i+b)}},i=1,2,...,dim\\ &amp;\text{where}\\ &amp;\qquad\qquad\qquad b=\sum_i\frac{\mu_{i0}^2-\mu_{i1}^2}{2\sigma_i^2}+ln\frac{\pi-1}{\pi}\\ &amp;\qquad\qquad\qquad w_i=\frac{\mu_{i1}-\mu_{i0}}{\sigma_i^2} \end{align*} \]</span></p><p>采用 MLE 或者 MAP 做参数求解:</p><p><span class="math display">\[ \begin{array}{l} w=arg~max_w\sum_{i=1}^Nln~p(y_i|x_i)\\ \Leftrightarrow \\ w=arg~min_w\sum_{i=1}^N{y_iln~p(y_i=1|x_i)+(1-y_i)ln~p(y_i=0|x_i)} \end{array} \]</span></p><h3 id="损失函数-2">2.3.2 损失函数</h3><p><strong>损失函数3 —— Cross Entropy Loss</strong></p><p><span class="math display">\[ \begin{array}{l} loss(x)=H_p(q)=\sum_{i=1}^N\sum_y(\int_y) p(y|x_i)ln\frac{1}{q(y|x_i)}\\ \text{especially for bernoulli distribution:}\\ loss(x)=\sum_{i=1}^Ny_i ln ~p(y_i|x_i)+(1-y_i)(1-ln~p(y_i|x_i)) \end{array} \]</span></p><p>简单理解，从概率角度：Cross Entropy损失函数衡量的是两个概率分布<span class="math inline">\(p\)</span>与<span class="math inline">\(q\)</span>之间的相似性，对真实分布估计的越准损失越小；从信息论角度：用编码方式<span class="math inline">\(q\)</span>对由编码方式<span class="math inline">\(p\)</span>产生的信息做编码，如果两种编码方式越接近，产生的信息损失越小。与Cross Entropy相关的一个概念是Kullback–Leibler divergence，后者是衡量两个概率分布接近程度的标量值，定义如下： <span class="math display">\[D_q(p) = \sum_x(\int_x) p(x)\log_2\left(\frac{p(x)}{q(x)} \right)\]</span> 当两个分布完全一致时其值为0，显然Cross Entropy与Kullback–Leibler divergence的关系是： <span class="math display">\[H_p(q)=H(p)+D_q(p)\]</span></p><p>关于交叉熵及其周边原理，有一篇文章写得特别好：<strong><a target="_blank" rel="noopener" href="http://colah.github.io/posts/2015-09-Visual-Information/">Visual Information Theory</a></strong>。</p><h2 id="bagging-and-boosting框架">2.4 Bagging and Boosting框架</h2><p>Bagging和Boosting是两类最常用以及好用的模型融合框架，殊途而同归。</p><h3 id="bagging框架">2.4.1 Bagging框架</h3>Bagging(Breiman, 1996) 方法是通过对训练样本和特征做有放回的抽样，并拟合若干个基础模型进而通过投票方式做最终分类决策的框架。每个基础分类器（可以是树形结构、神经网络等等任何分类模型）的特点是<strong>低偏差、高方差</strong>，框架通过(加权)投票方式降低方差，使得整体趋于<strong>低偏差、低方差</strong>。<center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1ap2hfjae8q6nta1ppd10l1fkv1g.png" width="600"></center><p>分析如下：</p><p>假设任务是学习一个模型 <span class="math inline">\(y=f(x)\)</span> ，我们通过抽样生成生成<span class="math inline">\(N\)</span> 个数据集，并训练得到<span class="math inline">\(N\)</span>个基础分类器<span class="math inline">\(c_i......c_N\)</span>。</p><p><span class="math display">\[ \begin{array}{l} \text{define}:\\ \qquad\qquad (1). y=f(x), \text{prediction function}\\ \qquad\qquad (2). o_i(x)=c_i(x), \text{ $c_i$ is the i-th classifier}\\ \qquad\qquad (3). \overline{o}(x)=\sum_{i=1}^Nw_io_i(x), \text{ where } \sum_{i=1}^Nw_i=1\\ \qquad\qquad (4). v_i(x)=[o_i(x)-\overline{o}(x)]^2\\ \qquad\qquad (5). \overline{v}(x)=\sum_{i=1}^Nw_iv_i(x)=\sum_{i=1}^Nw_i[o_i(x)-\overline{o}(x)]^2\\ \qquad\qquad (6). \overline{\epsilon}(x)=\sum_{i=1}^Nw_i[(f(x)-o_i(x))^2]\\ \qquad\qquad 7). e(x)=(f(x)-\overline{o}(x))^2 \\ \because \overline{v}(x)=\sum_{i=1}^Nw_i[o_i(x)-\overline{o}(x)]^2\\ \qquad\qquad =\sum_{i=1}^Nw_i[(f(x)-o_i(x))-(f(x)-\overline{o}(x))]^2\\ \qquad\qquad =\sum_{i=1}^Nw_i[(f(x)-o_i(x))^2+(f(x)-\overline{o}(x))^2\\ \qquad\qquad -2(f(x)-o_i(x))(f(x)-\overline{o}(x))]\\ \qquad\qquad =\sum_{i=1}^Nw_i[(f(x)-o_i(x))^2]-(f(x)-\overline{o}(x))^2\\ \therefore e(x)=\overline{\epsilon}(x)-\overline{v}(x) \end{array} \]</span></p><p>从结论可以发现多分类器投票机制的引入可以降低模型方差从而降低分类错误率，大家可以多理解理解这一系列推导。</p><h3 id="boosting框架">2.4.2 Boosting框架</h3>Boosting(Freund &amp; Shapire, 1996) 通过迭代方式训练若干基础分类器，每个分类器依据上一轮分类器产生的残差做权重调整，每轮的分类器需要够“简单”，具有<strong>高偏差、低方差</strong>的特点，框架再辅以(加权)投票方式降低偏差，使得整体趋于<strong>低偏差、低方差</strong>。<center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1ap2e2geqo3a11ur18od2b413ek13.png" width="600"></center><p>一个简单的总结: <span class="math display">\[ \begin{array}{l} F(x)=\sum_{i=1}^Nw_if_i(x)\\ \text{where:}\\ \qquad f_i\text{ is the base classier }i\\ \qquad w_i \text{ is the weight of classier }i\\ \qquad x \text{ is the feature vector of example}\\ \text{define:}\\ \qquad \text{(1). margin of an example(x,y) with respect to the classier is yF(x)}\\ \qquad \text{(2). cost function of $N$ examples is $C(F)=\frac{1}{N}\sum_{i=1}^NC(y_iF(x_i))$}\\ \text{Now we wish to find a new $f$ to add to F so that $C(F+\alpha f)$ can be decreased}\\ \because C(F+\alpha f)=C(F)+\alpha\langle\nabla{C(F)},f\rangle\\ \therefore \text{the greatest reduction of $C$ will satisfied: }\\ \qquad\qquad\qquad f=Max~-\langle\nabla{C(F)},f\rangle \end{array} \]</span></p><p>AnyBoost Algorithm</p><p>Boost算法是个框架，很多模型都能往进来套。 <span class="math display">\[ \begin{array}{l} F_0(x)=0\\ \text{for $i$=0 to T:}\\ \qquad f_{t+1}=classifier(F_t)\\ \qquad if~~-\langle\nabla{C(F)},f_{t+1}\rangle \le0:\\ \qquad\qquad return~F_t\\ \qquad choose~w_{t+1}\\ \qquad F_{t+1}=F_t+w_{t+1}f_{t+1}\\ return~F_{T+1} \end{array} \]</span></p><p>Q: boosting 和 margin的关系是什么（机器学习中margin的定义为<span class="math inline">\(yf(x)\)</span>）？</p><p>Q: 类似bagging，为什么boosting能够通过reweight及投票方式降低整体偏差？</p><h2 id="additive-tree-模型">2.5 Additive Tree 模型</h2><p>Additive tree models (ATMs)是指基础模型是树形结构的一类融合模型，可做分类、回归，很多经典的模型可以被看做ATM模型，比如Random forest 、Adaboost with trees、GBDT等。</p><p>ATM 对N棵决策树做加权融合，其判别函数为：</p><p><span class="math display">\[ \begin{array}{l} F(x)=\sum_{i=1}^Nw_if_i(x)\\ \text{where }f_i\text{ is the output of tree }i\\ \qquad\qquad\qquad w_i \text{ is the weight of tree }i\\ \qquad\qquad\qquad x \text{ is the feature vector of instance} \end{array} \]</span></p><h3 id="random-forests">2.5.1 Random Forests</h3>Random Forest 属于bagging类模型，每棵树会使用各自随机抽样样本和特征被独立的训练。<center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1ap2m8teo1kqg1up8mtvccpfns2a.png" width="400"></center><p><span class="math display">\[ \begin{array}{l} \text{for $t$ = 1 to $T$:}\\ \qquad \text{(1). Sample $n$ instances from the dataset with replacement}\\ \qquad \text{(2). Randomization:}\\ \qquad \qquad\text{$\bullet$ Bootstrap samples.}\\ \qquad \qquad\text{$\bullet$ Random selection of $K\le p$ split variables.}\\ \qquad \qquad\text{$\bullet$ Random selection of threshold.}\\ \qquad \text{(2). Train an low-bias unpruned decision or regression tree $f_t$ on the sampled instances }\\ \\ \text{The final is the average of the outputs from all the trees:}\\ F(x)=\sum_{i=1}^Tw_if_i(x)\\ ~~~where~\sum_{i=1}^Tw_i=1,~(such~ as ~w_i=\frac{1}{T}) \end{array} \]</span></p><h3 id="adaboost-with-trees">2.5.2 AdaBoost with trees</h3><p>AdaBoost with trees通过训练多个弱分类器来组合得到一个强分类器，每次迭代会生成一棵<strong>高偏差、低方差</strong>的树形弱分类器，每一轮的训练会更关注上一轮被分类器分错的样本，为其加大权重，训练过程如下：</p><p><span class="math display">\[ \begin{array}{l} f_0(x)=0\\ \text{for $i$=1 to $T$:}\\ \qquad minimize~\sum_{i=1}^NL(y_i,F_t(x_i)+\alpha_tf_t(x_i))\\ \qquad F_{t+1}(x)=F_t(x)+\alpha_tf_t(x)\\ \\ \text{The final is weighted average of the outputs from all the weak classifiers:}\\ F(x)=\sum_{i=1}^T\alpha_if_i(x)\\ \end{array} \]</span></p><center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1ap2oflv078q1h631peataq13ih2n.png" width="500"></center><center>From Bishop(2006)</center><h3 id="gradient-boosting-decision-tree">2.5.3 Gradient Boosting Decision Tree</h3><p>Gradient boosted 是一类boosting的技术，不同于Adaboost加大误分样本权重的策略，它每次迭代加的是上一轮梯度更新值： <span class="math display">\[\sum_{i=1}^NL(y_i,F_t(x_i)+\alpha_tf_t(x_i))\]</span></p><p>其训练过程如下:</p><p><span class="math display">\[ \begin{array}{l} F_{t+1}(x)=F_t(x)+\alpha_tf_t(x)\\ f_t(x_i)\thickapprox -\frac{\partial L(y_i,F_t(x_i))}{\partial F_t(x_i)}\\ \qquad \text{where $\alpha_t$ is the learning rate.} \end{array} \]</span></p>GBDT是基础分类器为决策树的可做分类和回归的模型。<center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/gbdt.png" width="400"></center>目前我认为最好的GBDT的实现是XGBoost: 其回归过程的示例图如下，通过对样本落到每棵树的叶子节点的权重值做累加来实现回归(或分类)：<center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1ap5297q1nte1tkpb3u1lfr1pk45c.png" width="500"></center><center>Regression Tree Ensemble from chentianqi</center><p>其原理推导如下：</p><p><span class="math display">\[ \begin{array}{l} \text{Prediction model: }F(x)=\sum_{i=1}^Tw_if_i(x)\\ \text{Objective: }obj^t=\sum_{i=1}^NL(y_i,F_i^t(x_i))+\Omega(f_t)\\ \qquad \text{where N is instance numbers and t is current trees.}\\ \because obj^t=\sum_{i=1}^NL(y_i,F_i^t(x_i))+\Omega(f_t)\\ \qquad =\sum_{i=1}^NL(y_i,F_i^{t-1}(x_i)+w_tf_t(x_i))+\Omega(f_t)\\ \text{Recall: }f(x+\Delta x)\thickapprox f(x)+\nabla f(x)\Delta x+\frac{1}{2}\nabla^2 f(x)\Delta x^2\\ \therefore obj^t\thickapprox\sum_{i=1}^N[L(y_i,F_i^{t-1}(x_i))+\nabla _{F_{t-1}}L(y_i,F_i^{t-1}(x_i))w_tf_t(x_i)\\ \qquad \qquad \frac{1}{2}\nabla _{F_{t-1}}^2L(y_i,F_i^{t-1}(x_i))w_t^2f_t^2(x_i)]+\Omega(f_t)\\ \text{set $g_i=\nabla _{F_{t-1}}L(y_i,F_i^{t-1}(x_i))$}\\ \qquad h_i=\nabla _{F_{t-1}}^2L(y_i,F_i^{t-1}(x_i))\\ \qquad obj^t\thickapprox \sum_{i=1}^N[L(y_i,F_i^{t-1}(x_i))+g_iw_tf_t(x_i)+\frac{1}{2}h_iw_t^2f_t^2(x_i)]+\Omega(f_t)\\ \because L(y_i,F_i^{t-1}(x_i)) \text{ is constant.}\\ \therefore \text{Our objective function is:}\\ \qquad obj^t=\sum_{i=1}^N[g_iw_tf_t(x_i)+\frac{1}{2}h_iw_t^2f_t^2(x_i)]+\Omega(f_t)+C\\ \text{Define tree by a vector of scores in leafs,any instance will be mapped to a leaf:}\\ f_t(x)=m_q(x),~~m\in R^T,~~q:R^d\rightarrow\{1,2,3,...,T\}\\ \Omega(f_t)=\gamma T+ \frac{1}{2}\lambda \sum_{i=1}^Tm_j^2,\\ \text{where $T$ is total number of leaf nodes of $t$ trees}\\ \qquad \qquad \text{$m_j$ is the weight of j-th leaf node.}\\ \text{Define the instance set in leaf $j$ as $I_j=\{i|j=q(x_i)\}$}\\ \text{Our new objective function is:}\\ obj^t=\sum_{i=1}^N[g_iw_tf_t(x_i)+\frac{1}{2}h_iw_t^2f_t^2(x_i)]+\Omega(f_t)\\ \qquad =\sum_{i=1}^N[g_iw_tm_q(x_i)+\frac{1}{2}h_iw_t^2m_q^2(x_i)]+\gamma T+ \frac{1}{2}\lambda \sum_{i=1}^Tm_j^2\\ \qquad =\sum_{j=1}^T[(\sum_{i \in I_j}g_i)w_tm_j+\frac{1}{2}(\sum_{i \in I_j}h_iw_t^2+\lambda )m_j^2]+\gamma T\\ \text{Define $G_j=\sum_{i \in I_j}g_i$ and $H_j=\sum_{i \in I_j}h_i$ then}\\ obj^t=\sum_{j=1}^T[G_jw_tm_j+\frac{1}{2}(H_jw_t^2+\lambda)m_j^2]+\gamma T\\ \text{For a quadratic function optimization problems:}\\ m_j^*=-\frac{G_j^2w_t}{H_jw_t^2+\lambda}\\ obj^*=-\frac{1}{2}\sum_{j=1}^T\frac{G_j^2w_t^2}{H_jw_t^2+\lambda}+\gamma T\\ \text{If we set $w_t=1$ then}\\ m_j^*=-\frac{G_j}{H_j+\lambda}\\ obj^*=-\frac{1}{2}\sum_{j=1}^T\frac{G_j^2}{H_j+\lambda}+\gamma T\\ \text{So when we add a split, our obtained gain is:}\\ gain=\underbrace{\frac{G_L^2}{H_L+\lambda}}_{left ~child}+\underbrace{\frac{G_R^2}{H_R+\lambda}}_{right~child}-\underbrace{\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}}_{do~not~split}-\gamma~~~~(thinking~why?) \end{array} \]</span></p><p>对GBDT来说依然避免不了过拟合，所以与传统机器学习一样，通过正则化策略可以降低这种风险：</p><ul><li>提前终止（Early Stopping）</li></ul><p>通过观察模型在验证集上的错误率，如果它变化不大则可以提前结束训练，控制迭代轮数（即树的个数）；</p><ul><li>收缩（Shrinkage）</li></ul><p><span class="math inline">\(F_{t+1}(x)=F_t(x)+\alpha_tf_t(x)\)</span> 从迭代的角度可以看成是学习率（learning rate），从融合（ensemble）的角度可以看成每棵树的权重，<span class="math inline">\(\alpha\)</span>的大小经验上可以取0.1，它是对模型泛化性和训练时长的折中；</p><ul><li>抽样（Subsampling）</li></ul><p>借鉴Bagging的思想，GBDT可以在每一轮树的构建中使用训练集中无放回抽样的样本，也可以对特征做抽样，模拟真实场景下的样本分布波动；</p><ul><li>目标函数中显式的正则化项</li></ul><p><span class="math inline">\(\Omega(f_t)=\gamma T+ \frac{1}{2}\lambda \sum_{i=1}^Tm_j^2\)</span> 通过对树的叶子节点个数、叶子节点权重做显式的正则化达到缓解过拟合的效果；</p><ul><li>参数放弃（Dropout）</li></ul>模拟深度学习里随机放弃更新权重的方法，可以在每新增一棵树的时候拟合随机抽取的一些树的残差，相关方法可以参考：<a target="_blank" rel="noopener" href="http://jmlr.org/proceedings/papers/v38/korlakaivinayak15.pdf">DART: Dropouts meet Multiple Additive Regression Trees</a>，文中对该方法和Shrinkage的方法做了比较：<center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1b0s1udkl1att1v8o1hfs11im1eepm.png" width="400"></center><p>XGBoost源码在: https://github.com/dmlc中，其包含非常棒的设计思想和实现，建议大家都去学习一下，一起添砖加瓦。原理部分我就不再多写了，看懂一篇论文即可，但特别需要注意的是文中提到的<strong>weighted quantile sketch</strong>算法，它用来解决当样本集权重分布不一致时如何选择分裂节点的问题：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1603.02754.pdf">XGBoost: A Scalable Tree Boosting System</a>。</p><h3 id="简单的例子">2.5.4 简单的例子</h3><p>下面是关于几个常用机器学习模型的对比，从中能直观地体会到不同模型的运作区别，数据集采用libsvm作者整理好的fourclass_scale数据集，机器学习工具采用sklearn，代码中模型未做任何调参，仅使用默认参数设置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">matplotlib.use(<span class="string">&#x27;Agg&#x27;</span>)</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> proj3d</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"><span class="keyword">from</span> sklearn.externals.joblib <span class="keyword">import</span> Memory</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_svmlight_file</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> cm</span><br><span class="line"><span class="keyword">from</span> matplotlib.ticker <span class="keyword">import</span> LinearLocator, FormatStrFormatter</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Dense,Dropout,Activation</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">download</span>(<span class="params">outpath</span>):</span></span><br><span class="line">  filename=outpath+<span class="string">&quot;/fourclass_scale&quot;</span></span><br><span class="line">  <span class="keyword">if</span> os.path.exists(filename) == <span class="literal">False</span>:</span><br><span class="line">    urllib.urlretrieve(<span class="string">&quot;https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/fourclass_scale&quot;</span>,filename)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_building</span>():</span></span><br><span class="line">  dtrain = load_svmlight_file(<span class="string">&#x27;fourclass_scale&#x27;</span>)</span><br><span class="line">  train_d=dtrain[<span class="number">0</span>].toarray()</span><br><span class="line">  train_l=dtrain[<span class="number">1</span>]</span><br><span class="line">  x1 = train_d[:,<span class="number">0</span>]</span><br><span class="line">  x2 = train_d[:,<span class="number">1</span>]</span><br><span class="line">  y = train_l</span><br><span class="line">  px1 = []</span><br><span class="line">  px2 = []</span><br><span class="line">  pl = []</span><br><span class="line">  nx1 = []</span><br><span class="line">  nx2 = []</span><br><span class="line">  nl = []</span><br><span class="line">  idx = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> y:</span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">1</span>:</span><br><span class="line">      px1.append(x1[idx]-<span class="number">0.5</span>)</span><br><span class="line">      px2.append(x2[idx]+<span class="number">0.5</span>)</span><br><span class="line">      pl.append(i)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      nx1.append(x1[idx]+<span class="number">0.8</span>)</span><br><span class="line">      nx2.append(x2[idx]-<span class="number">0.8</span>)</span><br><span class="line">      nl.append(i)</span><br><span class="line">    idx = idx + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">  x_axis, y_axis = np.meshgrid(np.linspace(x1.<span class="built_in">min</span>(), x1.<span class="built_in">max</span>(), <span class="number">100</span>), np.linspace(x2.<span class="built_in">min</span>(), x2.<span class="built_in">max</span>(), <span class="number">100</span>))</span><br><span class="line">  <span class="keyword">return</span> x_axis, y_axis, px1, px2, nx1, nx2, train_d, train_l</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">paint</span>(<span class="params">name, x_axis, y_axis, px1, px2, nx1, nx2, z</span>):</span></span><br><span class="line">  fig = plt.figure()</span><br><span class="line">  ax = Axes3D(fig)</span><br><span class="line">  ax=plt.subplot(projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line">  ax.scatter(px1,px2,c=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">  ax.scatter(nx1,nx2,c=<span class="string">&#x27;g&#x27;</span>)</span><br><span class="line">  ax.plot_surface(x_axis, y_axis,z.reshape(x_axis.shape), rstride=<span class="number">8</span>, cstride=<span class="number">8</span>, alpha=<span class="number">0.3</span>)</span><br><span class="line">  ax.contourf(x_axis, y_axis, z.reshape(x_axis.shape), zdir=<span class="string">&#x27;z&#x27;</span>, offset=-<span class="number">100</span>, cmap=cm.coolwarm)</span><br><span class="line">  ax.contourf(x_axis, y_axis, z.reshape(x_axis.shape), levels=[<span class="number">0</span>,<span class="built_in">max</span>(z)], cmap=cm.hot)</span><br><span class="line">  ax.set_xlabel(<span class="string">&#x27;X&#x27;</span>)</span><br><span class="line">  ax.set_ylabel(<span class="string">&#x27;Y&#x27;</span>)</span><br><span class="line">  ax.set_zlabel(<span class="string">&#x27;Z&#x27;</span>)</span><br><span class="line">  fig.savefig(name+<span class="string">&quot;.png&quot;</span>, <span class="built_in">format</span>=<span class="string">&#x27;png&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svc</span>(<span class="params">x_axis, y_axis, x,y</span>):</span></span><br><span class="line">  clf = svm.SVC()</span><br><span class="line">  clf.fit(x, y)</span><br><span class="line">  y = clf.predict(np.c_[x_axis.ravel(), y_axis.ravel()])</span><br><span class="line">  <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lr</span>(<span class="params">x_axis, y_axis, x,y</span>):</span></span><br><span class="line">  clf = LogisticRegression()</span><br><span class="line">  clf.fit(x, y)</span><br><span class="line">  y = clf.predict(np.c_[x_axis.ravel(), y_axis.ravel()])</span><br><span class="line">  <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ridge</span>(<span class="params">x_axis, y_axis, x,y</span>):</span></span><br><span class="line">  clf = Ridge()</span><br><span class="line">  clf.fit(x, y)</span><br><span class="line">  y = clf.predict(np.c_[x_axis.ravel(), y_axis.ravel()])</span><br><span class="line">  <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dt</span>(<span class="params">x_axis, y_axis, x,y</span>):</span></span><br><span class="line">  clf = GradientBoostingClassifier()</span><br><span class="line">  clf.fit(x, y)</span><br><span class="line">  y = clf.predict(np.c_[x_axis.ravel(), y_axis.ravel()])</span><br><span class="line">  <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn</span>(<span class="params">x_axis, y_axis, x,y</span>):</span></span><br><span class="line">  model = Sequential()</span><br><span class="line">  model.add(Dense(<span class="number">20</span>, input_dim=<span class="number">2</span>))</span><br><span class="line">  model.add(Activation(<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">  model.add(Dense(<span class="number">20</span>))</span><br><span class="line">  model.add(Activation(<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">  model.add(Dense(<span class="number">1</span>, activation=<span class="string">&#x27;tanh&#x27;</span>))</span><br><span class="line">  model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;mse&#x27;</span>,</span><br><span class="line">                optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">                metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">  model.fit(x,y,batch_size=<span class="number">20</span>, nb_epoch=<span class="number">50</span>, validation_split=<span class="number">0.2</span>)</span><br><span class="line">  y = model.predict(np.c_[x_axis.ravel(), y_axis.ravel()],batch_size=<span class="number">20</span>)</span><br><span class="line">  <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">  download(<span class="string">&quot;/root&quot;</span>)</span><br><span class="line">  x_axis, y_axis, px1, px2, nx1, nx2, train_d, train_l = data_building()</span><br><span class="line">  z = svc(x_axis, y_axis, train_d, train_l)</span><br><span class="line">  paint(<span class="string">&quot;svc&quot;</span>, x_axis, y_axis, px1, px2, nx1, nx2, z)</span><br><span class="line">  z = lr(x_axis, y_axis, train_d, train_l)</span><br><span class="line">  paint(<span class="string">&quot;lr&quot;</span>, x_axis, y_axis, px1, px2, nx1, nx2, z)</span><br><span class="line">  z = ridge(x_axis, y_axis, train_d, train_l)</span><br><span class="line">  paint(<span class="string">&quot;ridge&quot;</span>, x_axis, y_axis, px1, px2, nx1, nx2, z)</span><br><span class="line">  z = dt(x_axis, y_axis, train_d, train_l)</span><br><span class="line">  paint(<span class="string">&quot;gbdt&quot;</span>, x_axis, y_axis, px1, px2, nx1, nx2, z)</span><br><span class="line">  z = nn(x_axis, y_axis, train_d, train_l)</span><br><span class="line">  paint(<span class="string">&quot;nn&quot;</span>, x_axis, y_axis, px1, px2, nx1, nx2, z)</span><br><span class="line"></span><br></pre></td></tr></table></figure><center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1apkqel8gmtlervoud3k1ad2m.png" width="900"></center><center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1apme7kvi5klumr1cig181c2vm9.png" width="300"></center><h3 id="catboost">2.5.5 CatBoost</h3><p>基于不同的工程实现方式，目前常用的GBDT框架有3种，分别为：XGBoost（天奇）、LightGBM（微软）、CatBoost（Yandex）。三种实现在效果上并没有特别大的区别，区别主要在特征的处理尤其是类别特征上和建树过程上。</p><ul><li>类别特征</li></ul><p>在机器学习中有一大类特征叫类别特征，这类特征主要有两个特点：</p><blockquote><p>类别取值是离散的</p></blockquote><blockquote><p>每个类别取值非数值类型</p></blockquote><p>例如，“性别”这个特征，它一般有3个取值：“女”，“男”，“未知”，在特征处理阶段会被编码为离散的实数值或向量，然后进入模型。 - 类别特征编码 将类别取值编码为实数值有很多方法，大致如下：</p><p>1、 <strong>Label Encoding</strong></p><pre><code>每个类别赋值一个整数值。例如，性别“女”=0，“男”=1，“未知”=2，实践中这种编码方法用处有限。</code></pre><p>2、 <strong>One-Hot Encoding</strong></p><pre><code>创建一个维度与类别取值个数相同的向量，每个向量位置对应一个类别取值，当前类别取值为1，其他为0,这种编码最大问题是类别取值太多时，向量维度很高很稀疏，对模型学习效果和内存占用都是挑战（比如，NLP里几十万的词典表）。</code></pre><p>例如，性别特征编码：</p><table><thead><tr class="header"><th>性别</th><th style="text-align:center">编码</th></tr></thead><tbody><tr class="odd"><td>女</td><td style="text-align:center">100</td></tr><tr class="even"><td>男</td><td style="text-align:center">010</td></tr><tr class="odd"><td>未知</td><td style="text-align:center">001</td></tr></tbody></table><p>3、 <strong>Distributed Representation</strong></p><pre><code>类别取值会被表示为一个实数向量，这个向量维度远小于取值个数。最经典的是NLP里常用的word2vec，一举三得：实现向量表示、实现降维、向量具有隐语义。</code></pre><p>4、<strong>Target Encoding</strong></p><pre><code>本质上它是一种反馈类特征，利用了标注数据，在实践中，要提升效果，反馈类特征一定是你要最优先考虑的。
但使用时需要特别注意**Target Leakage**，例如：

1)、在训练集使用标注信息生成特征的方式，在测试集上无法实现

2)、在训练集使用了未来的信息做特征，如，使用了测试集信息

3)、特征生成时样本的统计意义不足，如，广告曝光不足
......</code></pre><p><strong>Target Encoding</strong>有很多版本：</p><p>1)、<strong>Greedy Target Statistics</strong></p><p>编码方式为：</p><p><span class="math display">\[f_{i,j}^{&#39;k}=\frac{\sum_{i=0}^{N}y_i \mathrm{I}\{f_{i,j}=f_{i,j}^{k}\} }{\sum_{i=0}^{N} \mathrm{I}\{f_{i,j}=f_{i,j}^{k}\}}\]</span> 其中:</p><p><span class="math inline">\((f_{i,j}^k,y_i)\quad \{ 0\leq i \leq N,0\leq j \leq M,0\leq k \leq L\}\)</span>：为训练数据集，如果<span class="math inline">\(f\)</span>是类别特征，则<span class="math inline">\(f_{i,j}^k\)</span>为第<span class="math inline">\(i\)</span>个样本第<span class="math inline">\(j\)</span>个特征的第<span class="math inline">\(k\)</span>种取值，<span class="math inline">\(y_i\)</span>为标注且<span class="math inline">\(y_i \in \{0,1\}\)</span>。</p><p><span class="math inline">\(N\)</span>：样本总量，<span class="math inline">\(M\)</span>：特征总量，<span class="math inline">\(L\)</span>：某类别特征下可取值总数</p><p><span class="math inline">\(f_{i,j}^{&#39;k}\)</span>：为第<span class="math inline">\(i\)</span>个样本第<span class="math inline">\(j\)</span>个特征的第<span class="math inline">\(k\)</span>种取值编码后的结果</p><p><span class="math inline">\(\mathrm{I}\{\cdot\}\)</span>：为指示函数</p><p>大白话是：以训练样本中的某一特征对样本GroupBy，计算每个Group（即当前类的所有取值）内标注的均值为新的特征。 例如，有以下广告曝光及点击样本：</p><table><thead><tr class="header"><th>样本编号</th><th style="text-align:center">性别特征</th><th style="text-align:center">是否点击广告</th></tr></thead><tbody><tr class="odd"><td>1</td><td style="text-align:center">女</td><td style="text-align:center">1</td></tr><tr class="even"><td>2</td><td style="text-align:center">女</td><td style="text-align:center">0</td></tr><tr class="odd"><td>3</td><td style="text-align:center">男</td><td style="text-align:center">0</td></tr><tr class="even"><td>4</td><td style="text-align:center">女</td><td style="text-align:center">1</td></tr><tr class="odd"><td>5</td><td style="text-align:center">男</td><td style="text-align:center">0</td></tr><tr class="even"><td>6</td><td style="text-align:center">未知</td><td style="text-align:center">1</td></tr><tr class="odd"><td>7</td><td style="text-align:center">女</td><td style="text-align:center">0</td></tr><tr class="even"><td>8</td><td style="text-align:center">男</td><td style="text-align:center">1</td></tr><tr class="odd"><td>9</td><td style="text-align:center">未知</td><td style="text-align:center">0</td></tr><tr class="even"><td>10</td><td style="text-align:center">女</td><td style="text-align:center">1</td></tr><tr class="odd"><td>11</td><td style="text-align:center">男</td><td style="text-align:center">1</td></tr><tr class="even"><td>12</td><td style="text-align:center">未知</td><td style="text-align:center">0</td></tr></tbody></table><p>分组计算：</p><p>“女”：<span class="math inline">\(ctr=\frac{3}{5}=0.6\)</span></p><p>“男”：<span class="math inline">\(ctr=\frac{2}{4}=0.5\)</span></p><p>“未知”：<span class="math inline">\(ctr=\frac{1}{3}=0.3\)</span></p><p>性别特征编码为：</p><table><thead><tr class="header"><th>样本编号</th><th style="text-align:center">性别特征</th><th style="text-align:center">性别特征编码</th></tr></thead><tbody><tr class="odd"><td>1</td><td style="text-align:center">女</td><td style="text-align:center">0.6</td></tr><tr class="even"><td>2</td><td style="text-align:center">女</td><td style="text-align:center">0.6</td></tr><tr class="odd"><td>3</td><td style="text-align:center">男</td><td style="text-align:center">0.5</td></tr><tr class="even"><td>4</td><td style="text-align:center">女</td><td style="text-align:center">0.6</td></tr><tr class="odd"><td>5</td><td style="text-align:center">男</td><td style="text-align:center">0.5</td></tr><tr class="even"><td>6</td><td style="text-align:center">未知</td><td style="text-align:center">0.3</td></tr><tr class="odd"><td>7</td><td style="text-align:center">女</td><td style="text-align:center">0.6</td></tr><tr class="even"><td>8</td><td style="text-align:center">男</td><td style="text-align:center">0.5</td></tr><tr class="odd"><td>9</td><td style="text-align:center">未知</td><td style="text-align:center">0.3</td></tr><tr class="even"><td>10</td><td style="text-align:center">女</td><td style="text-align:center">0.6</td></tr><tr class="odd"><td>11</td><td style="text-align:center">男</td><td style="text-align:center">0.5</td></tr><tr class="even"><td>12</td><td style="text-align:center">未知</td><td style="text-align:center">0.3</td></tr></tbody></table><p>2)、<strong>Greedy Target Statistics with Smoothes</strong></p><p>为了降低原始Greedy Target Statistics方法对处理低频特征值的劣势，通常会对它做平滑操作。 例如，有以下广告曝光及点击样本：</p><table><thead><tr class="header"><th>样本编号</th><th style="text-align:center">性别特征</th><th style="text-align:center">是否点击广告</th></tr></thead><tbody><tr class="odd"><td>1</td><td style="text-align:center">女</td><td style="text-align:center">1</td></tr><tr class="even"><td>2</td><td style="text-align:center">女</td><td style="text-align:center">0</td></tr><tr class="odd"><td>3</td><td style="text-align:center">男</td><td style="text-align:center">0</td></tr><tr class="even"><td>4</td><td style="text-align:center">女</td><td style="text-align:center">1</td></tr><tr class="odd"><td>5</td><td style="text-align:center">男</td><td style="text-align:center">0</td></tr><tr class="even"><td>6</td><td style="text-align:center">未知</td><td style="text-align:center">0</td></tr><tr class="odd"><td>7</td><td style="text-align:center">女</td><td style="text-align:center">0</td></tr><tr class="even"><td>8</td><td style="text-align:center">男</td><td style="text-align:center">0</td></tr><tr class="odd"><td>9</td><td style="text-align:center">未知</td><td style="text-align:center">0</td></tr><tr class="even"><td>10</td><td style="text-align:center">女</td><td style="text-align:center">1</td></tr><tr class="odd"><td>11</td><td style="text-align:center">男</td><td style="text-align:center">0</td></tr><tr class="even"><td>12</td><td style="text-align:center">未知</td><td style="text-align:center">0</td></tr></tbody></table><p>分组计算：</p><p>“女”：<span class="math inline">\(ctr=\frac{3}{5}=0.6\)</span></p><p>“男”：<span class="math inline">\(ctr=\frac{0}{4}=0\)</span></p><p>“未知”：<span class="math inline">\(ctr=\frac{0}{3}=0\)</span></p><p>性别特征编码为：</p><table><thead><tr class="header"><th>样本编号</th><th style="text-align:center">性别特征</th><th style="text-align:center">性别特征编码</th></tr></thead><tbody><tr class="odd"><td>1</td><td style="text-align:center">女</td><td style="text-align:center">0.6</td></tr><tr class="even"><td>2</td><td style="text-align:center">女</td><td style="text-align:center">0.6</td></tr><tr class="odd"><td>3</td><td style="text-align:center">男</td><td style="text-align:center">0</td></tr><tr class="even"><td>4</td><td style="text-align:center">女</td><td style="text-align:center">0.6</td></tr><tr class="odd"><td>5</td><td style="text-align:center">男</td><td style="text-align:center">0</td></tr><tr class="even"><td>6</td><td style="text-align:center">未知</td><td style="text-align:center">0</td></tr><tr class="odd"><td>7</td><td style="text-align:center">女</td><td style="text-align:center">0.6</td></tr><tr class="even"><td>8</td><td style="text-align:center">男</td><td style="text-align:center">0</td></tr><tr class="odd"><td>9</td><td style="text-align:center">未知</td><td style="text-align:center">0</td></tr><tr class="even"><td>10</td><td style="text-align:center">女</td><td style="text-align:center">0.6</td></tr><tr class="odd"><td>11</td><td style="text-align:center">男</td><td style="text-align:center">0</td></tr><tr class="even"><td>12</td><td style="text-align:center">未知</td><td style="text-align:center">0</td></tr></tbody></table><p>显然，这种编码方式，将来做模型训练大概率会过拟合，可以加一个正则项来缓解，编码方式改为：</p><p><span class="math display">\[ f_{i,j}^{&#39;k}= \lambda^k \frac{\sum_{i=0}^{N}y_i}{N} + (1-\lambda^k)\frac{\sum_{i=0}^{N}y_i \mathrm{I}\{f_{i,j}=f_{i,j}^{k}\} }{\sum_{i=0}^{N} \mathrm{I}\{f_{i,j}=f_{i,j}^{k}\}} \]</span></p><p>其中，<span class="math inline">\(0\leq \lambda \leq 1\)</span></p><p>变换一种形式：</p><p><span class="math display">\[f_{i,j}^{&#39;k}=\frac{\sum_{i=0}^{N}y_i \mathrm{I}\{f_{i,j}=f_{i,j}^{k}\} +\alpha^k p^k}{\sum_{i=0}^{N} \mathrm{I}\{f_{i,j}=f_{i,j}^{k}\}+\alpha^k}\]</span> 其中，<span class="math inline">\(p^k=\frac{\sum_{i=0}^{N}y_i}{N}\)</span>，<span class="math inline">\(\alpha^k=\frac{\sum_{i=0}^{N}\lambda^k \mathrm{I}\{f_{i,j}=f_{i,j}^{k}\}}{1-\lambda^k}\)</span>(显然<span class="math inline">\(0\leq\alpha\leq 1\)</span>)</p><p>大白话：用性别这个特征的平均ctr来平滑每个类别取值的ctr，假设，“女”的<span class="math inline">\(\lambda=0\)</span>，“男”的<span class="math inline">\(\lambda=0.5\)</span>，“未知”的<span class="math inline">\(\lambda=0.6\)</span>，则：</p><p>分组计算：</p><p>平均ctr：<span class="math inline">\(\overline{ctr} =\frac{3}{12}=0.25\)</span></p><p>“女”：<span class="math inline">\(0 \cdot \overline{ctr} + 1 \cdot ctr=\frac{3}{5}=0.6\)</span></p><p>“男”：<span class="math inline">\(0.5 \cdot \overline{ctr} + 0.5 \cdot ctr=0.5 \cdot 0.25+\frac{0}{4}=0.125\)</span></p><p>“未知”：<span class="math inline">\(0.6 \cdot \overline{ctr} + 0.4 \cdot ctr=0.6 \cdot 0.25+\frac{0}{3}=0.15\)</span></p><p>性别特征编码为：</p><table><thead><tr class="header"><th>样本编号</th><th style="text-align:center">性别特征</th><th style="text-align:center">性别特征编码</th></tr></thead><tbody><tr class="odd"><td>1</td><td style="text-align:center">女</td><td style="text-align:center">0.6</td></tr><tr class="even"><td>2</td><td style="text-align:center">女</td><td style="text-align:center">0.6</td></tr><tr class="odd"><td>3</td><td style="text-align:center">男</td><td style="text-align:center">0.125</td></tr><tr class="even"><td>4</td><td style="text-align:center">女</td><td style="text-align:center">0.6</td></tr><tr class="odd"><td>5</td><td style="text-align:center">男</td><td style="text-align:center">0.125</td></tr><tr class="even"><td>6</td><td style="text-align:center">未知</td><td style="text-align:center">0.15</td></tr><tr class="odd"><td>7</td><td style="text-align:center">女</td><td style="text-align:center">0.6</td></tr><tr class="even"><td>8</td><td style="text-align:center">男</td><td style="text-align:center">0.125</td></tr><tr class="odd"><td>9</td><td style="text-align:center">未知</td><td style="text-align:center">0.15</td></tr><tr class="even"><td>10</td><td style="text-align:center">女</td><td style="text-align:center">0.6</td></tr><tr class="odd"><td>11</td><td style="text-align:center">男</td><td style="text-align:center">0.125</td></tr><tr class="even"><td>12</td><td style="text-align:center">未知</td><td style="text-align:center">0.15</td></tr></tbody></table><p>这种方法的特点是简单，适用于内置到Boosting框架中，但缺点是会出现<strong>Target Leakage</strong>现象，例如： 假设类别特征依然是“性别”，如果在训练集上</p><table><thead><tr class="header"><th>样本编号</th><th style="text-align:center">性别特征</th><th style="text-align:center">是否点击广告</th></tr></thead><tbody><tr class="odd"><td>1</td><td style="text-align:center">女</td><td style="text-align:center">1</td></tr><tr class="even"><td>2</td><td style="text-align:center">女</td><td style="text-align:center">0</td></tr><tr class="odd"><td>3</td><td style="text-align:center">男</td><td style="text-align:center">1</td></tr><tr class="even"><td>4</td><td style="text-align:center">男</td><td style="text-align:center">0</td></tr></tbody></table><p>平均ctr：<span class="math inline">\(\overline{ctr} =\frac{2}{4}=0.5\)</span></p><p>“女”：<span class="math inline">\(p(y=1|女)=0.5\)</span></p><p>“男”：<span class="math inline">\(p(y=1|男)=0.5\)</span></p><p>编码：<span class="math inline">\(f_i^k=\frac{y_i+\alpha \overline{ctr}}{1+\alpha}\)</span></p><p>显然，在训练集上<span class="math inline">\(f_i^k=\frac{0.5+\alpha\overline{ctr}}{1+\alpha}\)</span>是最佳分割点，意味着在未来测试集上，不管特征分布是如何，“性别”这个类别特征所有取值下的编码一定是：</p><p><span class="math inline">\(f_i^k=\frac{0.5+\alpha\overline{ctr}}{1+\alpha}=\frac{0.5+\alpha0.5}{1+\alpha}=0.5\)</span>，显然模型过拟合了。</p><p>3)、<strong>Target Encoding with Beta Distribution</strong></p><p>通过加权平均方式，可以很好的平滑特征，但是缺点之一是超参数需要人工拍且不可解释，由于实际当中大部分分类问题是二分类（或可以转化为二分类），而二分类问题大多可以通过Bernoulli Distribution建模，而Bernoulli Distribution又有一个很好的正交分布Beta Distribution，因此利用贝叶斯MAP框架，可以假设参数服从Beta Distribution，即：</p><ul><li><p>点击率CTR：<span class="math inline">\(r \sim Beta (\alpha, \beta),P(r|\alpha,\beta) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}r^{\alpha-1}(1-r)^{\beta-1}\)</span></p></li><li><p>点击Clicks：<span class="math inline">\(C \sim Binomial(I,r),P(c|I,r)\propto{r^{C}}{(1-r)^{I-C}}\)</span></p></li></ul><p>其中<span class="math inline">\(C\)</span>为点击数，<span class="math inline">\(I\)</span>为曝光数，<span class="math inline">\(r\)</span>为点击率</p><p>则：</p><p><span class="math display">\[ \begin{equation*} \begin{aligned} P(C_i,...C_N|I_i,...I_N,\alpha,\beta)&amp;=\prod_{i=1}^{N}P(C_i|I_i,\alpha,\beta) \\ &amp;=\prod_{i=1}^{N}\int_{r_i} P(C_i,r_i|I_i,\alpha,\beta)dr_i \\ &amp;=\prod_{i=1}^{N}\int_{r_i} P(C_i|I_i,r_i)\cdot P(r_i|\alpha,\beta)dr_i \\ &amp;\propto \prod_{i=1}^{N}\int_{r_i}r_i^{C_i}(1-r_i)^{I_i-C_i}r_i^{\alpha-1}(1-r_i)^{\beta-1} \\ &amp;=\propto \prod_{i=1}^{N}\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\frac{\Gamma(C_i+\alpha)}{\Gamma(\alpha)}\frac{\Gamma(I_i-C_i+\beta)}{\Gamma(\beta)} \end{aligned} \end{equation*} \]</span></p><p>做参数估计，得到<span class="math inline">\(\alpha\)</span>和<span class="math inline">\(\beta\)</span>的估计<span class="math inline">\(\hat{\alpha}\)</span>和<span class="math inline">\(\hat{\beta}\)</span>，则平滑后的点击率<span class="math inline">\(\hat{r_i}\)</span>为：</p><p><span class="math display">\[ \begin{equation*} \begin{aligned} \hat{r_i}&amp;=\frac{C_i+\hat{\alpha_i}}{I_i+\hat{\alpha_i}+\hat{\beta_i}} \\ &amp;=\frac{\hat{\alpha_i}+\hat{\beta_i}}{I_i+\hat{\alpha_i}+\hat{\beta_i}} \cdot \frac{\hat{\alpha_i}}{\hat{\alpha_i}+\hat{\beta_i}} +\frac{I_i}{I_i+\hat{\alpha_i}+\hat{\beta_i}} \cdot \frac{C_i}{I_i} \\ &amp;=\frac{\hat{\alpha_i}+\hat{\beta_i}}{I_i+\hat{\alpha_i}+\hat{\beta_i}}\cdot \frac{\hat{\alpha_i}}{\hat{\alpha_i}+\hat{\beta_i}} + (1-\frac{\hat{\alpha_i}+\hat{\beta_i}}{I_i+\hat{\alpha_i}+\hat{\beta_i}}) \cdot \frac{C_i}{I_i} \\ &amp;=\lambda_i \cdot \overline{ctr}+(1-\lambda_i) \cdot ctr_i \end{aligned} \end{equation*} \]</span></p><p>最终，新的编码方式如下：</p><p><span class="math display">\[ f_{i,j}^{&#39;k}= \lambda^k \frac{\sum_{i=0}^{N}y_i}{N} + (1-\lambda^k)\frac{\sum_{i=0}^{N}y_i \mathrm{I}\{f_{i,j}=f_{i,j}^{k}\} }{\sum_{i=0}^{N} \mathrm{I}\{f_{i,j}=f_{i,j}^{k}\}} \]</span></p><p>其中<span class="math inline">\(\lambda^k=\frac{\hat{\alpha}+\hat{\beta}}{\mathrm{I}\{f_{i,j}=f_{i,j}^{k}\}+\hat{\alpha}+\hat{\beta}}\)</span></p><p>这种方法实践效果更好，尤其在特征工程阶段，但它计算复杂，相当于在用一个模型生成特征，且不适合内置在Boost类框架中。</p><p>4)、<strong>Handout Target Statistics</strong></p><p>针对Greedy Target Statistics with Smoothes遇到的问题，一种改进是将训练集分成两部分，一部分用来生成TS特征，一部分用来训练，但显然可用训练数据变少了，尤其在标注样本不是那么rich的场景下。</p><p>5)、<strong>Leave-one-out Target Statistics</strong></p><p>一般会和交叉验证配合使用，简单说就是训练集留1个样本做测试，其他样本做训练，假设<span class="math inline">\(f_{i,j}^k\)</span>为当前被“leave”的样本，则其编码后结果为：</p><p><span class="math display">\[ f_{i,j}^{&#39;k}=\frac{\sum_{i=0}^{N}y_i \mathrm{I}\{f_{i,j}=f_{i,j}^{k}\}-y_k +\alpha^k p^k}{\sum_{i=0}^{N} \mathrm{I}\{f_{i,j}=f_{i,j}^{k}\}-1+\alpha^k} \]</span></p><p>训练集最佳分裂点是：</p><p><span class="math display">\[ f_{i,j}^{&#39;k}=\frac{\sum_{i=0}^{N}y_i \mathrm{I}\{f_{i,j}=f_{i,j}^{k}\}-0.5 +\alpha^k p^k}{\sum_{i=0}^{N} \mathrm{I}\{f_{i,j}=f_{i,j}^{k}\}-1+\alpha^k} \]</span> 显然，发生了“Target Leakage”导致模型过拟合。</p><p>假设类别特征依然是“性别”，如果在训练集上</p><table><thead><tr class="header"><th>样本编号</th><th style="text-align:center">性别特征</th><th style="text-align:center">是否点击广告</th></tr></thead><tbody><tr class="odd"><td>1</td><td style="text-align:center">女</td><td style="text-align:center">1</td></tr><tr class="even"><td>2</td><td style="text-align:center">女</td><td style="text-align:center">0</td></tr><tr class="odd"><td>3</td><td style="text-align:center">男</td><td style="text-align:center">1</td></tr><tr class="even"><td>4</td><td style="text-align:center">男</td><td style="text-align:center">0</td></tr><tr class="odd"><td>1</td><td style="text-align:center">女</td><td style="text-align:center">0</td></tr><tr class="even"><td>2</td><td style="text-align:center">女</td><td style="text-align:center">0</td></tr><tr class="odd"><td>3</td><td style="text-align:center">男</td><td style="text-align:center">1</td></tr><tr class="even"><td>4</td><td style="text-align:center">男</td><td style="text-align:center">1</td></tr></tbody></table><p>“女”编码为：<span class="math inline">\(p(y=1|女)=\frac{1-1+ap}{4-1+a}=\frac{ap}{3+a}\)</span></p><p>“女”编码为：<span class="math inline">\(p(y=0|女)=\frac{1-0+ap}{4-1+a}=\frac{1+ap}{3+a}\)</span></p><p>显然“女”的最佳分割点是：<span class="math inline">\(\frac{0.5+ap}{3+a}\)</span></p><p>“男”编码为：<span class="math inline">\(p(y=1|男)=\frac{3-1+ap}{4-1+a}=\frac{2+ap}{3+a}\)</span></p><p>“男”编码为：<span class="math inline">\(p(y=0|男)=\frac{3-0+ap}{4-1+a}=\frac{3+ap}{3+a}\)</span></p><p>显然“男”的最佳分割点是：<span class="math inline">\(\frac{2.5+ap}{3+a}\)</span></p><p>出现过拟合！</p><p>6)、<strong>Ordered Target Statistics</strong></p><p>借鉴Online Learning的方式，所有样本的TS特征生成只依赖当前时间线之前的历史样本，如果样本没有时间属性，则利用某个分布生成随机数，为所有样本赋予"顺序"。为了降低模型方差，每步Gradient Boosting迭代会重新为样本赋予“顺序”。</p><ul><li>Oblivious Decision Trees</li></ul>基于Oblivious Tree的决策树，Oblivious有人翻译为对称，有人翻译为遗忘，前者体现了结构上的特点，后者体现了行为上的特点。 从结构上讲，只有一个入度为0的节点且为根节点，所有叶子节点(出度为0)为分类节点，其他节点为中间节点，任意一条从根节点到叶子节点的路径，每个中间节点只出现一次，中间节点，即internal或test node为分类变量，每一层的中间节点变量一样，<strong>注意，每层的每个分类变量一样但判断条件可以不一样</strong>，所以结构上看是对称的，似乎叫Symmetric Tree更合适。 作者把它称为Oblivious，我认为是基于它行为上的特点，即每个分类变量(中间节点)的触发与整个路径上的所有变量的顺序及它们所处的层次决定，而不是由该变量节点本身决定，所以对比传统决策树，似乎“遗忘”了历史的判断行为，每次都要依着从根节点开始的路径对每个分类变量逐个做判断。典型的结构如下图：<center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1e366blci7k512pq1vduq2v6519.png" width="600"></center>注意看，在age这个属性上，不同的路径可以有不同的判断条件，这种结构在特征选择上比较高效。再一个，每个到达叶子节点的路径上的分类变量都是一样的，这些变量一定是所有输入变量的子集，从这个角度看，其实是在做<strong>维度缩减</strong>。此外Oblivious Tree也可以看做是Oblivious Oblivious Read-Once Decision Graphs的展开形式，如下图：<center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1e364p1q9rkocvq1juo1cs96jq9.png" width="600"></center><p>详情可以看《<strong>Bottom-Up Induction of Oblivious Read-Once Decision Graphs</strong>》一文。</p><ul><li>Decision Table</li></ul><p>决策表是一种古老但是非常高效的规则匹配方法。用于基于规则的决策系统，如早期的专家系统，匹配模式形如：<strong>if A then B</strong>，最典型的有两类：</p><pre><code>* Condition-Action Rules

A为条件，B为动作，例如：if x发了工资 then x去银行还月供。

* Logical Rules

A和B为一阶逻辑表达式，例如：if x是男人 then x是哺乳动物。</code></pre><p>利用Decision Table可以做分类器，例如最常用的Decision Table Majority，它有两个组成部分：</p><pre><code>* Schema，所有决策表中要用到特征的集合。

* Body，依据训练数据得到的命中模式的集合。</code></pre><p>典型的例子如下：</p><center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1e36utda26u3k4712t5d8d10001g.png" width="600"></center><p>回顾Oblivious Decision Trees的结构，非常适合实现一个DTM，定义某个损失函数，构造ODT，最终保留下的中间分类节点集合为Schema，从根节点到叶子节点的所有路径组成的集合是Body。</p><ul><li>CatBoost简介</li></ul><p>CatBoost是Yandex在2017年推出的一个GBDT实现框架，论文见：《<a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/7898-catboost-unbiased-boosting-with-categorical-features.pdf">CatBoost: unbiased boosting with categorical features</a>》，它的基础学习器是Oblivious Decision Trees，结构上平衡、不容易过拟合、Inference速度极快，支持自动将类别特征处理为数值特征并做特征交叉，为提高模型精度和泛化性能，在Prediction Shift和Gradient Bias上做了理论分析和解决。</p><p>1、<strong>Prediction Shift</strong></p><p>简单回顾第二章Boosting相关内容： <span class="math display">\[ \begin{array}{l} F(x)=\sum_{i=1}^Nw_if_i(x)\\ 其中:\\ \quad \quad f_i是第i轮迭代得到的基础分类器\\ \quad \quad w_i是第i个分类器的权重\\ \quad \quad x是样本特征集\\ 定义:\\ \quad \quad \text{(1). 样本总数为 $N$,损失函数定义为$C(F)$,例如： $C(F(x))=\frac{1}{N}\sum_{i=1}^NC(y_iF(x_i))$}\\ \quad \quad \text{(2). $g^t=g^t(x,y)=\frac{\partial C(y,s)}{\partial s}|_{s=F^{t-1}(x)}$,$h^t=h^t(x,y)=\frac{\partial ^2C(y,s)}{\partial s^2}|_{s=F^{t-1}(x)}$}\\ \text{现在我们希望当前轮迭代能够找到一个函数 $f(x)$ ,使得在新分类器$F^t(x)=F^{t-1}(x)+\alpha f(x)$下的损失函数值 $C(F^{t-1}(x)+\alpha f(x))$ 能够下降}\\ 依据泰勒展开式:\\ \because C(F^t(x))=C(F^{t-1}(x)+\alpha f(x))=C(F^{t-1}(x))+\alpha {g^t} f+\frac{1}{2}h^t(\xi)f^2\\ \therefore 根据拉格朗日定理得到:\\ f=-\alpha g^t\\ 由于f是个函数，所以通常用利用最小二乘法对其做拟合:\\ f^t=\mathop{argmin}\limits_{f \in F}~ \mathbb{E}(-f(x)-g^t(x,y))^2 \end{array} \]</span> 这里会有三个问题，即所谓shift，导致最终模型泛化能力下降：</p><blockquote><p>1、训练样本和测试样本梯度值的分布可能不一致；</p></blockquote><blockquote><p>2、采用类似最小二乘法做函数拟合可能出现偏差，因为背后的假设是梯度值分布服从正态分布，可能与真实世界里的分布不一致；</p></blockquote><blockquote><p>3、如果每步迭代都使用相同数据集，则得到的模型是有偏的且偏差与数据集大小成反比；反之，如果每步迭代使用相互独立的数据集，则得到的训练模型是无偏的。</p></blockquote><p>以上shift最终造成预测时出现Prediction Shift，假设我们有无限大的标注数据集，那就简单了，每步迭代都独立的抽一个数据集出来，可以训练出无偏模型，但实际上不可能，所以这个问题只可以缓解不能解决，因为你永远不能准确知道真实样本的完美分布，只能摸着石头过河，广告、推荐和搜索里把这个方法叫做E&amp;E(Exploitation &amp; Exploration)，本质上Boostng Tree的生成过程就是通过E&amp;E方法迭代生成：使用某个样本集exploit得到当前“最优”的模型，即<span class="math inline">\(F^{t-1}(x)\)</span>，用这个模型explore另外一个样本集，根据结果和某种策略优化模型得到新的“最优”模型<span class="math inline">\(F^t(x)\)</span>，如此往复，直到找到Exploitation &amp; Exploration的最佳trade-off点。</p><p>2、 <strong>Ordered boosting</strong></p><p>为了缓解Prediction Shift，一个取巧的方法是：</p><p>• 用随机排序函数<span class="math inline">\(\sigma\)</span>对所有样本重排序</p><p>• 维护<span class="math inline">\(n\)</span>个独立的模型，每个模型<span class="math inline">\(M_i\)</span>由排序后的前<span class="math inline">\(i\)</span>个训练样本生成</p><p>• 对第<span class="math inline">\(j\)</span>个样本的残差用模型<span class="math inline">\(M_{j-1}\)</span>来估计</p><p>• 原来每个迭代使用由相同样本得到的模型F求梯度的方式改为用辅助模型M估计：</p><span class="math display">\[g^t:=\frac{\partial L(y,s)}{s}|_{s=F^{t-1}(x)}\Rightarrow g^t:=\frac{\partial L(y,s)}{s}|_{s=M^{t-1}(x)}\]</span><center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1e3er90lfo882vu1v234415gu1j.png" width="400"></center><p><strong>Algorithm 1:</strong> <strong>Ordered Boosting</strong></p><blockquote><p><strong><em>input:</em></strong> <span class="math inline">\(\{(x_k,y_k)\}_{k=1}^n为所有样本,且由\sigma函数重新随机排序, T为树的个数\)</span></p></blockquote><blockquote><p><span class="math inline">\(M_i \leftarrow 0 \quad for \quad i=1..n,n为模型个数\)</span></p></blockquote><blockquote><p><span class="math inline">\(for \quad t\leftarrow1 \quad to \quad T \quad do\)</span></p></blockquote><blockquote><p><span class="math inline">\(\quad for \quad i\leftarrow1 \quad to \quad n \quad do\)</span></p></blockquote><blockquote><p><span class="math inline">\(\quad \quad for \quad j\leftarrow1 \quad to \quad i \quad do\)</span></p></blockquote><blockquote><p><span class="math inline">\(\quad \quad \quad\quad g_j\leftarrow\frac{\partial L(y_j,s)}{s}|_{s=M_{j-1}(x_j)}(采用最小二乘损失函数:g_j\leftarrow y_j-M_{j-1}(x_j)\)</span></p></blockquote><blockquote><p><span class="math inline">\(\quad \quad \Delta M\leftarrow LearnModel(x_j,g_j)\quad for \quad j=1..i\)</span></p></blockquote><blockquote><p><span class="math inline">\(\quad \quad M_i=M_i+\Delta M\)</span></p></blockquote><blockquote><p><span class="math inline">\(return \quad M_n\)</span></p></blockquote><p>由于要训练<span class="math inline">\(n\)</span>个模型，时间和空间复杂度都上升了<span class="math inline">\(n\)</span>倍，所以这个算法实操性较低。 CatBoost对建树算法做了改进，有对<strong>Algorithm 1</strong>算法效率改进的Ordered模式和类似于传统GBDT的Plain模式，其中Ordered模式在小数据集上优势明显。</p><p>建树的算法<strong>Algorithm 2</strong>说明如下：</p><center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/a2.png" width="800"></center><p>在Ordered模式下，计算梯度的时间复杂度是<span class="math inline">\(O(sn^2)\)</span>，包含选择排序函数的时间、扫描样本生成辅助函数的时间、扫描样本生成梯度的时间，CatBoost在实现时用了几个技巧：</p><p>1、不维护全部<span class="math inline">\(M_{r,j}(i)\)</span>，转而只维护<span class="math inline">\(M^{&#39;}_{r,j}(i)=M_{r,2^j}(i),j=1,...,\lceil log_2^n\rceil,\sigma_r(i)\leq2^{j+1}\)</span>，这样时间复杂度会降到<span class="math inline">\(O(sn)\)</span></p><p>2、每次迭代时对样本做抽样，这个方法能有效降低过拟合</p><p>3、由产生样本过程导致的bias，例如，排第一页的广告比第100页的广告更容易被用户看到、点到，那第100页的广告如果被点击了，这条样本的含金量明显更高，所以位置bias产生的样本可以通过给样本权重的方式或把位置作为特征等方法校正，相关论文可以看：</p><p>《<a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/research/wp-content/uploads/2017/04/main-1.pdf">Model Ensemble for Click Prediction in Bing Search Ads</a>》</p><p>《<a target="_blank" rel="noopener" href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/45286.pdf">Learning to Rank with Selection Bias in Personal Search</a>》</p><p>《<a target="_blank" rel="noopener" href="https://www.researchgate.net/publication/335771749_PAL_a_position-bias_aware_learning_framework_for_CTR_prediction_in_live_recommender_systems">PAL: a position-bias aware learning framework for CTR prediction in live recommender systems</a>》</p><p>《<a target="_blank" rel="noopener" href="https://projecteuclid.org/euclid.aos/1176345338">The bayesian bootstrap</a>》</p><p>3、<strong>完整算法描述</strong></p><center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1e3jn8bq91vlofsk3tc1lb516h22c.png" width="800"></center><center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1e3jnra3u19v71ec21djqff31mtt2p.png" width="800"></center><p>4、<strong>代码实践</strong></p><p>CatBoost官网上有大量教程，这里给个简单例子：</p><p>1、定义Model类</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=UTF-8</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    class CatBoostModel</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedShuffleSplit</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"><span class="keyword">import</span> gc</span><br><span class="line"><span class="keyword">import</span> catboost <span class="keyword">as</span> cb</span><br><span class="line"><span class="keyword">from</span> catboost <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CatBoostModel</span>():</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="keyword">assert</span> kwargs[<span class="string">&#x27;catboost_params&#x27;</span>]       <span class="comment"># 定义模型参数</span></span><br><span class="line">        <span class="keyword">assert</span> kwargs[<span class="string">&#x27;eval_ratio&#x27;</span>]            <span class="comment"># 定义训练集划分比例</span></span><br><span class="line">        <span class="keyword">assert</span> kwargs[<span class="string">&#x27;early_stopping_rounds&#x27;</span>] <span class="comment"># 缓解过拟合，提前结束迭代参数</span></span><br><span class="line">        <span class="keyword">assert</span> kwargs[<span class="string">&#x27;num_boost_round&#x27;</span>]       <span class="comment"># 控制树的个数</span></span><br><span class="line">        <span class="keyword">assert</span> kwargs[<span class="string">&#x27;cat_features&#x27;</span>]          <span class="comment"># 类别特征列表</span></span><br><span class="line">        <span class="keyword">assert</span> kwargs[<span class="string">&#x27;all_features&#x27;</span>]          <span class="comment"># 所有特征列表</span></span><br><span class="line"></span><br><span class="line">        self.catboost_params = kwargs[<span class="string">&#x27;catboost_params&#x27;</span>]</span><br><span class="line">        self.eval_ratio = kwargs[<span class="string">&#x27;eval_ratio&#x27;</span>]</span><br><span class="line">        self.early_stopping_rounds = kwargs[<span class="string">&#x27;early_stopping_rounds&#x27;</span>]</span><br><span class="line">        self.num_boost_round = kwargs[<span class="string">&#x27;num_boost_round&#x27;</span>]</span><br><span class="line">        self.cat_features = kwargs[<span class="string">&#x27;cat_features&#x27;</span>]</span><br><span class="line">        self.all_features = kwargs[<span class="string">&#x27;all_features&#x27;</span>]</span><br><span class="line"></span><br><span class="line">        self.selected_features_ = <span class="literal">None</span></span><br><span class="line">        self.X = <span class="literal">None</span></span><br><span class="line">        self.y = <span class="literal">None</span></span><br><span class="line">        self.model = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_selected_features</span>(<span class="params">self, topk</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Fit the training data to FeatureSelector</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        list :</span></span><br><span class="line"><span class="string">                Return the index of imprtant feature.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">assert</span> topk &gt; <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        self.selected_features_ = self.feature_importance.argsort()[-topk:][::-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.selected_features_</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, X, num_iteration=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.model.predict(X, num_iteration)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_fea_importance</span>(<span class="params">self, clf, columns</span>):</span></span><br><span class="line">        importances = clf.feature_importances_</span><br><span class="line">        indices = np.argsort(importances)[::-<span class="number">1</span>]</span><br><span class="line">        importance_list = []</span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(columns)):</span><br><span class="line">            importance_list.append((columns[indices[f]], importances[indices[f]]))</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;%2d) %-*s %f&quot;</span> % (f + <span class="number">1</span>, <span class="number">30</span>, columns[indices[f]], importances[indices[f]]))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;another feature importances with prettified=True\n&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(clf.get_feature_importance(prettified=<span class="literal">True</span>))</span><br><span class="line">        importance_df = pd.DataFrame(importance_list, columns=[<span class="string">&#x27;Features&#x27;</span>, <span class="string">&#x27;Importance&#x27;</span>])</span><br><span class="line">        <span class="keyword">return</span> importance_df</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_test_split</span>(<span class="params">self, X, y, test_size, random_state=<span class="number">2020</span></span>):</span></span><br><span class="line">        sss = <span class="built_in">list</span>(StratifiedShuffleSplit(</span><br><span class="line">            n_splits=<span class="number">1</span>, test_size=test_size, random_state=random_state).split(X, y))</span><br><span class="line">        X_train = np.take(X, sss[<span class="number">0</span>][<span class="number">0</span>], axis=<span class="number">0</span>)</span><br><span class="line">        X_eval = np.take(X, sss[<span class="number">0</span>][<span class="number">1</span>], axis=<span class="number">0</span>)</span><br><span class="line">        y_train = np.take(y, sss[<span class="number">0</span>][<span class="number">0</span>], axis=<span class="number">0</span>)</span><br><span class="line">        y_eval = np.take(y, sss[<span class="number">0</span>][<span class="number">1</span>], axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> [X_train, X_eval, y_train, y_eval]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">catboost_model_train</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                        df,</span></span></span><br><span class="line"><span class="params"><span class="function">                        finetune=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                        target_name=<span class="string">&#x27;Label&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                        id_index=<span class="string">&#x27;Id&#x27;</span></span>):</span></span><br><span class="line">        df = df.loc[df[target_name].isnull() == <span class="literal">False</span>]</span><br><span class="line">        feature_name = [i <span class="keyword">for</span> i <span class="keyword">in</span> df.columns <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> [target_name, id_index]]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> feature_name:</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">in</span> self.cat_features:</span><br><span class="line">                <span class="comment">#df[i].fillna(-999, inplace=True)</span></span><br><span class="line">                <span class="keyword">if</span> df[i].fillna(<span class="string">&#x27;na&#x27;</span>).nunique() &lt; <span class="number">12</span>:</span><br><span class="line">                    df.loc[:, i] = df.loc[:, i].fillna(<span class="string">&#x27;na&#x27;</span>).astype(<span class="string">&#x27;category&#x27;</span>)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    df.loc[:, i] = LabelEncoder().fit_transform(df.loc[:, i].fillna(<span class="string">&#x27;na&#x27;</span>).astype(<span class="built_in">str</span>))</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">type</span>(df.loc[<span class="number">0</span>,i])!=<span class="built_in">str</span> <span class="keyword">or</span> <span class="built_in">type</span>(df.loc[<span class="number">0</span>,i])!=<span class="built_in">int</span> <span class="keyword">or</span>  <span class="built_in">type</span>(df.loc[<span class="number">0</span>,i])!=long:</span><br><span class="line">                    df.loc[:, i] = df.loc[:, i].astype(<span class="built_in">str</span>)</span><br><span class="line"></span><br><span class="line">        X_train, X_eval, y_train, y_eval = self.train_test_split(df[feature_name],</span><br><span class="line">                                                          df[target_name].values,</span><br><span class="line">                                                          self.eval_ratio,</span><br><span class="line">                                                          random.seed(<span class="number">41</span>))</span><br><span class="line">        <span class="keyword">del</span> df</span><br><span class="line">        gc.collect()</span><br><span class="line"></span><br><span class="line">        catboost_train = Pool(data=X_train, label=y_train, cat_features=self.cat_features, feature_names=self.all_features)</span><br><span class="line">        catboost_eval = Pool(data=X_eval, label=y_eval, cat_features=self.cat_features, feature_names=self.all_features)</span><br><span class="line"></span><br><span class="line">        self.model = cb.train(params=self.catboost_params,</span><br><span class="line">                               init_model=finetune,</span><br><span class="line">                               pool=catboost_train,</span><br><span class="line">                               num_boost_round=self.num_boost_round,</span><br><span class="line">                               eval_set=catboost_eval,</span><br><span class="line">                               verbose_eval=<span class="number">50</span>,</span><br><span class="line">                               plot=<span class="literal">True</span>,</span><br><span class="line">                               early_stopping_rounds=self.early_stopping_rounds)</span><br><span class="line"></span><br><span class="line">        self.feature_importance  = self.get_fea_importance(self.model, self.all_features)</span><br><span class="line">        metrics = self.model.eval_metrics(data=catboost_eval,metrics=[<span class="string">&#x27;AUC&#x27;</span>],plot=<span class="literal">True</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;AUC values:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(np.array(metrics[<span class="string">&#x27;AUC&#x27;</span>])))</span><br><span class="line">        <span class="keyword">return</span> self.feature_importance, metrics, self.model</span><br></pre></td></tr></table></figure>2、定义catboost trainer（里面涉及nni的部分先忽略，后面再讲）<p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=UTF-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> bz2</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> os.path</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_svmlight_file</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="keyword">import</span> nni</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"><span class="keyword">import</span> gc</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> CatBoostModel</span><br><span class="line"><span class="keyword">from</span> catboost.datasets <span class="keyword">import</span> adult</span><br><span class="line"></span><br><span class="line">logger = logging.getLogger(<span class="string">&#x27;auto_gbdt&#x27;</span>)</span><br><span class="line"></span><br><span class="line">TARGET_NAME = <span class="string">&#x27;income&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_default_parameters</span>():</span></span><br><span class="line">    params_cb = &#123;</span><br><span class="line">       <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;Ordered&#x27;</span>,</span><br><span class="line">       <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;CrossEntropy&#x27;</span>,</span><br><span class="line">       <span class="string">&#x27;eval_metric&#x27;</span>: <span class="string">&#x27;AUC&#x27;</span>,</span><br><span class="line">       <span class="string">&#x27;custom_metric&#x27;</span>: [<span class="string">&#x27;AUC&#x27;</span>, <span class="string">&#x27;Accuracy&#x27;</span>],</span><br><span class="line">       <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.05</span>,</span><br><span class="line">       <span class="string">&#x27;random_seed&#x27;</span>: <span class="number">2020</span>,</span><br><span class="line">       <span class="string">&#x27;depth&#x27;</span>: <span class="number">2</span>,</span><br><span class="line">       <span class="string">&#x27;l2_leaf_reg&#x27;</span>: <span class="number">3.8</span>,</span><br><span class="line">       <span class="string">&#x27;thread_count&#x27;</span>: <span class="number">16</span>,</span><br><span class="line">       <span class="string">&#x27;use_best_model&#x27;</span>: <span class="literal">True</span>,</span><br><span class="line">       <span class="string">&#x27;verbose&#x27;</span>: <span class="number">0</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> params_cb</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_features_name</span>():</span></span><br><span class="line">    alls = [<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;workclass&#x27;</span>, <span class="string">&#x27;fnlwgt&#x27;</span>, <span class="string">&#x27;education&#x27;</span>, <span class="string">&#x27;education-num&#x27;</span>, <span class="string">&#x27;marital-status&#x27;</span>, <span class="string">&#x27;occupation&#x27;</span>, <span class="string">&#x27;relationship&#x27;</span>, <span class="string">&#x27;race&#x27;</span>, <span class="string">&#x27;sex&#x27;</span>, <span class="string">&#x27;capital-gain&#x27;</span>,  <span class="string">&#x27;capital-loss&#x27;</span>,  <span class="string">&#x27;hours-per-week&#x27;</span>, <span class="string">&#x27;native-country&#x27;</span>]</span><br><span class="line">    cats = [<span class="string">&#x27;workclass&#x27;</span>, <span class="string">&#x27;education&#x27;</span>, <span class="string">&#x27;marital-status&#x27;</span>, <span class="string">&#x27;occupation&#x27;</span>, <span class="string">&#x27;relationship&#x27;</span>, <span class="string">&#x27;race&#x27;</span>, <span class="string">&#x27;sex&#x27;</span>, <span class="string">&#x27;native-country&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> alls, cats</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_prepare_cleaner</span>(<span class="params">target</span>):</span></span><br><span class="line">    train_df, test_df = adult()</span><br><span class="line">    le = LabelEncoder()</span><br><span class="line">    le.fit(train_df[target])</span><br><span class="line">    train_df[target] = le.transform(train_df[target])</span><br><span class="line"></span><br><span class="line">    le.fit(test_df[target])</span><br><span class="line">    test_df[target] = le.transform(test_df[target])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_df, test_df</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cat_fea_cleaner</span>(<span class="params">df, target_name, id_index, cat_features</span>):</span></span><br><span class="line">    df = df.loc[df[target_name].isnull() == <span class="literal">False</span>]</span><br><span class="line">    feature_name = [i <span class="keyword">for</span> i <span class="keyword">in</span> df.columns <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> [target_name, id_index]]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> feature_name:</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> cat_features:</span><br><span class="line">            <span class="keyword">if</span> df[i].fillna(<span class="string">&#x27;na&#x27;</span>).nunique() &lt; <span class="number">12</span>:</span><br><span class="line">                df.loc[:, i] = df.loc[:, i].fillna(<span class="string">&#x27;na&#x27;</span>).astype(<span class="string">&#x27;category&#x27;</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                df.loc[:, i] = LabelEncoder().fit_transform(df.loc[:, i].fillna(<span class="string">&#x27;na&#x27;</span>).astype(<span class="built_in">str</span>))</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">type</span>(df.loc[<span class="number">0</span>,i])!=<span class="built_in">str</span> <span class="keyword">or</span> <span class="built_in">type</span>(df.loc[<span class="number">0</span>,i])!=<span class="built_in">int</span> <span class="keyword">or</span>  <span class="built_in">type</span>(df.loc[<span class="number">0</span>,i])!=long:</span><br><span class="line">                df.loc[:, i] = df.loc[:, i].astype(<span class="built_in">str</span>)</span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainer_and_tester_run</span>(<span class="params">train_df, test_df, all_features, cat_features</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># get parameters from tuner</span></span><br><span class="line">    RECEIVED_PARAMS = nni.get_next_parameter()</span><br><span class="line">    logger.debug(RECEIVED_PARAMS)</span><br><span class="line">    PARAMS = get_default_parameters()</span><br><span class="line">    PARAMS.update(RECEIVED_PARAMS)</span><br><span class="line">    logger.debug(PARAMS)</span><br><span class="line"></span><br><span class="line">    cb = CatBoostModel(catboost_params=PARAMS,</span><br><span class="line">                    eval_ratio=<span class="number">0.33</span>,</span><br><span class="line">                    early_stopping_rounds=<span class="number">20</span>,</span><br><span class="line">                    cat_features=cat_features,</span><br><span class="line">                    all_features=all_features,</span><br><span class="line">                    num_boost_round=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">    logger.debug(<span class="string">&quot;The trainning process is starting...&quot;</span>)</span><br><span class="line">    train_df = cat_fea_cleaner(train_df, TARGET_NAME, <span class="string">&#x27;index&#x27;</span>, cat_features)</span><br><span class="line">    test_df = cat_fea_cleaner(test_df, TARGET_NAME, <span class="string">&#x27;index&#x27;</span>, cat_features)</span><br><span class="line">    feature_imp, val_score, clf = \</span><br><span class="line">    cb.catboost_model_train(df=train_df,</span><br><span class="line">                            target_name=TARGET_NAME,</span><br><span class="line">                            id_index=<span class="string">&#x27;id&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    logger.info(feature_imp)</span><br><span class="line">    logger.info(val_score)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">del</span> train_df</span><br><span class="line">    gc.collect()</span><br><span class="line">    logger.debug(<span class="string">&quot;The trainning process is ended.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    av_auc = inference(clf, test_df, all_features, cat_features)</span><br><span class="line">    nni.report_final_result(av_auc)</span><br><span class="line">    <span class="keyword">del</span> test_df</span><br><span class="line">    gc.collect()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inference</span>(<span class="params">clf, test_df, fea, cat_fea</span>):</span></span><br><span class="line"></span><br><span class="line">    logger.debug(<span class="string">&quot;The testing process is starting...&quot;</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        y_pred = clf.predict(test_df[fea])</span><br><span class="line">        auc = roc_auc_score(test_df[TARGET_NAME].values, y_pred)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;auc of prediction:&#123;0&#125;&quot;</span>.<span class="built_in">format</span>(auc))</span><br><span class="line">        <span class="keyword">del</span> test_df</span><br><span class="line">        gc.collect()</span><br><span class="line"></span><br><span class="line">        logger.debug(<span class="string">&quot;The inference process is ended.&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> auc</span><br><span class="line">    <span class="keyword">except</span> ValueError:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_offline</span>():</span></span><br><span class="line">    alls, cats = get_features_name()</span><br><span class="line">    <span class="built_in">print</span>(alls, cats)</span><br><span class="line">    train_df, test_df = data_prepare_cleaner(TARGET_NAME)</span><br><span class="line">    <span class="built_in">print</span>(train_df)</span><br><span class="line">    <span class="built_in">print</span>(test_df)</span><br><span class="line">    trainer_and_tester_run(train_df, test_df, alls, cats)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    run_offline()</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br></pre></td><td class="code"><pre><span class="line">[root@GPU-AI01 cat_test]# python3 catboost_trainer.py</span><br><span class="line">[&#x27;age&#x27;, &#x27;workclass&#x27;, &#x27;fnlwgt&#x27;, &#x27;education&#x27;, &#x27;education-num&#x27;, &#x27;marital-status&#x27;, &#x27;occupation&#x27;, &#x27;relationship&#x27;, &#x27;race&#x27;, &#x27;sex&#x27;, &#x27;capital-gain&#x27;, &#x27;capital-loss&#x27;, &#x27;hours-per-week&#x27;, &#x27;native-country&#x27;] [&#x27;workclass&#x27;, &#x27;education&#x27;, &#x27;marital-status&#x27;, &#x27;occupation&#x27;, &#x27;relationship&#x27;, &#x27;race&#x27;, &#x27;sex&#x27;, &#x27;native-country&#x27;]</span><br><span class="line">        age         workclass    fnlwgt   education  education-num      marital-status         occupation  ...   race     sex capital-gain  capital-loss  hours-per-week  native-country income</span><br><span class="line">0      39.0         State-gov   77516.0   Bachelors           13.0       Never-married       Adm-clerical  ...  White    Male       2174.0           0.0            40.0   United-States      0</span><br><span class="line">1      50.0  Self-emp-not-inc   83311.0   Bachelors           13.0  Married-civ-spouse    Exec-managerial  ...  White    Male          0.0           0.0            13.0   United-States      0</span><br><span class="line">2      38.0           Private  215646.0     HS-grad            9.0            Divorced  Handlers-cleaners  ...  White    Male          0.0           0.0            40.0   United-States      0</span><br><span class="line">3      53.0           Private  234721.0        11th            7.0  Married-civ-spouse  Handlers-cleaners  ...  Black    Male          0.0           0.0            40.0   United-States      0</span><br><span class="line">4      28.0           Private  338409.0   Bachelors           13.0  Married-civ-spouse     Prof-specialty  ...  Black  Female          0.0           0.0            40.0            Cuba      0</span><br><span class="line">...     ...               ...       ...         ...            ...                 ...                ...  ...    ...     ...          ...           ...             ...             ...    ...</span><br><span class="line">32556  27.0           Private  257302.0  Assoc-acdm           12.0  Married-civ-spouse       Tech-support  ...  White  Female          0.0           0.0            38.0   United-States      0</span><br><span class="line">32557  40.0           Private  154374.0     HS-grad            9.0  Married-civ-spouse  Machine-op-inspct  ...  White    Male          0.0           0.0            40.0   United-States      1</span><br><span class="line">32558  58.0           Private  151910.0     HS-grad            9.0             Widowed       Adm-clerical  ...  White  Female          0.0           0.0            40.0   United-States      0</span><br><span class="line">32559  22.0           Private  201490.0     HS-grad            9.0       Never-married       Adm-clerical  ...  White    Male          0.0           0.0            20.0   United-States      0</span><br><span class="line">32560  52.0      Self-emp-inc  287927.0     HS-grad            9.0  Married-civ-spouse    Exec-managerial  ...  White  Female      15024.0           0.0            40.0   United-States      1</span><br><span class="line"></span><br><span class="line">[32561 rows x 15 columns]</span><br><span class="line">        age     workclass    fnlwgt     education  education-num      marital-status  ...     sex capital-gain capital-loss hours-per-week  native-country  income</span><br><span class="line">0      25.0       Private  226802.0          11th            7.0       Never-married  ...    Male          0.0          0.0           40.0   United-States       0</span><br><span class="line">1      38.0       Private   89814.0       HS-grad            9.0  Married-civ-spouse  ...    Male          0.0          0.0           50.0   United-States       0</span><br><span class="line">2      28.0     Local-gov  336951.0    Assoc-acdm           12.0  Married-civ-spouse  ...    Male          0.0          0.0           40.0   United-States       1</span><br><span class="line">3      44.0       Private  160323.0  Some-college           10.0  Married-civ-spouse  ...    Male       7688.0          0.0           40.0   United-States       1</span><br><span class="line">4      18.0           NaN  103497.0  Some-college           10.0       Never-married  ...  Female          0.0          0.0           30.0   United-States       0</span><br><span class="line">...     ...           ...       ...           ...            ...                 ...  ...     ...          ...          ...            ...             ...     ...</span><br><span class="line">16276  39.0       Private  215419.0     Bachelors           13.0            Divorced  ...  Female          0.0          0.0           36.0   United-States       0</span><br><span class="line">16277  64.0           NaN  321403.0       HS-grad            9.0             Widowed  ...    Male          0.0          0.0           40.0   United-States       0</span><br><span class="line">16278  38.0       Private  374983.0     Bachelors           13.0  Married-civ-spouse  ...    Male          0.0          0.0           50.0   United-States       0</span><br><span class="line">16279  44.0       Private   83891.0     Bachelors           13.0            Divorced  ...    Male       5455.0          0.0           40.0   United-States       0</span><br><span class="line">16280  35.0  Self-emp-inc  182148.0     Bachelors           13.0  Married-civ-spouse  ...    Male          0.0          0.0           60.0   United-States       1</span><br><span class="line"></span><br><span class="line">[16281 rows x 15 columns]</span><br><span class="line">[03/29/2020, 04:36:35 PM] WARNING (nni) Requesting parameter without NNI framework, returning empty dict</span><br><span class="line">[&#x27;workclass&#x27;, &#x27;education&#x27;, &#x27;marital-status&#x27;, &#x27;occupation&#x27;, &#x27;relationship&#x27;, &#x27;race&#x27;, &#x27;sex&#x27;, &#x27;native-country&#x27;] [&#x27;age&#x27;, &#x27;workclass&#x27;, &#x27;fnlwgt&#x27;, &#x27;education&#x27;, &#x27;education-num&#x27;, &#x27;marital-status&#x27;, &#x27;occupation&#x27;, &#x27;relationship&#x27;, &#x27;race&#x27;, &#x27;sex&#x27;, &#x27;capital-gain&#x27;, &#x27;capital-loss&#x27;, &#x27;hours-per-week&#x27;, &#x27;native-country&#x27;]</span><br><span class="line">&lt;IPython.core.display.HTML object&gt;</span><br><span class="line">MetricVisualizer(layout=Layout(align_self=&#x27;stretch&#x27;, height=&#x27;500px&#x27;))</span><br><span class="line">0:      test: 0.8316184 best: 0.8316184 (0)     total: 79ms     remaining: 1m 18s</span><br><span class="line">50:     test: 0.9020779 best: 0.9020779 (50)    total: 942ms    remaining: 17.5s</span><br><span class="line">100:    test: 0.9081992 best: 0.9081992 (100)   total: 1.71s    remaining: 15.2s</span><br><span class="line">150:    test: 0.9108707 best: 0.9108707 (150)   total: 2.48s    remaining: 13.9s</span><br><span class="line">200:    test: 0.9133666 best: 0.9133666 (200)   total: 3.27s    remaining: 13s</span><br><span class="line">250:    test: 0.9161119 best: 0.9161119 (250)   total: 4.05s    remaining: 12.1s</span><br><span class="line">300:    test: 0.9181722 best: 0.9181726 (299)   total: 4.8s     remaining: 11.2s</span><br><span class="line">350:    test: 0.9191926 best: 0.9191926 (350)   total: 5.56s    remaining: 10.3s</span><br><span class="line">400:    test: 0.9205207 best: 0.9205207 (400)   total: 6.3s     remaining: 9.41s</span><br><span class="line">450:    test: 0.9210106 best: 0.9210106 (450)   total: 7.05s    remaining: 8.59s</span><br><span class="line">500:    test: 0.9215160 best: 0.9215160 (500)   total: 7.81s    remaining: 7.78s</span><br><span class="line">550:    test: 0.9221193 best: 0.9221193 (550)   total: 8.59s    remaining: 7s</span><br><span class="line">600:    test: 0.9227742 best: 0.9227742 (600)   total: 9.37s    remaining: 6.22s</span><br><span class="line">650:    test: 0.9232136 best: 0.9232136 (650)   total: 10.1s    remaining: 5.41s</span><br><span class="line">700:    test: 0.9234202 best: 0.9234202 (700)   total: 10.8s    remaining: 4.61s</span><br><span class="line">750:    test: 0.9238425 best: 0.9238428 (749)   total: 11.6s    remaining: 3.83s</span><br><span class="line">800:    test: 0.9242466 best: 0.9242466 (800)   total: 12.4s    remaining: 3.07s</span><br><span class="line">850:    test: 0.9244801 best: 0.9244826 (849)   total: 13.1s    remaining: 2.3s</span><br><span class="line">900:    test: 0.9246592 best: 0.9246633 (899)   total: 13.8s    remaining: 1.52s</span><br><span class="line">950:    test: 0.9248365 best: 0.9248397 (949)   total: 14.6s    remaining: 752ms</span><br><span class="line">999:    test: 0.9250032 best: 0.9250045 (998)   total: 15.3s    remaining: 0us</span><br><span class="line"></span><br><span class="line">bestTest = 0.9250045138</span><br><span class="line">bestIteration = 998</span><br><span class="line"></span><br><span class="line">Shrink model to first 999 iterations.</span><br><span class="line"> 1) capital-gain                   30.102152</span><br><span class="line"> 2) relationship                   21.504190</span><br><span class="line"> 3) capital-loss                   11.414022</span><br><span class="line"> 4) age                            10.377241</span><br><span class="line"> 5) education-num                  7.234561</span><br><span class="line"> 6) occupation                     6.331733</span><br><span class="line"> 7) hours-per-week                 4.476134</span><br><span class="line"> 8) marital-status                 4.354055</span><br><span class="line"> 9) sex                            1.146389</span><br><span class="line">10) education                      1.092454</span><br><span class="line">11) workclass                      0.776362</span><br><span class="line">12) fnlwgt                         0.577348</span><br><span class="line">13) native-country                 0.382720</span><br><span class="line">14) race                           0.230639</span><br><span class="line">another feature importances with prettified=True</span><br><span class="line"></span><br><span class="line">        Feature Id  Importances</span><br><span class="line">0     capital-gain    30.102152</span><br><span class="line">1     relationship    21.504190</span><br><span class="line">2     capital-loss    11.414022</span><br><span class="line">3              age    10.377241</span><br><span class="line">4    education-num     7.234561</span><br><span class="line">5       occupation     6.331733</span><br><span class="line">6   hours-per-week     4.476134</span><br><span class="line">7   marital-status     4.354055</span><br><span class="line">8              sex     1.146389</span><br><span class="line">9        education     1.092454</span><br><span class="line">10       workclass     0.776362</span><br><span class="line">11          fnlwgt     0.577348</span><br><span class="line">12  native-country     0.382720</span><br><span class="line">13            race     0.230639</span><br><span class="line">&lt;IPython.core.display.HTML object&gt;</span><br><span class="line">MetricVisualizer(layout=Layout(align_self=&#x27;stretch&#x27;, height=&#x27;500px&#x27;))</span><br><span class="line">AUC values:[0.83161836 0.85856642 0.85813673 0.86103314 0.86071689 0.86121893</span><br><span class="line"> 0.86441522 0.86505078 0.8784926  0.87975055 0.88290187 0.88250307</span><br><span class="line"> 0.88799037 0.8881507  0.89029036 0.88999382 0.88988625 0.89090279</span><br><span class="line"> 0.89101198 0.89160979 0.89201995 0.89295319 0.89335747 0.89347394</span><br><span class="line"> 0.89391874 0.89388563 0.89537425 0.89577286 0.89558464 0.89714728</span><br><span class="line"> 0.89720971 0.89743348 0.89766614 0.89780103 0.89835231 0.89859723</span><br><span class="line"> 0.89845935 0.89914869 0.89911805 0.89974913 0.89990595 0.90033138</span><br><span class="line"> 0.90038467 0.90098889 0.90072697 0.90107988 0.90084311 0.90124622</span><br><span class="line"> 0.90129837 0.90169675 0.90207794 0.90213554 0.90252241 0.90278218</span><br><span class="line"> 0.90291333 0.90347889 0.90362643 0.90369515 0.90384743 0.90364836</span><br><span class="line"> 0.90427544 0.90441459 0.90463238 0.90473693 0.90474323 0.9047752</span><br><span class="line"> 0.90479827 0.90473968 0.90509671 0.9052363  0.90524989 0.90552186</span><br><span class="line"> 0.90563292 0.90562407 0.90597002 0.90596054 0.90612973 0.90613532</span><br><span class="line"> 0.90626529 0.90624885 0.90629925 0.9065094  0.90687207 0.90713255</span><br><span class="line"> 0.90713464 0.90714229 0.90716147 0.90720291 0.90736582 0.9074965</span><br><span class="line"> 0.90754282 0.90756655 0.907661   0.90770332 0.9077639  0.90790968</span><br><span class="line"> 0.9079417  0.90798504 0.90802753 0.90811397 0.90819922 0.90826913</span><br><span class="line"> 0.90827529 0.90832273 0.90834522 0.90836268 0.90852687 0.90865518</span><br><span class="line"> 0.90867734 0.90877048 0.9088641  0.90892809 0.90899198 0.90898232</span><br><span class="line"> 0.90901235 0.90917898 0.90931463 0.90933462 0.90946639 0.90955688</span><br><span class="line"> 0.90957899 0.90955768 0.9096148  0.90963834 0.90960947 0.9097182</span><br><span class="line"> 0.90968601 0.90968109 0.90973949 0.90986055 0.90987831 0.90990626</span><br><span class="line"> 0.90988658 0.90996061 0.9100206  0.91016861 0.9102051  0.91031329</span><br><span class="line"> 0.91035212 0.91039234 0.91038859 0.91055811 0.91059551 0.91060905</span><br><span class="line"> 0.91068851 0.91069095 0.91070089 0.91075133 0.9107646  0.91083661</span><br><span class="line"> 0.91087072 0.91088294 0.91089601 0.9109779  0.91109368 0.91112637</span><br><span class="line"> 0.91117719 0.91131483 0.91133695 0.91135528 0.91138891 0.91139388</span><br><span class="line"> 0.91141477 0.91143149 0.91147063 0.91155253 0.91162016 0.91166083</span><br><span class="line"> 0.91172069 0.91181031 0.91196481 0.91200488 0.91207325 0.91211531</span><br><span class="line"> 0.91215074 0.91216651 0.91216438 0.91216644 0.91217042 0.91222321</span><br><span class="line"> 0.91224604 0.91226224 0.91238903 0.91245311 0.91250443 0.9125005</span><br><span class="line"> 0.91256733 0.9126408  0.91267537 0.9127926  0.91283471 0.91296865</span><br><span class="line"> 0.91309914 0.91321599 0.9132311  0.91326122 0.91331664 0.9133272</span><br><span class="line"> 0.91333757 0.91335631 0.91336663 0.91339323 0.91351306 0.91353504</span><br><span class="line"> 0.91364762 0.91371905 0.91374356 0.91382455 0.91383544 0.91387963</span><br><span class="line"> 0.9138874  0.9139162  0.9139656  0.91401361 0.91409119 0.91417138</span><br><span class="line"> 0.91415125 0.91426151 0.91427733 0.91430731 0.91443747 0.91448261</span><br><span class="line"> 0.91453059 0.91459567 0.91462631 0.91465691 0.91466103 0.91475955</span><br><span class="line"> 0.91475329 0.91484831 0.91488644 0.9149258  0.91522573 0.91525917</span><br><span class="line"> 0.91535504 0.91542878 0.91546454 0.91549223 0.91553097 0.91555825</span><br><span class="line"> 0.91559304 0.91562558 0.91568611 0.91569478 0.91571794 0.9157357</span><br><span class="line"> 0.91576043 0.91577251 0.9158588  0.91609757 0.91611192 0.91617368</span><br><span class="line"> 0.91615905 0.91616662 0.91621344 0.91624148 0.91624617 0.916279</span><br><span class="line"> 0.91631196 0.91642947 0.91647835 0.91652667 0.91651392 0.91652581</span><br><span class="line"> 0.91653713 0.91655006 0.91665427 0.91667051 0.916788   0.91681452</span><br><span class="line"> 0.91685526 0.91696372 0.91706001 0.91709748 0.91709928 0.91717672</span><br><span class="line"> 0.91719126 0.91729565 0.91739161 0.91742034 0.9174929  0.91750469</span><br><span class="line"> 0.91754306 0.9175746  0.91757541 0.91759738 0.91761036 0.91759407</span><br><span class="line"> 0.91763139 0.91772489 0.91776103 0.9179556  0.9179898  0.91801864</span><br><span class="line"> 0.91801737 0.91803603 0.91805739 0.91813814 0.91814534 0.91817263</span><br><span class="line"> 0.9181722  0.91824609 0.91827275 0.91829577 0.91829899 0.91831358</span><br><span class="line"> 0.91837373 0.91842195 0.91842025 0.91844374 0.91845804 0.91856584</span><br><span class="line"> 0.91857612 0.91863774 0.91862941 0.91863533 0.91871476 0.91873683</span><br><span class="line"> 0.91873685 0.91873453 0.91876589 0.91877062 0.91877522 0.91875817</span><br><span class="line"> 0.91876122 0.9187661  0.91875142 0.91882763 0.91882948 0.91885122</span><br><span class="line"> 0.91885425 0.91883814 0.91885245 0.91885491 0.91889654 0.91892941</span><br><span class="line"> 0.91899658 0.91898947 0.91903944 0.91905251 0.91906383 0.91905976</span><br><span class="line"> 0.91905803 0.91906125 0.91911411 0.91912297 0.91911189 0.9191422</span><br><span class="line"> 0.91916427 0.91918881 0.91919262 0.9192038  0.91922066 0.91922383</span><br><span class="line"> 0.91924121 0.91924974 0.91929625 0.91931439 0.9193681  0.91936356</span><br><span class="line"> 0.9194005  0.91940401 0.91942551 0.91941168 0.91949196 0.91950447</span><br><span class="line"> 0.91950522 0.91950731 0.91951408 0.91954548 0.919564   0.91956135</span><br><span class="line"> 0.91961966 0.91962117 0.91961662 0.91963093 0.91963782 0.9197918</span><br><span class="line"> 0.91981146 0.91984902 0.91987796 0.91986474 0.91986896 0.91988572</span><br><span class="line"> 0.91999293 0.91999407 0.9200143  0.92006417 0.920051   0.92006654</span><br><span class="line"> 0.92015715 0.92027774 0.92028039 0.92028351 0.92029304 0.9203403</span><br><span class="line"> 0.92041614 0.9204284  0.920495   0.92051366 0.92052067 0.92052967</span><br><span class="line"> 0.92054984 0.92056874 0.92059948 0.9206144  0.92060015 0.92060062</span><br><span class="line"> 0.9206     0.92060142 0.92060772 0.92063425 0.92063752 0.92064007</span><br><span class="line"> 0.92064424 0.92065741 0.92066167 0.92067399 0.92068185 0.92068403</span><br><span class="line"> 0.92072576 0.92073774 0.92074077 0.92074873 0.92073982 0.92076398</span><br><span class="line"> 0.92077724 0.92078624 0.92078373 0.92078809 0.92077483 0.92077691</span><br><span class="line"> 0.92080732 0.92081864 0.92082224 0.92081307 0.92081577 0.92081407</span><br><span class="line"> 0.9208495  0.92085911 0.92088312 0.92088725 0.92088677 0.92088559</span><br><span class="line"> 0.92089568 0.92089648 0.92090354 0.92092367 0.92092732 0.92094607</span><br><span class="line"> 0.92101058 0.92102503 0.92103919 0.92102465 0.92103474 0.92103725</span><br><span class="line"> 0.92102598 0.92102195 0.92102607 0.92103857 0.92103682 0.92103957</span><br><span class="line"> 0.92106103 0.92107088 0.92107722 0.92108452 0.92108708 0.92114306</span><br><span class="line"> 0.92115324 0.92119644 0.9212017  0.92121074 0.92122964 0.92124253</span><br><span class="line"> 0.92124826 0.92127222 0.92129136 0.92131528 0.92131712 0.92132233</span><br><span class="line"> 0.9213274  0.92132897 0.92131902 0.92132323 0.92133725 0.92138855</span><br><span class="line"> 0.92138997 0.92144297 0.92145268 0.92144453 0.92144766 0.92148863</span><br><span class="line"> 0.92148243 0.92148764 0.92149299 0.92150881 0.92149493 0.92149905</span><br><span class="line"> 0.92150045 0.92151196 0.92151603 0.92152768 0.92153251 0.92151722</span><br><span class="line"> 0.92152228 0.92152569 0.92153124 0.92162734 0.92162582 0.92164117</span><br><span class="line"> 0.9216549  0.92165462 0.92171068 0.92171977 0.92172706 0.92172754</span><br><span class="line"> 0.92173966 0.92175084 0.92175719 0.92175633 0.92175221 0.92176424</span><br><span class="line"> 0.92176529 0.92177016 0.92177178 0.92178025 0.92176628 0.92184211</span><br><span class="line"> 0.9218522  0.92185713 0.92185623 0.92186669 0.92191529 0.92194717</span><br><span class="line"> 0.92195285 0.92197383 0.92197056 0.92197052 0.92197658 0.92198013</span><br><span class="line"> 0.92196483 0.92197075 0.92197885 0.92198245 0.92199188 0.92204175</span><br><span class="line"> 0.92204227 0.92209778 0.92210214 0.92210863 0.92211934 0.92216049</span><br><span class="line"> 0.92216462 0.92216983 0.92217963 0.92217537 0.92221117 0.92241212</span><br><span class="line"> 0.92241638 0.92242576 0.92243073 0.92243177 0.92244111 0.92245001</span><br><span class="line"> 0.92246045 0.9224605  0.92249186 0.92249266 0.9225063  0.92253676</span><br><span class="line"> 0.92254249 0.92254519 0.92255234 0.92255675 0.9225657  0.92257275</span><br><span class="line"> 0.92256906 0.92257072 0.92255637 0.92255892 0.922562   0.92256087</span><br><span class="line"> 0.92257162 0.92257129 0.92257129 0.92257015 0.92255466 0.92255267</span><br><span class="line"> 0.92254007 0.92253922 0.92254197 0.92254647 0.92254566 0.9225891</span><br><span class="line"> 0.9225908  0.92258696 0.92259497 0.92259644 0.92275269 0.9227705</span><br><span class="line"> 0.92277424 0.92276742 0.92277045 0.92277491 0.92278784 0.92278968</span><br><span class="line"> 0.9227921  0.92279944 0.92280498 0.92281464 0.92282772 0.92282279</span><br><span class="line"> 0.92295612 0.92295963 0.92295584 0.92297024 0.92295882 0.92296086</span><br><span class="line"> 0.92295636 0.92296015 0.92296214 0.92297611 0.92297597 0.92297367</span><br><span class="line"> 0.92297282 0.9229821  0.92299015 0.92299285 0.92299162 0.92299494</span><br><span class="line"> 0.92304206 0.92307967 0.92308431 0.92310553 0.92310695 0.92310871</span><br><span class="line"> 0.92310894 0.9231188  0.92311927 0.92312131 0.92313343 0.92313457</span><br><span class="line"> 0.92313679 0.92313154 0.92313566 0.92314866 0.92315825 0.92315967</span><br><span class="line"> 0.92320145 0.92320557 0.92321357 0.92322352 0.92323133 0.92323763</span><br><span class="line"> 0.92323815 0.92324663 0.92324834 0.92325137 0.92325492 0.92325402</span><br><span class="line"> 0.92325374 0.92326515 0.92327003 0.92327538 0.92326444 0.92326927</span><br><span class="line"> 0.92326937 0.92325672 0.92327946 0.92327837 0.92327979 0.92328097</span><br><span class="line"> 0.92328386 0.92329125 0.92329248 0.92329977 0.92330783 0.92330854</span><br><span class="line"> 0.92331124 0.92331299 0.92334018 0.92333804 0.92333752 0.92334894</span><br><span class="line"> 0.92334851 0.92334979 0.92335064 0.92335164 0.92336736 0.92336869</span><br><span class="line"> 0.92337068 0.9233623  0.92336433 0.92336395 0.92336964 0.92337428</span><br><span class="line"> 0.92338446 0.92339521 0.92339592 0.92340004 0.92342017 0.92341999</span><br><span class="line"> 0.92342126 0.92342264 0.92341984 0.92341984 0.92341833 0.92340795</span><br><span class="line"> 0.92341307 0.92342212 0.92346853 0.92346801 0.92349373 0.92351225</span><br><span class="line"> 0.92351931 0.92352476 0.92356085 0.92356151 0.92356483 0.92357894</span><br><span class="line"> 0.92361887 0.92361683 0.92362503 0.92362484 0.92364317 0.92366462</span><br><span class="line"> 0.92366391 0.92366415 0.92366709 0.92367481 0.92368707 0.92368769</span><br><span class="line"> 0.92368816 0.92368991 0.92368925 0.92369735 0.92372847 0.92372814</span><br><span class="line"> 0.92372667 0.92373089 0.92373169 0.92373538 0.92376077 0.92376134</span><br><span class="line"> 0.92377977 0.9238052  0.92380307 0.92382703 0.92383556 0.92384276</span><br><span class="line"> 0.92384248 0.92384338 0.92384195 0.92384991 0.92384778 0.92385673</span><br><span class="line"> 0.92385702 0.92385579 0.92388179 0.92390358 0.92390452 0.92392219</span><br><span class="line"> 0.92393938 0.9239624  0.92396657 0.92397055 0.92397022 0.92398855</span><br><span class="line"> 0.92398784 0.92399916 0.92401161 0.92402559 0.92402535 0.92412785</span><br><span class="line"> 0.92413405 0.92415006 0.92416323 0.92416356 0.92416346 0.9241593</span><br><span class="line"> 0.92415797 0.92415541 0.92415636 0.92415508 0.92415357 0.9241574</span><br><span class="line"> 0.92417905 0.92419297 0.92420628 0.92421429 0.92421362 0.92421457</span><br><span class="line"> 0.92422599 0.92422475 0.92422423 0.92422291 0.92422414 0.9242293</span><br><span class="line"> 0.92423915 0.92424469 0.92424659 0.92424834 0.9242482  0.92426392</span><br><span class="line"> 0.92426804 0.9242796  0.92428093 0.92429978 0.92430196 0.92430196</span><br><span class="line"> 0.92432313 0.92432455 0.9243191  0.92431726 0.9243156  0.9243146</span><br><span class="line"> 0.92432185 0.924339   0.92433824 0.92435922 0.92436339 0.92436391</span><br><span class="line"> 0.92437964 0.92437731 0.92438267 0.92438025 0.92437954 0.92437973</span><br><span class="line"> 0.92441307 0.92441795 0.92441701 0.92442023 0.9244314  0.92442975</span><br><span class="line"> 0.92442918 0.92442804 0.92442899 0.92443202 0.92443264 0.92443145</span><br><span class="line"> 0.92444026 0.92445457 0.92445348 0.92445267 0.92445163 0.92445144</span><br><span class="line"> 0.92446295 0.92447209 0.92446982 0.92448261 0.92448014 0.92448966</span><br><span class="line"> 0.92450439 0.92450236 0.92450188 0.92450184 0.92449956 0.92451074</span><br><span class="line"> 0.92451017 0.9245222  0.92452661 0.92452457 0.92453395 0.92453172</span><br><span class="line"> 0.92454541 0.92454967 0.92454584 0.92454304 0.92454058 0.92454039</span><br><span class="line"> 0.92453722 0.92455214 0.92455304 0.92454953 0.92456061 0.92457629</span><br><span class="line"> 0.9245798  0.92457681 0.92459439 0.92460656 0.92462025 0.92461693</span><br><span class="line"> 0.92462337 0.92461906 0.92463341 0.9246309  0.92463697 0.92463474</span><br><span class="line"> 0.92463332 0.92463242 0.92462896 0.92462574 0.92463891 0.92464047</span><br><span class="line"> 0.92464241 0.92464023 0.92464147 0.92465189 0.92465809 0.9246633</span><br><span class="line"> 0.92465923 0.92466605 0.92466898 0.92467012 0.92466695 0.92466572</span><br><span class="line"> 0.92467879 0.92468964 0.92471355 0.92472397 0.92472525 0.9247262</span><br><span class="line"> 0.92473302 0.92472966 0.92473449 0.92473468 0.92473392 0.92473316</span><br><span class="line"> 0.92473132 0.92472805 0.92472582 0.92472492 0.92472293 0.9247263</span><br><span class="line"> 0.92473454 0.92473511 0.92473743 0.92473648 0.92475415 0.92476172</span><br><span class="line"> 0.92475898 0.92477077 0.92479057 0.92479355 0.92479284 0.92480303</span><br><span class="line"> 0.9248143  0.92481302 0.92482448 0.92482211 0.92482301 0.92482368</span><br><span class="line"> 0.92482808 0.92482638 0.92482931 0.92483178 0.92483741 0.92483784</span><br><span class="line"> 0.9248386  0.92483969 0.92483646 0.9248404  0.92485238 0.92485067</span><br><span class="line"> 0.92486796 0.92488321 0.92488009 0.92487786 0.92487805 0.92488516</span><br><span class="line"> 0.92488994 0.92489098 0.92490003 0.92490704 0.92490391 0.92491362</span><br><span class="line"> 0.92492802 0.92493299 0.9249463  0.92494289 0.92493906 0.92493816</span><br><span class="line"> 0.92494223 0.92493996 0.9249382  0.92493849 0.9249491  0.92494697</span><br><span class="line"> 0.92494711 0.92495838 0.92495838 0.92497013 0.92498078 0.92497794</span><br><span class="line"> 0.92497695 0.92497827 0.92497619 0.924976   0.92498211 0.92498495</span><br><span class="line"> 0.92499078 0.92500338 0.92500087 0.92500049 0.92499874 0.92499874</span><br><span class="line"> 0.92499883 0.92499935 0.92500451]</span><br><span class="line">AUC of prediction:0.8603217542453206</span><br></pre></td></tr></table></figure><h2 id="人工神经网络-neural-network">2.6 人工神经网络-Neural Network</h2>神经网络在维基百科上的定义是：NN is a network inspired by biological neural networks (the central nervous systems of animals, in particular the brain) which are used to estimate or approximate functions that can depend on a large number of inputs that are generally unknown.(from wikipedia)<center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1ap79lfscsq013g11c4g1aqu1jb4m.png" width="300"></center><h3 id="神经元">2.6.1 神经元</h3>神经元是神经网络和SVM这类模型的基础模型和来源，它是一个具有如下结构的线性模型：<center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1ap6vqg8k1nursgn1kv541nevd9.png" width="300"></center><p>其输出模式为：</p><blockquote><p><span class="math display">\[ \begin{array}{l} output &amp; = &amp; \left\{ \begin{array}{1} 0 &amp; if~ \sum_j w_j x_j + b \leq 0 \\ 1 &amp; if~ \sum_j w_j x_j + b&gt; 0 \end{array} \right. \end{array} \]</span></p></blockquote>示意图如下：<center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1ap79nd0j1m5cuhbv0ltog1ao313.png" width="400"></center><h3 id="神经网络的常用结构">2.6.2 神经网络的常用结构</h3>神经网络由一系列神经元组成，典型的神经网络结构如下：<center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1ap7b1i9s156h8n91noe1v0b1kk41g.png" width="500"></center><p>其中最左边是输入层，包含若干输入神经元，最右边是输出层，包含若干输出神经元，介于输入层和输出层的所有层都叫隐藏层，由于神经元的作用，任何权重的微小变化都会导致输出的微小变化，即这种变化是平滑的。</p>神经元的各种组合方式得到性质不一的神经网络结构 :<center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1ap7hp4el195d12al1d2119h3r8e46.png" width="350"></center><center>前馈神经网络</center><center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1asuka9jl1u479sd1k7l64l11s2m.png" width="350"></center><center>反向传播神经网络</center><center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1ap7k2i4u1qeq14871ge613np5r5d.png" width="350"></center><center>循环神经网络</center><center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1ap7i6rfq1ua917bf12llgol1vcs50.png" width="450"></center><center>卷积神经网络</center><center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1ap7kf21k8c2me14dr1sd1t5d67.png" width="350"></center><center>自编码器</center><center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1ap7kod2dpkn1cqu1oksvmd3ms6t.png" width="600"></center><center>Google DeepMind 记忆神经网络(用于AlphaGo)</center><h3 id="一个简单的神经网络例子">2.6.3 一个简单的神经网络例子</h3><p>假设随机变量 <span class="math inline">\(x \sim N(0,1)\)</span>, 使用3层神经网络拟合该分布：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib</span><br><span class="line">matplotlib.use(&#x27;Agg&#x27;)</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import random</span><br><span class="line">import math</span><br><span class="line">import keras</span><br><span class="line">from keras.models import Sequential</span><br><span class="line">from keras.layers.core import Dense,Dropout,Activation</span><br><span class="line"></span><br><span class="line">def gd(x,m,s):</span><br><span class="line">  left=1/(math.sqrt(2*math.pi)*s)</span><br><span class="line">  right=math.exp(-math.pow(x-m,2)/(2*math.pow(s,2)))</span><br><span class="line">  return left*right</span><br><span class="line"></span><br><span class="line">def pt(x, y1, y2):</span><br><span class="line">  if len(x) != len(y1) or len(x) != len(y2):</span><br><span class="line">    print &#x27;input error.&#x27;</span><br><span class="line">    return</span><br><span class="line">  plt.figure(num=1, figsize=(20, 6))</span><br><span class="line">  plt.title(&#x27;NN fitting Gaussian distribution&#x27;, size=14)</span><br><span class="line">  plt.xlabel(&#x27;x&#x27;, size=14)</span><br><span class="line">  plt.ylabel(&#x27;y&#x27;, size=14)</span><br><span class="line">  plt.plot(x, y1, color=&#x27;b&#x27;, linestyle=&#x27;--&#x27;, label=&#x27;Gaussian distribution&#x27;)</span><br><span class="line">  plt.plot(x, y2, color=&#x27;r&#x27;, linestyle=&#x27;-&#x27;, label=&#x27;NN fitting&#x27;)</span><br><span class="line">  plt.legend(loc=&#x27;upper left&#x27;)</span><br><span class="line">  plt.savefig(&#x27;ann.png&#x27;, format=&#x27;png&#x27;)</span><br><span class="line"></span><br><span class="line">def ann(train_d, train_l, prd_d):</span><br><span class="line">  if len(train_d) == 0 or len(train_d) != len(train_l):</span><br><span class="line">    print &#x27;training data error.&#x27;</span><br><span class="line">    return</span><br><span class="line">  model = Sequential()</span><br><span class="line">  model.add(Dense(30, input_dim=1))</span><br><span class="line">  model.add(Activation(&#x27;relu&#x27;))</span><br><span class="line">  model.add(Dense(30))</span><br><span class="line">  model.add(Activation(&#x27;relu&#x27;))</span><br><span class="line"></span><br><span class="line">  model.add(Dense(1, activation=&#x27;sigmoid&#x27;))</span><br><span class="line">  model.compile(loss=&#x27;mse&#x27;,</span><br><span class="line">              optimizer=&#x27;rmsprop&#x27;,</span><br><span class="line">              metrics=[&#x27;accuracy&#x27;])</span><br><span class="line">  model.fit(train_d,train_l,batch_size=250, nb_epoch=50, validation_split=0.2)</span><br><span class="line">  p = model.predict(prd_d,batch_size=250)</span><br><span class="line">  return p</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">  x = np.linspace(-5, 5, 10000)</span><br><span class="line">  idx = random.sample(x, 900)</span><br><span class="line">  train_d = []</span><br><span class="line">  train_l = []</span><br><span class="line">  for i in idx:</span><br><span class="line">    train_d.append(x[i])</span><br><span class="line">    train_l.append(gd(x[i],0,1))</span><br><span class="line"></span><br><span class="line">  y1 = []</span><br><span class="line">  y2 = []</span><br><span class="line">  for i in x:</span><br><span class="line">    y1.append(gd(i,0,1))</span><br><span class="line"></span><br><span class="line">  y2 = ann(np.array(train_d).reshape(len(train_d), 1), np.array(train_l), np.array(x).reshape(len(x), 1))</span><br><span class="line">  pt(x, y1, y2.tolist())</span><br></pre></td></tr></table></figure><p></p><center><img data-src="https://vivounicorn.github.io/images/ai_chapter_2/image_1ap87o3no141a1tnvid984h1vnv7n.png" width="600"></center></div><div class="popular-posts-header">相关文章推荐</div><ul class="popular-posts"><li class="popular-posts-item"><div class="popular-posts-title"><a href="/article/d677b2e0.html" rel="bookmark">机器学习与人工智能技术分享-第三章 机器学习中的统一框架</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/article/fb9cd06d.html" rel="bookmark">机器学习与人工智能技术分享-第七章 金融风控</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/article/ad2261a2.html" rel="bookmark">机器学习与人工智能技术分享-第四章 最优化原理</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/article/301f2134.html" rel="bookmark">机器学习与人工智能技术分享-第十一章 OCR</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/article/b12a240.html" rel="bookmark">机器学习与人工智能技术分享-第九章 语义分割</a></div></li></ul><footer class="post-footer"><div class="post-tags"><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a> <a href="/tags/Bagging/" rel="tag"># Bagging</a> <a href="/tags/Boosting/" rel="tag"># Boosting</a> <a href="/tags/%E7%AC%AC%E4%BA%8C%E7%AB%A0/" rel="tag"># 第二章</a> <a href="/tags/LR/" rel="tag"># LR</a> <a href="/tags/SVM/" rel="tag"># SVM</a> <a href="/tags/ATM/" rel="tag"># ATM</a> <a href="/tags/RF/" rel="tag"># RF</a> <a href="/tags/GBDT/" rel="tag"># GBDT</a></div><div class="post-nav"><div class="post-nav-item"><a href="/article/f082dc68.html" rel="prev" title="机器学习与人工智能技术分享-第一章 基本概念"><i class="fa fa-chevron-left"></i> 机器学习与人工智能技术分享-第一章 基本概念</a></div><div class="post-nav-item"><a href="/article/d677b2e0.html" rel="next" title="机器学习与人工智能技术分享-第三章 机器学习中的统一框架">机器学习与人工智能技术分享-第三章 机器学习中的统一框架 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener("tabs:register",()=>{let e=CONFIG.comments["activeClass"];if(CONFIG.comments.storage&&(e=localStorage.getItem("comments_active")||e),e){let t=document.querySelector(`a[href="#comment-${e}"]`);t&&t.click()}}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{t.target.matches(".tabs-comment .tab-content .tab-pane")&&(t=t.target.classList[1],localStorage.setItem("comments_active",t))})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BB%BA%E6%A8%A1%E6%96%B9%E6%B3%95%E5%9B%9E%E9%A1%BE"><span class="nav-text">2. 建模方法回顾</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE"><span class="nav-text">2.0 偏差与方差</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-linear-regression"><span class="nav-text">2.1 线性回归-Linear Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86"><span class="nav-text">2.1.1 模型原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-text">2.1.2 损失函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA-support-vector-machine"><span class="nav-text">2.2 支持向量机-Support Vector Machine</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86-1"><span class="nav-text">2.2.1 模型原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-1"><span class="nav-text">2.2.2 损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E6%96%B9%E6%B3%95"><span class="nav-text">2.2.3 核方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92-logistic-regression"><span class="nav-text">2.3 逻辑回归-Logistic Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86-2"><span class="nav-text">2.3.1 模型原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-2"><span class="nav-text">2.3.2 损失函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bagging-and-boosting%E6%A1%86%E6%9E%B6"><span class="nav-text">2.4 Bagging and Boosting框架</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#bagging%E6%A1%86%E6%9E%B6"><span class="nav-text">2.4.1 Bagging框架</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#boosting%E6%A1%86%E6%9E%B6"><span class="nav-text">2.4.2 Boosting框架</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#additive-tree-%E6%A8%A1%E5%9E%8B"><span class="nav-text">2.5 Additive Tree 模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#random-forests"><span class="nav-text">2.5.1 Random Forests</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#adaboost-with-trees"><span class="nav-text">2.5.2 AdaBoost with trees</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gradient-boosting-decision-tree"><span class="nav-text">2.5.3 Gradient Boosting Decision Tree</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%80%E5%8D%95%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="nav-text">2.5.4 简单的例子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#catboost"><span class="nav-text">2.5.5 CatBoost</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-neural-network"><span class="nav-text">2.6 人工神经网络-Neural Network</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E5%85%83"><span class="nav-text">2.6.1 神经元</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%B8%B8%E7%94%A8%E7%BB%93%E6%9E%84"><span class="nav-text">2.6.2 神经网络的常用结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BE%8B%E5%AD%90"><span class="nav-text">2.6.3 一个简单的神经网络例子</span></a></li></ol></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="张磊" src="https://vivounicorn.github.io/images/wali.png"><p class="site-author-name" itemprop="name">张磊</p><div class="site-description" itemprop="description"></div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">16</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">2</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">49</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="sidebar-button motion-element"><i class="fa fa-comment"></i> Chat</div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/vivounicorn" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;vivounicorn" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:zhangleisuper@gmail.com" title="E-Mail → mailto:zhangleisuper@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a> </span><span class="links-of-author-item"><a href="https://weibo.com/vivounicorn" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;vivounicorn" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a></span></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2022</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">张磊</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span title="站点总字数">552k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="站点阅读时长">8:22</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动</div></div></footer></div><script src="//cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js"></script><script src="//cdn.bootcdn.net/ajax/libs/jquery/3.6.0/jquery.min.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js"></script><script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script>!function(){var e,t,o,n,r=document.getElementsByTagName("link");if(0<r.length)for(i=0;i<r.length;i++)"canonical"==r[i].rel.toLowerCase()&&r[i].href&&(e=r[i].href);t=(e||window.location.protocol).split(":")[0],e=e||window.location.href,window,o=e,n=document.referrer,/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(o)||(t="https"===String(t).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif",n?(t+="?r="+encodeURIComponent(document.referrer),o&&(t+="&l="+o)):o&&(t+="?l="+o),(new Image).src=t)}()</script><script src="/js/local-search.js"></script><script>"undefined"==typeof MathJax?(window.MathJax={loader:{load:["[tex]/mhchem"],source:{"[tex]/amsCd":"[tex]/amscd","[tex]/AMScd":"[tex]/amscd"}},tex:{inlineMath:{"[+]":[["$","$"]]},packages:{"[+]":["mhchem"]},tags:"ams"},options:{renderActions:{findScript:[10,d=>{document.querySelectorAll('script[type^="math/tex"]').forEach(e=>{var t=!!e.type.match(/; *mode=display/);const a=new d.options.MathItem(e.textContent,d.inputJax[0],t);t=document.createTextNode("");e.parentNode.replaceChild(t,e),a.start={node:t,delim:"",n:0},a.end={node:t,delim:"",n:0},d.math.push(a)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}},function(){var e=document.createElement("script");e.src="//cdnjs.cloudflare.com/ajax/libs/mathjax/3.0.5/es5/tex-mml-chtml.js",e.defer=!0,document.head.appendChild(e)}()):(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset())</script><script>NexT.utils.loadComments(document.querySelector("#valine-comments"),()=>{NexT.utils.getScript("//cdnjs.cloudflare.com/ajax/libs/valine/1.3.10/Valine.min.js",()=>{var i=["nick","mail","link"],e="nick,mail,link".split(",").filter(e=>i.includes(e));new Valine({el:"#valine-comments",verify:!1,notify:!0,appId:"m8FPP0CqMpxyTuUvaVOX9qVV-gzGzoHsz",appKey:"Ori6X9PXqQyURvwgl7HT5TJj",placeholder:"赠人玫瑰，手有余香",avatar:"mm",meta:e,pageSize:"10",visitor:!0,lang:"zh-cn",path:location.pathname,recordIP:!1,serverURLs:""})},window.Valine)})</script></body></html>