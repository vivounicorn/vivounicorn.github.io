<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><meta name="google-site-verification" content="-l60HPLrjDNbr3Ni1wLsNkiKiCWUAmxiC_ObB8vNMF0"><meta name="msvalidate.01" content="AF3396A141E1B198CA1BE76915B3969F"><meta name="yandex-verification" content="ee8492bd2e7708db"><meta name="baidu-site-verification" content="code-OBKi1CbRLy"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css"><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"vivounicorn.github.io",root:"/",scheme:"Muse",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="本章对机器学习在NLP领域开创性的一些模型做了原理介绍和实践展示，包括RNN、LSTM以及燃烧了整个行业的Transformer。"><meta property="og:type" content="article"><meta property="og:title" content="机器学习与人工智能技术分享-第六章 循环神经网络与Transformers"><meta property="og:url" content="https://vivounicorn.github.io/article/8dd65dea.html"><meta property="og:site_name" content="业精于勤，荒于嬉；行成于思，毁于随。"><meta property="og:description" content="本章对机器学习在NLP领域开创性的一些模型做了原理介绍和实践展示，包括RNN、LSTM以及燃烧了整个行业的Transformer。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB-%E7%AC%AC%E5%85%AD%E7%AB%A0-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8ETransformers-the%20transformer"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/image_1d8hvg8ee5bq1msr2ng13sb1cls1m.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/image_1d9bnmgsa50b1ksr16kb18vg10gh50.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/image_1d9h56nst1c125o31pdu1u351no75q.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/image_1d9hecgavrti1hml4le1kbn1lli9.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/image_1ddpb4ckassq1l4b1evk1utu16e613.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/image_1ddpb61bnag91iu3ce8ivvrq41g.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/image_1eck5ab2i1k6en2p110u4ic1i801t.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/image_1edgkmmg78npnk1cbq1vmpaq81e.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/image_1echc1m1417va1si31n9l4uqo4813.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/image_1echc0ujc1crhpl71tov2vt165im.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/image_1echdls5q1f991o6f1bba1fb5bor1g.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/image_1ecm4fbi01ci51p1bgvj1opf1kag2a.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/2d"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/3d"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/image_1ed8qo1fid0m1rmk1qupdb0s1311.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/saddle.gif"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/image_1edj5o3bpprf1te3jlc6c5qmb2v.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/image_1edj6hipk4o5138gi491htcbi93c.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/image_1edj6hu8vaf019av52n1d70h263p.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/image_1eei0s9k6138i1ms215b1vut19lc9.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/image_1eei1f9j81t94tbv1g3g1dn0jsv19.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/image_1eei1hdcleacos17k51jt31td51m.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/image_1een7h4n513ku1b6uq8e9sjk0h9.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/image_1eend2gj51ahg1c30e0id2113u9m.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/image_1eengrro49501c9ur94d581t541g.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/image_1eeniuive1ehfgojdr31s7t4302d.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/image_1eenjfahi1kov1bf4igd13rd10pk37.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/image_1ef3r2gohi481di0af21v9vn859.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/image_1ef93p7r11rsmv6aop414ud1aqs19.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/image_1ef97as8ol8kr301q571g01l012c.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/image_1eend2gj51ahg1c30e0id2113u9m.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/lstm-focus%5B0%5D.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/LSTM3-SimpleRNN%5B0%5D..png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/LSTM3-chain%5B0%5D..png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/forget-gate.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/input-gate.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/output.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/output-gate.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/image_1emdpl8qlgg6q6l18gt1udq177i9.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/image_1emdqc2sgvspaoa18uk1bqs1t87m.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/image_1emdtm03k1r4l1mq342e14enu6t13.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/image_1fbislg3d5m5221g4d15kl1rbc9.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-企业微信截图_16337719682456.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-企业微信截图_16337732972491.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-企业微信截图_16337793705067.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-企业微信截图_16337791134694.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-企业微信截图_16337800556564.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-企业微信截图_16337807036222.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-企业微信截图_16338713082289.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-attention.jpg"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-企业微信截图_16348962307501.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-企业微信截图_16348992418770.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-2021-11-05 15-32-45 的屏幕截图.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-2021-11-05 16-34-38 的屏幕截图.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-2021-11-10 13-53-21 的屏幕截图.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/2021-11-10 15-36-57 的屏幕截图.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-2021-11-05 19-12-34 的屏幕截图.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/2021-11-16%2011-54-31%20%E7%9A%84%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-2021-11-25 10-36-09 的屏幕截图.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-2021-11-25 10-58-44 的屏幕截图.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/2021-11-25%2011-12-43%20的屏幕截图.png"><meta property="og:image" content="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-the transformer"><meta property="article:published_time" content="2021-09-05T12:15:54.000Z"><meta property="article:modified_time" content="2021-12-18T06:25:42.000Z"><meta property="article:author" content="张磊"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="transformer"><meta property="article:tag" content="attention"><meta property="article:tag" content="RNN"><meta property="article:tag" content="LSTM"><meta property="article:tag" content="第六章"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB-%E7%AC%AC%E5%85%AD%E7%AB%A0-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8ETransformers-the%20transformer"><link rel="canonical" href="https://vivounicorn.github.io/article/8dd65dea.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>机器学习与人工智能技术分享-第六章 循环神经网络与Transformers | 业精于勤，荒于嬉；行成于思，毁于随。</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">业精于勤，荒于嬉；行成于思，毁于随。</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">花晨月夕</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>站点地图</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div><div><img itemprop="image" src="https://vivounicorn.github.io/images/background.jpg"></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://vivounicorn.github.io/article/8dd65dea.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="https://vivounicorn.github.io/images/wali.png"><meta itemprop="name" content="张磊"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="业精于勤，荒于嬉；行成于思，毁于随。"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">机器学习与人工智能技术分享-第六章 循环神经网络与Transformers</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-09-05 20:15:54" itemprop="dateCreated datePublished" datetime="2021-09-05T20:15:54+08:00">2021-09-05</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2021-12-18 14:25:42" itemprop="dateModified" datetime="2021-12-18T14:25:42+08:00">2021-12-18</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a> </span></span><span id="/article/8dd65dea.html" class="post-meta-item leancloud_visitors" data-flag-title="机器学习与人工智能技术分享-第六章 循环神经网络与Transformers" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/article/8dd65dea.html#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/article/8dd65dea.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>84k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>1:16</span></span></div></header><div class="post-body" itemprop="articleBody"><p><img src="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB-%E7%AC%AC%E5%85%AD%E7%AB%A0-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8ETransformers-the%20transformer" height="500"> 本章对机器学习在NLP领域开创性的一些模型做了原理介绍和实践展示，包括RNN、LSTM以及燃烧了整个行业的Transformer。 <span id="more"></span></p><h1 id="循环神经网络与transformers">6. 循环神经网络与Transformers</h1><h2 id="rnn">6.1 RNN</h2><h3 id="基本原理">6.1.1 基本原理</h3><p>序列类问题是我们日常生活中常见的一类问题：我们读的文章，我们说话的语音等等，要么是在空间上的序列，要么是在时间的序列，序列的每个单元之间有前驱后继的语义或序列相关性，比如：当我们说，“这是我们伟大的××”，这里××是“祖国”的概率远远大于“板凳”，所以在NLP领域，应用大概可以分为几种：</p><p>1、根据当前上下文语义预测接下来出现某个文本的概率；</p><p>2、通过语言模型生成新的文本；</p><p>3、文本的通用NLP任务，例如词性标注、文本分类等；</p><p>4、机器翻译；</p><p>5、文本表示及编码解码。</p>传统的神经网络并没有很好的解决这种序列问题，于是Recurrent Neural Networks这种网络被提了出来：<center><img src="https://vivounicorn.github.io/images/ai_chapter_6/image_1d8hvg8ee5bq1msr2ng13sb1cls1m.png" width="600"></center><p>乍一看就是节点自带环路的网络，广义来看，可以在这个节点上展开，只是这种展开和输入的字数有关，比如输入为10个字，则展开10层。</p><p>从一个角度看，不同于传统神经网络会假设所有输入及输出是相互独立的，RNN正相反，认为节点间天然有相关性；另一个角度是，认为RNN具有“记忆”能力，它能把历史上相关节点状态“全部”记住，但实际情况是，我们当前说的一句话和较久前说的话未必有很强的关系，如果“全部”记住，一没必要、二计算量巨大。</p><h3 id="bptt-原理">6.1.2 BPTT 原理</h3>以最简单的RNN为例，说明背后算法原理：<center><img src="https://vivounicorn.github.io/images/ai_chapter_6/image_1d9bnmgsa50b1ksr16kb18vg10gh50.png" width="450"></center><p>定义以下符号： <span class="math inline">\(x_i(t)\)</span>：输入层第<span class="math inline">\(i\)</span>个节点；</p><p><span class="math inline">\(s_h(t-1)\)</span>：前一个状态的隐藏层第<span class="math inline">\(h\)</span>个节点；</p><p><span class="math inline">\(s_j(t)\)</span>：当前状态的隐藏层第<span class="math inline">\(j\)</span>个节点；</p><p><span class="math inline">\(y_k(t)\)</span>：输出层第<span class="math inline">\(k\)</span>个节点；</p><p><span class="math inline">\(V\)</span>：输入层到隐藏层权重矩阵；</p><p><span class="math inline">\(U\)</span>：前一状态隐藏层到后一状态隐藏层权重矩阵；</p><p><span class="math inline">\(W\)</span>：隐藏层到输出层权重矩阵;</p><p><span class="math inline">\(f\)</span>：隐藏层激活函数；</p><p><span class="math inline">\(g\)</span>：输出层激活函数。</p><p>网络的前向传播关系为：</p><ul><li>输入层到隐藏层</li></ul><p><span class="math display">\[net_j(t)=\sum_{i=0}^nx_i(t)v_{ji}+\sum_{h=0}^m s_h(t-1)u_{jh}+b_j\]</span> <span class="math display">\[s_j(t)=f(net_j(t))\]</span> 常用的<span class="math inline">\(f\)</span>函数为Sigmoid类函数，如：<span class="math display">\[f(net)=\frac{1}{1+e^{-net}}\]</span></p><ul><li>隐藏层到输出层</li></ul><p><span class="math display">\[net_k(t)=\sum_{j=0}^ms_j(t)w_{kj}+b_k\]</span> <span class="math display">\[y_k(t)=g(net_k(t))\]</span></p><p>常用的<span class="math inline">\(g\)</span>函数为指数族函数，如：</p><p><span class="math display">\[g(net_k)=\frac{e^{net_k}}{\sum_pe^{net_p}}\]</span></p><p>这里需要注意的一个关键点是：<span class="math inline">\(U\)</span>、<span class="math inline">\(V\)</span>、<span class="math inline">\(W\)</span>权重矩阵在不同时刻是共享的。</p><p>网络的反向传播关系：</p><p>只要网络的损失函数可微，那么任意一个前馈神经网络都可以通过误差反向传播（BP）做参数学习，BP本质是利用链式求导，使用梯度下降（GD）算法的最优化求解过程，而翻看前面第四章最优化原理，其求解就是给定目标函数，确定搜索步长和搜索方向的故事，GD的权重更新公式为（其中O为目标函数）： <span class="math display">\[\Delta w=-\eta \frac{\partial(O)}{\partial(w)} \]</span></p><p>同样回看第四章，目标函数多种多样，比如常见的有: SSE：<span class="math display">\[O=\frac{1}{2}\sum_i\sum_j(\hat{y}_{ij}-y_{ij})^2\]</span> cross entropy:<span class="math display">\[O=\sum_i\sum_j\hat{y}_{ij}ln(y_{ij})+(1-\hat{y}_{ij})(1-lny_{ij})\]</span></p><p>但不管哪种目标函数，一般总可以分为线性部分（变量的线性组合）和非线性部分（激活函数），显然求导过程中线性部分最简单，非线性部分最复杂，所以上述权重更新公式可以拆解为：</p><p><span class="math display">\[\Delta w=-\eta \frac{\partial(O)}{\partial(net)}\frac{\partial(net)}{\partial(w)} \]</span> 显然：<span class="math inline">\(\frac{\partial(net)}{\partial(w)}\)</span>很容易计算，而<span class="math inline">\(\frac{\partial(O)}{\partial(net)}\)</span>比较难计算，定义： <span class="math inline">\(\delta=-\frac{\partial(O)}{\partial(net)}\)</span>为每个节点的误差向量，那么整个权重的更新核心考量就是怎么计算和传播<span class="math inline">\(\delta\)</span>。</p><ul><li>输出层任意一个节点<span class="math inline">\(k\)</span></li></ul><p><span class="math display">\[ \delta_{pk}=\frac{\partial(O)}{\partial(y_{pk})}\frac{\partial(y_{pk})}{\partial(net_{pk})}=\frac{\partial(O)}{\partial(y_{pk})} g^{&#39;}(s_{pk}) \]</span></p><ul><li>隐藏层任意一个节点<span class="math inline">\(j\)</span></li></ul><p><span class="math display">\[ \delta_{pj}=-(\sum_k^m\frac{\partial(O)}{\partial(y_{pk})}\frac{\partial(y_{pk})}{\partial(net_{pk})}\frac{\partial(net_{pk})}{\partial(s_{pj})})\frac{\partial(s_{pj})}{\partial(net_{pj})}=\sum_k^m\delta_{pk}w_{kj}f^{&#39;}(s_{pj}) \]</span></p><p>基于以上推导得到：</p><ul><li>当前状态隐藏层到输出层权重更新公式</li></ul><p><span class="math display">\[\Delta w_{kj}=\eta \sum_p^m \delta_{pk}s_{pj} \]</span></p><ul><li>输入层到当前状态隐藏层权重更新公式</li></ul><p><span class="math display">\[\Delta v_{ji}=\eta \sum_p^n \delta_{pj}x_{pi} \]</span></p><ul><li>上一状态隐藏层到当前状态隐藏层权重更新公式</li></ul><p><span class="math display">\[\Delta u_{ji}=\eta \sum_p^m \delta_{pj}s_{ph}(t-1) \]</span></p><ul><li>任一隐层在某个时间状态下的误差</li></ul><p><strong>注意</strong>，有意思的来了：</p><p>对RNN做展开后如图：</p><center><img src="https://vivounicorn.github.io/images/ai_chapter_6/image_1d9h56nst1c125o31pdu1u351no75q.png" width="450"></center>其中<span class="math inline">\(s\)</span>状态前后依赖，所以类似的逻辑，误差会按照时间向后传播，如图：<center><img src="https://vivounicorn.github.io/images/ai_chapter_6/image_1d9hecgavrti1hml4le1kbn1lli9.png" width="450"></center><p>于是误差反向传播公式变为：</p><p><span class="math display">\[ \delta_{pj}(t-1)=-(\sum_h^m\frac{\partial(O)}{\partial(y_{ph})}\frac{\partial(y_{pk})}{\partial(net_{pk})}\frac{\partial(net_{pk})}{\partial(s_{pj})})\frac{\partial(s_{pj})}{\partial(net_{pj})}=\sum_h^m\delta_{ph}(t)u_{hj}f^{&#39;}(s_{pj}(t-1)) \]</span> 其中：<span class="math inline">\(h\)</span>是在<span class="math inline">\(t\)</span>时刻的任何一个隐层节点，<span class="math inline">\(j\)</span>是在<span class="math inline">\(t-1\)</span>时刻的任何一个隐层节点，高层的<span class="math inline">\(\delta\)</span>可以通过循环递归的计算出来，所有<span class="math inline">\(\delta\)</span>计算完毕后累加求和并应用在<span class="math inline">\(U\)</span>、<span class="math inline">\(V\)</span>的权重更新中。</p><h3 id="代码实践">6.1.3 代码实践</h3><p>问题描述：给定一个字符，生成（预测）之后的n个字符，并使得整个句子看上去有语义含义。</p><p>1、训练数据生成如下图：</p><center><img src="https://vivounicorn.github.io/images/ai_chapter_6/image_1ddpb4ckassq1l4b1evk1utu16e613.png" width="600"></center>2、过程说明如下图：<center><img src="https://vivounicorn.github.io/images/ai_chapter_6/image_1ddpb61bnag91iu3ce8ivvrq41g.png" width="600"></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RnnModeling</span>:</span></span><br><span class="line">    bert_len = <span class="number">768</span>                                                      <span class="comment"># length of bert nector.</span></span><br><span class="line">    txt_data_size = <span class="number">0</span>                                                   <span class="comment"># text data size of char level.</span></span><br><span class="line">    iteration = <span class="number">1000</span>                                                    <span class="comment"># iteration of training.</span></span><br><span class="line">    sequence_length = <span class="number">5</span>                                                 <span class="comment"># window of text context.</span></span><br><span class="line">    batch_size = <span class="number">0</span>                                                      <span class="comment"># training batch.</span></span><br><span class="line">    input_size = <span class="number">0</span>                                                      <span class="comment"># size of input layer.</span></span><br><span class="line">    hidden_size = <span class="number">100</span>                                                   <span class="comment"># size of hidden layer.</span></span><br><span class="line">    output_size = <span class="number">0</span>                                                     <span class="comment"># size of output layer.</span></span><br><span class="line">    learning_rate = <span class="number">0.001</span>                                               <span class="comment"># learning rate of optimization algorithm.</span></span><br><span class="line"></span><br><span class="line">    bert_path = <span class="string">&quot;&quot;</span>                                                      <span class="comment"># the path of bert model,you can download it through https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip.</span></span><br><span class="line">    word2vec_path = <span class="string">&quot;&quot;</span>                                                  <span class="comment"># the path of word2vec model,you can download the pre-train model from internet.</span></span><br><span class="line">    chars_set = []                                                      <span class="comment"># all chars in text data.</span></span><br><span class="line">    check_point_dir = <span class="string">&quot;&quot;</span>                                               <span class="comment"># path of check point.</span></span><br><span class="line"></span><br><span class="line">    char_to_int = &#123;&#125;                                                    <span class="comment"># char level encoding, char-&gt;int</span></span><br><span class="line">    int_to_char = &#123;&#125;                                                    <span class="comment"># char level decoding, int-&gt;char</span></span><br><span class="line">    char_encoded = &#123;&#125;                                                   <span class="comment"># one hot encoding</span></span><br><span class="line"></span><br><span class="line">    V = []                                                              <span class="comment"># weight matrix: from input to hidden.</span></span><br><span class="line">    U = []                                                              <span class="comment"># weight matrix: from hidden to hidden.</span></span><br><span class="line">    W = []                                                              <span class="comment"># weight matrix: from hidden to output.</span></span><br><span class="line"></span><br><span class="line">    b_h = []                                                            <span class="comment"># bias vector of hidden layer.</span></span><br><span class="line">    b_y = []                                                            <span class="comment"># bias vector of output layer.</span></span><br><span class="line">    h_prev = []                                                         <span class="comment"># previous hidden state.</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_fine_tuning_path</span>(<span class="params">self, bert_path=<span class="string">&quot;&quot;</span>, word2vec_path=<span class="string">&quot;&quot;</span>, check_point_dir=<span class="string">&quot;&quot;</span></span>):</span></span><br><span class="line">        self.bert_path = bert_path</span><br><span class="line">        self.word2vec_path = word2vec_path</span><br><span class="line">        self.check_point_dir = check_point_dir</span><br><span class="line">        <span class="keyword">if</span> word2vec_path != <span class="string">&quot;&quot;</span> <span class="keyword">and</span> <span class="keyword">not</span> os.path.exists(word2vec_path):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;[Error] the path of word2vec is not exists.&quot;</span>)</span><br><span class="line">            exit(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> bert_path != <span class="string">&quot;&quot;</span> <span class="keyword">and</span> <span class="keyword">not</span> os.path.exists(bert_path):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;[Error] the path of bert is not exists.&quot;</span>)</span><br><span class="line">            exit(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> check_point_dir != <span class="string">&quot;&quot;</span> <span class="keyword">and</span> <span class="keyword">not</span> os.path.exists(check_point_dir):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;[Info] the path of check point is not exists, we&#x27;ll create make it.&quot;</span>)</span><br><span class="line">            os.makedirs(check_point_dir)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">copy_model</span>(<span class="params">self, rnn</span>):</span></span><br><span class="line">        self.bert_len = rnn.bert_len</span><br><span class="line">        self.txt_data_size = rnn.bert_len</span><br><span class="line">        self.iteration = rnn.iteration</span><br><span class="line">        self.sequence_length = rnn.sequence_length</span><br><span class="line">        self.batch_size = rnn.batch_size</span><br><span class="line">        self.input_size = rnn.input_size</span><br><span class="line">        self.hidden_size = rnn.hidden_size</span><br><span class="line">        self.output_size = rnn.output_size</span><br><span class="line">        self.learning_rate = rnn.learning_rate</span><br><span class="line">        self.chars_set = rnn.chars_set</span><br><span class="line">        self.char_to_int = rnn.char_to_int</span><br><span class="line">        self.int_to_char = rnn.int_to_char</span><br><span class="line">        self.char_encoded = rnn.char_encoded</span><br><span class="line">        self.V = rnn.V</span><br><span class="line">        self.U = rnn.U</span><br><span class="line">        self.W = rnn.W</span><br><span class="line">        self.b_h = rnn.b_h</span><br><span class="line">        self.b_y = rnn.b_y</span><br><span class="line">        self.h_prev = rnn.h_prev</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">training_data_analysis</span>(<span class="params">self, txt_data, mode</span>):</span></span><br><span class="line">        self.txt_data_size = <span class="built_in">len</span>(txt_data)</span><br><span class="line">        chars = <span class="built_in">list</span>(<span class="built_in">set</span>(txt_data))</span><br><span class="line">        self.output_size = <span class="built_in">len</span>(chars)</span><br><span class="line">        self.char_to_int = <span class="built_in">dict</span>((c, i) <span class="keyword">for</span> i, c <span class="keyword">in</span> <span class="built_in">enumerate</span>(chars))</span><br><span class="line">        self.int_to_char = <span class="built_in">dict</span>((i, c) <span class="keyword">for</span> i, c <span class="keyword">in</span> <span class="built_in">enumerate</span>(chars))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">&quot;one-hot&quot;</span>:</span><br><span class="line">            self.input_size = <span class="built_in">len</span>(chars)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> i, c <span class="keyword">in</span> <span class="built_in">enumerate</span>(chars):</span><br><span class="line">                letter = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(chars))]</span><br><span class="line">                letter[self.char_to_int[c]] = <span class="number">1</span></span><br><span class="line">                self.char_encoded[c] = np.array(letter)</span><br><span class="line">        <span class="keyword">elif</span> mode == <span class="string">&quot;bert&quot;</span>:</span><br><span class="line">            self.input_size = self.bert_len</span><br><span class="line"></span><br><span class="line">            <span class="keyword">from</span> bert_serving.client <span class="keyword">import</span> BertClient</span><br><span class="line">            bc = BertClient(timeout=<span class="number">1000</span>)</span><br><span class="line">            <span class="comment"># start server: bert-serving-start -model_dir=D:\Github\bert\chinese_L-12_H-768_A-12 -num_worker=1</span></span><br><span class="line">            <span class="keyword">for</span> i, c <span class="keyword">in</span> <span class="built_in">enumerate</span>(chars):</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> c.strip():</span><br><span class="line">                    self.char_encoded[c] = np.array(bc.encode([<span class="string">&#x27;&lt;S&gt;&#x27;</span>]))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    self.char_encoded[c] = np.array(bc.encode([c]))</span><br><span class="line">                <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&#x27;[Debug] bert vector length: %d&#x27;</span> % (<span class="built_in">len</span>(self.char_encoded)))</span><br><span class="line">        <span class="keyword">elif</span> mode == <span class="string">&quot;w2v&quot;</span>:</span><br><span class="line">            <span class="keyword">import</span> gensim</span><br><span class="line">            model = gensim.models.KeyedVectors.load_word2vec_format(self.word2vec_path, binary=<span class="literal">False</span>)</span><br><span class="line">            self.input_size = model.vector_size</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> i, c <span class="keyword">in</span> <span class="built_in">enumerate</span>(chars):</span><br><span class="line">                <span class="keyword">if</span> model.__contains__(c):</span><br><span class="line">                    self.char_encoded[c] = np.array(model[c])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    self.char_encoded[c] = np.zeros((self.input_size, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;[Error] mode type error. it should be one-hot or bert or w2v.&quot;</span>)</span><br><span class="line">            exit(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">model_building</span>(<span class="params">self, itr, seq_len, lr, h_size</span>):</span></span><br><span class="line">        self.iteration = itr</span><br><span class="line">        self.sequence_length = seq_len</span><br><span class="line">        self.learning_rate = lr</span><br><span class="line">        self.batch_size = <span class="built_in">round</span>((self.txt_data_size / self.sequence_length) + <span class="number">0.5</span>)</span><br><span class="line">        self.hidden_size = h_size</span><br><span class="line"></span><br><span class="line">        self.V = np.random.randn(self.hidden_size, self.input_size) * <span class="number">0.01</span>              <span class="comment"># weight input -&gt; hidden.</span></span><br><span class="line">        self.U = np.random.randn(self.hidden_size, self.hidden_size) * <span class="number">0.01</span>             <span class="comment"># weight hidden -&gt; hidden</span></span><br><span class="line">        self.W = np.random.randn(self.output_size, self.hidden_size) * <span class="number">0.01</span>             <span class="comment"># weight hidden -&gt; output</span></span><br><span class="line"></span><br><span class="line">        self.b_h = np.zeros((self.hidden_size, <span class="number">1</span>))</span><br><span class="line">        self.b_y = np.zeros((self.output_size, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        self.h_prev = np.zeros((self.hidden_size, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forwardprop</span>(<span class="params">self, labeling, inputs, h_prev</span>):</span></span><br><span class="line">        x, s, y, p = &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;</span><br><span class="line">        s[-<span class="number">1</span>] = np.copy(h_prev)</span><br><span class="line">        loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(inputs)):                                                        <span class="comment"># t is a &quot;time step&quot;.</span></span><br><span class="line">            x[t] = self.char_encoded[inputs[t]].reshape(-<span class="number">1</span>, <span class="number">1</span>)                              <span class="comment"># input vector.</span></span><br><span class="line">            s[t] = np.tanh(np.dot(self.V, x[t]) + np.dot(self.U, s[t - <span class="number">1</span>]) + self.b_h)      <span class="comment"># hidden state. f(x(t)*V + s(t-1)*U + b), f=tanh.</span></span><br><span class="line">            y[t] = np.dot(self.W, s[t]) + self.b_y                                          <span class="comment"># f(s(t)*W + b), f=x.</span></span><br><span class="line">            p[t] = np.exp(y[t]) / np.<span class="built_in">sum</span>(np.exp(y[t]))                                      <span class="comment"># softmax. f(x)=exp(x)/sum(exp(x))</span></span><br><span class="line">            loss += -np.log(p[t][self.char_to_int[labeling[t]]])                            <span class="comment"># cross-entropy loss.</span></span><br><span class="line">        <span class="keyword">return</span> loss, p, s, x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backprop</span>(<span class="params">self, p, labeling, inputs, s, x</span>):</span></span><br><span class="line">        dV, dU, dW = np.zeros_like(self.V), np.zeros_like(self.U), np.zeros_like(self.W)    <span class="comment"># make all zero matrices.</span></span><br><span class="line">        dbh, dby = np.zeros_like(self.b_h), np.zeros_like(self.b_y)</span><br><span class="line">        delta_pj_1 = np.zeros_like(s[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># error reversed</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(inputs))):</span><br><span class="line">            dy = np.copy(p[t])                                          <span class="comment"># &quot;dy&quot; means &quot;δpk&quot;</span></span><br><span class="line">            dy[self.char_to_int[labeling[t]]] -= <span class="number">1</span>                      <span class="comment"># when using cross entropy loss, δpk=d-y.</span></span><br><span class="line">            dW += np.dot(dy, s[t].T)                                    <span class="comment"># dw/η=δpk * s(t)</span></span><br><span class="line">            dby += dy</span><br><span class="line">            delta_pk_w = np.dot(self.W.T, dy) + delta_pj_1              <span class="comment"># δpk * w.</span></span><br><span class="line">            delta_pj = (<span class="number">1</span> - s[t] * s[t]) * delta_pk_w                   <span class="comment"># δpj = δpk * w * f&#x27;(x); f(x)=tanh; f&#x27;(x)= tanh&#x27;(x) = 1-tanh^2(x)</span></span><br><span class="line">            dbh += delta_pj</span><br><span class="line">            dV += np.dot(delta_pj, x[t].T)                              <span class="comment"># dv/η = δpj * x(t)</span></span><br><span class="line">            dU += np.dot(delta_pj, s[t - <span class="number">1</span>].T)                          <span class="comment"># du/η = δpj * s(t-1)</span></span><br><span class="line">            delta_pj_1 = np.dot(self.U.T, delta_pj)                     <span class="comment"># δpj (t-1) = δpj * u</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> dparam <span class="keyword">in</span> [dV, dU, dW, dbh, dby]:</span><br><span class="line">            np.clip(dparam, -<span class="number">1</span>, <span class="number">1</span>, out=dparam)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dV, dU, dW, dbh, dby</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">model_reading</span>(<span class="params">self, model_read_path</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(model_read_path):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;[Error] the model path %s is not exists.&quot;</span> % model_read_path)</span><br><span class="line">            exit(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        f = <span class="built_in">open</span>(model_read_path, <span class="string">&#x27;rb&#x27;</span>)</span><br><span class="line">        rnn = pickle.load(f)</span><br><span class="line">        ce = pickle.load(f)</span><br><span class="line">        rnn.char_encoded = ce</span><br><span class="line">        f.close()</span><br><span class="line">        self.copy_model(rnn)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">model_saving</span>(<span class="params">self, model_save_path</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(model_save_path):</span><br><span class="line">            os.system(<span class="string">r&quot;touch &#123;&#125;&quot;</span>.<span class="built_in">format</span>(model_save_path))</span><br><span class="line"></span><br><span class="line">        f = <span class="built_in">open</span>(model_save_path, <span class="string">&#x27;wb&#x27;</span>)</span><br><span class="line">        pickle.dump(self, f, protocol=-<span class="number">1</span>)</span><br><span class="line">        pickle.dump(self.char_encoded, f, protocol=-<span class="number">1</span>)</span><br><span class="line">        f.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">model_training</span>(<span class="params">self, txt_data, ischeck=<span class="literal">False</span></span>):</span></span><br><span class="line">        chk_path = self.check_point_dir + <span class="string">&quot;/final.p&quot;</span></span><br><span class="line">        <span class="keyword">if</span> ischeck <span class="keyword">and</span> os.path.exists(chk_path):</span><br><span class="line">            rnn.model_reading(chk_path)</span><br><span class="line"></span><br><span class="line">        mV, mU, mW = np.zeros_like(self.V), np.zeros_like(self.U), np.zeros_like(self.W)</span><br><span class="line">        mbh, mby = np.zeros_like(self.b_h), np.zeros_like(self.b_y)</span><br><span class="line">        loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.iteration):</span><br><span class="line">            self.h_prev = np.zeros((self.hidden_size, <span class="number">1</span>))</span><br><span class="line">            data_pointer = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> b <span class="keyword">in</span> <span class="built_in">range</span>(self.batch_size):</span><br><span class="line">                inputs = [ch <span class="keyword">for</span> ch <span class="keyword">in</span> txt_data[data_pointer:data_pointer + self.sequence_length]]</span><br><span class="line">                targets = [ch <span class="keyword">for</span> ch <span class="keyword">in</span> txt_data[data_pointer + <span class="number">1</span>:data_pointer + self.sequence_length + <span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (data_pointer + self.sequence_length + <span class="number">1</span> &gt;= <span class="built_in">len</span>(txt_data) <span class="keyword">and</span> b == self.batch_size - <span class="number">1</span>):</span><br><span class="line">                    targets.append(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line"></span><br><span class="line">                loss, ps, hs, xs = self.forwardprop(targets, inputs, self.h_prev)</span><br><span class="line"></span><br><span class="line">                dV, dU, dW, dbh, dby = self.backprop(ps, targets, inputs, hs, xs)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> weight, g, his <span class="keyword">in</span> <span class="built_in">zip</span>([self.V, self.U, self.W, self.b_h, self.b_y],</span><br><span class="line">                                              [dV, dU, dW, dbh, dby],</span><br><span class="line">                                              [mV, mU, mW, mbh, mby]):</span><br><span class="line">                    his += g * g                                      <span class="comment"># RMSProp updata</span></span><br><span class="line">                    e = <span class="number">0.5</span> * his / (i + <span class="number">1</span>) + <span class="number">0.5</span> * g * g             <span class="comment"># RMSProp updata</span></span><br><span class="line">                    weight += -self.learning_rate * g / np.sqrt(e + <span class="number">1e-8</span>)   <span class="comment"># RMSProp update</span></span><br><span class="line"></span><br><span class="line">                data_pointer += self.sequence_length</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;[Debug] iteration %d, loss value: %f&#x27;</span> % (i, loss))</span><br><span class="line">                <span class="keyword">if</span> ischeck:</span><br><span class="line">                    self.model_saving(self.check_point_dir+<span class="string">&quot;/chk&quot;</span>+<span class="built_in">str</span>(i)+<span class="string">&quot;.p&quot;</span>)</span><br><span class="line">                    self.model_saving(chk_path)</span><br><span class="line"></span><br><span class="line">        self.model_saving(chk_path)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">model_inference</span>(<span class="params">self, test_char, length</span>):</span></span><br><span class="line">        x = self.char_encoded[test_char].reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        idx = []</span><br><span class="line">        h = np.zeros((self.hidden_size,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(length):</span><br><span class="line">            h = np.tanh(np.dot(self.V, x) + np.dot(self.U, h) + self.b_h)</span><br><span class="line">            y = np.dot(self.W, h) + self. b_y</span><br><span class="line">            p = np.exp(y) / np.<span class="built_in">sum</span>(np.exp(y))</span><br><span class="line">            ix = <span class="built_in">list</span>(p).index(<span class="built_in">max</span>(<span class="built_in">list</span>(p)))</span><br><span class="line">            x = self.char_encoded[self.int_to_char[ix]].reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            idx.append(ix)</span><br><span class="line">        txt = <span class="string">&#x27;&#x27;</span>.join(self.int_to_char[i] <span class="keyword">for</span> i <span class="keyword">in</span> idx)</span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&#x27;[Debug] %s-%s&#x27;</span> % (test_char, txt))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self, txt_data, bert_path=<span class="string">&quot;&quot;</span>, word2vec_path=<span class="string">&quot;&quot;</span>, check_point_path=<span class="string">&quot;&quot;</span>, ischeck=<span class="literal">True</span>, model_type=<span class="string">&quot;bert&quot;</span>, \</span></span></span><br><span class="line"><span class="params"><span class="function">            itr=<span class="number">1000</span>, seq_len=<span class="number">10</span>, lr=<span class="number">0.001</span>, h_size=<span class="number">100</span></span>):</span></span><br><span class="line">        self.set_fine_tuning_path(bert_path, word2vec_path, check_point_path)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> ischeck:</span><br><span class="line">            self.training_data_analysis(txt_data, model_type)</span><br><span class="line">            self.model_building(itr, seq_len, lr, h_size)</span><br><span class="line"></span><br><span class="line">        self.model_training(txt_data, ischeck)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    txt_data = <span class="string">&quot;当地时间6月17日，第53届巴黎-布尔歇国际航空航天展览会（即巴黎航展）开幕。 开幕当天，法国总统马克龙亲自为法国、德国与西班牙三国联合研制的“新一代战斗机”（NGF）的全尺寸模型揭幕。法、德、西三国防长也出席了模型揭幕仪式，并在仪式后签署了三方合作协议，正式欢迎西班牙加入“新一代战机”的联合研制。NGF与美国的F-22、F-35、俄罗斯的苏-57以及中国的歼-20一样，同属第五代战斗机。&quot;</span></span><br><span class="line">    rnn = RnnModeling()</span><br><span class="line">    rnn.set_fine_tuning_path(check_point_dir=<span class="string">&quot;e://&quot;</span>, word2vec_path=<span class="string">&#x27;E:\\BaiduNetdiskDownload\\zhwiki\\zhwiki_2017_03.sg_50d.word2vec&#x27;</span>)</span><br><span class="line">    rnn.run(txt_data, ischeck=<span class="literal">False</span>, check_point_path=<span class="string">&quot;e://&quot;</span>)</span><br><span class="line">    rnn.model_inference(<span class="string">&#x27;法&#x27;</span>, <span class="number">10</span>)</span><br><span class="line">    rnn.model_inference(<span class="string">&#x27;巴&#x27;</span>, <span class="number">10</span>)</span><br><span class="line">    rnn.model_inference(<span class="string">&#x27;歼&#x27;</span>, <span class="number">10</span>)</span><br><span class="line">    rnn.model_inference(<span class="string">&#x27;新&#x27;</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>训练及测试结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[Debug] iteration 0, loss value: 28.059744</span><br><span class="line">[Debug] iteration 100, loss value: 2.966153</span><br><span class="line">[Debug] iteration 200, loss value: 1.247683</span><br><span class="line">[Debug] iteration 300, loss value: 0.931227</span><br><span class="line">[Debug] iteration 400, loss value: 0.848960</span><br><span class="line">[Debug] iteration 500, loss value: 0.812240</span><br><span class="line">[Debug] iteration 600, loss value: 0.791726</span><br><span class="line">[Debug] iteration 700, loss value: 0.779109</span><br><span class="line">[Debug] iteration 800, loss value: 0.770437</span><br><span class="line">[Debug] iteration 900, loss value: 0.764572</span><br><span class="line">[Debug] 法-国、德国与美国的F-</span><br><span class="line">[Debug] 巴-黎航展）开幕。 开幕</span><br><span class="line">[Debug] 歼--20一样，同属第五</span><br><span class="line">[Debug] 新-一代战斗机”（NGF</span><br></pre></td></tr></table></figure><p></p><h2 id="混沌理论">6.2 混沌理论</h2><p>关于混沌(Chaos)一词，西方和东方在哲学认知和神话传说上惊人的相似。例如古希腊神话中描述的：万物之初，先有混沌，是一个无边无际、空空如也的空间，在发生某种扰动后，诞生了大地之母Gaea等等，世界从此开始。中国古代神话中，天地未开之前，宇宙以混沌状模糊一团，盘古开天辟地后世界从此开始。</p><p>而在现代自然科学中，混沌理论的发展反映了人们对客观世界认知一步步演化的过程。人类对自然规律的认知，也从确定性(Deterministic)认知主导逐步演进到概率性(Probabilistic)认知主导。</p><ul><li><p>经典力学与机械决定论</p><p>牛顿1686年创立了基于万有引力定律和三大定律的古典力学，即：第一定律，在沒有被外力作用下的物体，会保持静止或匀速直线运动状态；第二定律，物体的加速度与物体所受外力的合力成成正比，与物体本身的质量成反比，且加速度方向与合力方向相同；第三定律，两个物体间的作用力和反作用力大小相等方向相反。牛顿的这种基于确定性认知的绝对时空观在很长一段时间占据主导位置，例如拉普拉斯甚至认为：没有什么是不确定的，宇宙的现在是由其过去决定的，只要给定初始条件，智者可以用一个公式概括整个宇宙，预测宇宙未来的发展。拉普拉斯对于概率论有着巨大的贡献，但他认为概率论只是对决定论的补充而已。</p></li><li><p>三体问题</p><p>在那个年代，虽然人们对太阳、地球、月亮的运动规律了解的比较清楚，但对三个天体在长时间运动过程中，状态是否保持稳定、能否永远稳定运行等相关问题却没有什么认知，即理论上认为确定性的事情，而事实上却无法用已知的数学模型表达，这个就是天体力学中的经典模型——三体问题。19世纪末，人们已经知道，在一般三体问题中每个天体在其他两个天体引力作用下的运动方程都可以表示成6个一阶常微分方程，这意味着总共需要求18个完全积分才能获得完整解析解，而理论上只能得到10个完全积分，即描述三个或三个以上天体运动的方程组不可积分，更不能得到解析解。 虽然后来欧拉和拉格朗日分别在给定约束条件下求得了限制性三体问题的5个特殊解，即著名的<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E7%82%B9">拉格朗日点</a>，但通用三体问题依然无解。</p></li><li><p>庞加莱的错误</p><p>庞加莱(就是那个提出庞加莱猜想的庞加莱)在1887年参加瑞典国王发起的“太阳系是稳定的么”的竞赛中，对限定性三体问题发表了一篇论文，初稿出来后，一个名叫<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Lars_Edvard_Phragm%C3%A9n">弗拉格曼</a>的26岁年轻人发现了其中有不明确的部分，而庞加莱在修改过程中发现了原来的证明有错误，于是在深思熟虑后彻底抛弃了原来通过定量分析求解的方法，转而以定性分析求解并成功给出三体问题的定性解答，而那个错误是由于初始条件的微小误差导致最终结果的南辕北辙，这个观察清晰而定性的开辟了混沌理论。</p></li><li><p>蝴蝶效应</p><p>现代科学史中，真正意义上的混沌理论是MIT的<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E7%88%B1%E5%BE%B7%E5%8D%8E%C2%B7%E8%AF%BA%E9%A1%BF%C2%B7%E6%B4%9B%E4%BC%A6%E8%8C%A8">洛伦茨</a>(Lorenz)提出的，在此之前人们原本以为，只要配上动力学公式和超级计算机，就能模拟出自然界的各种现象，1961年，洛伦茨用Royal McBee LGP-30计算机(16k内存，每秒60次乘法运算)做气象动力学模拟实验时，由于一个偶然的对初始值做四舍五入的处理，导致模拟结果大相径庭。基于这次实验，1963年，洛伦茨在《气象学报》发表了《確定性的非周期流》系列，以物理意义更加明确的数学模型表示和发展了混沌理论。</p><center><img src="https://vivounicorn.github.io/images/ai_chapter_6/image_1eck5ab2i1k6en2p110u4ic1i801t.png" width="350"> 洛伦茨系统</center></li></ul><p>简单看一个关于流体热力传导的问题：</p><center><img src="https://vivounicorn.github.io/images/ai_chapter_6/image_1edgkmmg78npnk1cbq1vmpaq81e.png" width="350"></center><p>当温差较小时，热力会以传导的方式从热的板块到冷的板块，当温差较大时，下面的暖流体上升，上面的冷流体下沉，冷、热板块会产生对流滚动。针对这个问题，洛伦茨将他原始方程中除三个傅立叶系数外的其他系数都设为0，得到了简化的微分方程：</p><p><span class="math display">\[ \begin{align*} &amp; \dot{x}=-\sigma x+\sigma y \\ &amp; \dot{y}=-xz+rx-y \\ &amp; \dot{z}=xy-bz \\ \end{align*} \]</span></p><p>其中<span class="math inline">\(\sigma\)</span>是Prandtl系数、<span class="math inline">\(r\)</span>是Rayleigh系数、<span class="math inline">\(b\)</span>是系统参数，决定了循环的宽度(图中<span class="math inline">\(T_u\)</span>与<span class="math inline">\(T_l\)</span>质检的距离)、<span class="math inline">\(x\)</span>与循环流体的流苏成正比，且<span class="math inline">\(x&gt;0\)</span>时流体顺时针对流，<span class="math inline">\(x&lt;0\)</span>时流体逆时针对流、<span class="math inline">\(y\)</span>与温差成正比、<span class="math inline">\(z\)</span>与垂直温度曲线与平衡温度曲线的失真成正比。</p><p>洛伦茨发现，当<span class="math inline">\(\sigma=10\)</span>且<span class="math inline">\(b=8/3\)</span>时，只要Rayleigh系数超过<span class="math inline">\(r\approx 24.74\)</span>，系统就会表现为"混沌"，即所有的解似乎对初始条件都很敏感，几乎所有的解显然既不是周期性解，也不收敛于周期性解。换句话说，洛伦茨用一个确定性的方程告诉我们一个热力学动态系统的不可预知性。</p><p>回想电视剧里看到的离奇故事：</p><p>1、因为一滴雨水掉在了马的眼睛里，马摔倒了；</p><p>2、恰好骑马的是一名斥候，斥候受伤了；</p><p>3、斥候受伤导致手上的情报没有被及时送到军营，军队战败了；</p><p>4、军队战败导致重要城市丢失，敌军长驱直入进入都城；</p><p>5、都城被灭，皇帝被杀，国家灭亡。</p><p>混沌理论的核心思想是：初始条件的微小差别或变化，可以导致最终结果发生剧烈的变化。</p><p>下面从理论方面做一些简单介绍，帮助理解未来我们会用到的一些概念。</p><h3 id="一维映射">6.2.1 一维映射</h3><p>1、<strong>动态系统(Dynamical System)</strong></p><p>一个动态系统由一组可能的状态组成，再加上一个用过去的状态来确定现在的状态的规则。最典型的动态系统是时间离散动态系统(discrete-time dynamical system)和时间连续动态系统(m continuous-time dynamical system)，前面我们介绍的RNN就是一种离散动态系统。</p><p>很多现实当中问题往往是随着时间演化的动态系统，例如：模拟细菌生长过程，在给定初始细菌数后，随着时间流逝，细菌数量增长的模型如下：</p><p><span class="math display">\[x_n=f(x_{n-1})=2x_{n-1}=f(f(x_{n-2}))=f^n(x_0)=2^nx_0\]</span> <span class="math inline">\(x_0\)</span>表示初始细菌数，<span class="math inline">\(n\)</span>表示随时间演化，显然这个增长过程是以指数增长的。</p>2、<strong>固点(Fixed Points)</strong> 如果动态系统有映射<span class="math inline">\(f\)</span>，且满足<span class="math inline">\(f(p)=p\)</span>，则<span class="math inline">\(p\)</span>被称为固点。还以上面细菌生长过程为例，几何意义如下图表示：<center><img src="https://vivounicorn.github.io/images/ai_chapter_6/image_1echc1m1417va1si31n9l4uqo4813.png" width="450"></center><p>利用<span class="math inline">\(y=x\)</span>直线发现动态系统的固点只有x=0这一点，画出动态系统的演化轨迹(虚线部分)，随着时间流逝，细菌种群规模趋向于正无穷。</p><p>但真实情况是，受限于环境、资源等因素，细菌种群规模不可能无限大，所以修改动态系统为： <span class="math display">\[x_n=f(x_{n-1})=2x_{n-1}(1-x_{n-1})=f(f(x_{n-2}))=f^n(x_0)\]</span></p>几何意义如下图表示：<center><img src="https://vivounicorn.github.io/images/ai_chapter_6/image_1echc0ujc1crhpl71tov2vt165im.png" width="450"></center><p>利用<span class="math inline">\(y=x\)</span>直线发现动态系统的固点有x=0和x=0.5这两个点，画出动态系统的演化轨迹（虚线），随着时间流逝，不管初始种群<span class="math inline">\(x_0\)</span>取多少，细菌种群规模最终趋向于0.5(被吸引到0.5)，用R做个简单模拟：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">g &lt;- <span class="keyword">function</span>(x)&#123;</span><br><span class="line">  <span class="built_in">return</span>(<span class="number">2</span>*x*(<span class="number">1</span>-x))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">gk &lt;- <span class="keyword">function</span>(k, x)&#123;</span><br><span class="line">  <span class="keyword">for</span>(i <span class="keyword">in</span> <span class="number">1</span>:k)&#123;</span><br><span class="line">    x = g(x)</span><br><span class="line">    print(x)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">k=<span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (i <span class="keyword">in</span> <span class="number">1</span>:<span class="number">10</span>)&#123;</span><br><span class="line">  x=runif(<span class="number">1</span>)</span><br><span class="line">  gk(k, x)</span><br><span class="line">  print(<span class="string">&quot;=====&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>部分结果如下：</p><table><thead><tr class="header"><th>t</th><th style="text-align:right">y(x=0.941631)</th><th style="text-align:right">y(x=0.6455615 )</th><th style="text-align:right">y(x=0.1207315 )</th></tr></thead><tbody><tr class="odd"><td>1</td><td style="text-align:right">0.1099241</td><td style="text-align:right">0.4576237</td><td style="text-align:right">0.2123107</td></tr><tr class="even"><td>2</td><td style="text-align:right">0.1956815</td><td style="text-align:right">0.4964085</td><td style="text-align:right">0.3344698</td></tr><tr class="odd"><td>3</td><td style="text-align:right">0.3147805</td><td style="text-align:right">0.4999742</td><td style="text-align:right">0.4451995</td></tr><tr class="even"><td>4</td><td style="text-align:right">0.4313875</td><td style="text-align:right">0.5</td><td style="text-align:right">0.4939938</td></tr><tr class="odd"><td>5</td><td style="text-align:right">0.4905846</td><td style="text-align:right">0.5</td><td style="text-align:right">0.4999279</td></tr><tr class="even"><td>6</td><td style="text-align:right">0.4998227</td><td style="text-align:right">0.5</td><td style="text-align:right">0.5</td></tr><tr class="odd"><td>7</td><td style="text-align:right">0.4999999</td><td style="text-align:right">0.5</td><td style="text-align:right">0.5</td></tr><tr class="even"><td>8</td><td style="text-align:right">0.5</td><td style="text-align:right">0.5</td><td style="text-align:right">0.5</td></tr><tr class="odd"><td>9</td><td style="text-align:right">0.5</td><td style="text-align:right">0.5</td><td style="text-align:right">0.5</td></tr><tr class="even"><td>10</td><td style="text-align:right">0.5</td><td style="text-align:right">0.5</td><td style="text-align:right">0.5</td></tr></tbody></table><p>3、<strong>稳定的固点(Stability of Fixed Points)</strong></p><p>假设动态系统的映射为<span class="math inline">\(f(x) =\frac{(3x-x^3)}{2}\)</span>，几何形态如下：</p><center><img src="https://vivounicorn.github.io/images/ai_chapter_6/image_1echdls5q1f991o6f1bba1fb5bor1g.png" width="450"></center><p>利用<span class="math inline">\(y=x\)</span>直线发现动态系统的固点有<span class="math inline">\(x_0=0\)</span>和<span class="math inline">\(x_1=1\)</span>和<span class="math inline">\(x_2=-1\)</span>这三个点，画出动态系统的演化轨迹（虚线），其中<span class="math inline">\(x_1\)</span>和<span class="math inline">\(x_2\)</span>两个点被称为稳定固点，在<span class="math inline">\(x_1\)</span>和<span class="math inline">\(x_2\)</span>两个值的某个邻域内，y会分别收敛于1和-1两个值，<span class="math inline">\(x_0\)</span>为不稳定固点，在它的+邻域内y会被推到上半区，-邻域内y会被推到下半区。</p><p>4、<strong>吸引固点(Sink)与排斥固点(Source)</strong></p><p>首先对点<span class="math inline">\(p\)</span>定义它的邻域：</p><p><span class="math display">\[N_\epsilon(p)=\{x\in R:|x-p|&lt;\epsilon\},0&lt;\epsilon&lt;&lt;1\]</span></p><p>其次，假设动态系统有映射<span class="math inline">\(f\)</span>，点<span class="math inline">\(p\)</span>为实数值，且满足<span class="math inline">\(f(p)=p\)</span>，如果存在<span class="math inline">\(p\)</span>的邻域<span class="math inline">\(N_\epsilon(p)\)</span>，使得所有邻域内的点会被吸引到<span class="math inline">\(p\)</span>点，即：</p><p><span class="math display">\[lim_{k-&gt;\infty}f^k(x)=p,x\in N_\epsilon(p)\]</span></p><p>则点<span class="math inline">\(p\)</span>被称作Sink，反之如果邻域内的点会被排斥远离点<span class="math inline">\(p\)</span>，则点<span class="math inline">\(p\)</span>被称作Source。</p><p>数学化表示如下，记住这个表示，未来解释为什么RNN无法利用梯度下降学到长依赖关系时会用到：</p><p>如果<span class="math inline">\(f\)</span>是一个在实数集上的平滑映射，假设<span class="math inline">\(p\)</span>是<span class="math inline">\(f\)</span>的固点，则：</p><p>1、如果<span class="math inline">\(|f&#39;(p)|&lt;1\)</span>，则<span class="math inline">\(p\)</span>是吸引固点Sink；</p><p>2、如果<span class="math inline">\(|f&#39;(p)|&gt;1\)</span>，则<span class="math inline">\(p\)</span>是排斥固点Source。</p><p><strong>证明</strong>： 假设<span class="math inline">\(\alpha\)</span>是介于<span class="math inline">\(|f&#39;(p)|\)</span>和1之间的任意实数，对于：</p><p><span class="math display">\[lim_{x-&gt;p}\frac{|f(x)-f(p)|}{|x-p|}=|f&#39;(p)|\]</span> 存在一个<span class="math inline">\(p\)</span>的邻域<span class="math inline">\(N_\epsilon(p),\epsilon&gt;0\)</span>，使得：</p><p><span class="math display">\[\frac{|f(x)-f(p)|}{|x-p|}&lt;a&lt;1,(x\in N_\epsilon(p),x\neq p)\]</span></p><p>换句话说，相比<span class="math inline">\(x\)</span>，<span class="math inline">\(f(x)\)</span>更接近<span class="math inline">\(p\)</span>，也说明，如果<span class="math inline">\(x\in N_\epsilon(p)\)</span>，则<span class="math inline">\(f(x)\in N_\epsilon(p)\)</span>，以此类推，<span class="math inline">\(f^2(x)\)</span>、<span class="math inline">\(f^3(x)\)</span>也满足此性质，归纳下变成： <span class="math display">\[|f^k(x)-p|\leq\alpha^k|x-p|,k\geq 1\]</span></p><p>所以<span class="math inline">\(p\)</span>是一个吸引固点Sink。</p><p>换一个角度，从一阶泰勒展开式或导数的定义来看：</p><p>在<span class="math inline">\(p\)</span>点的一阶泰勒展开式： <span class="math display">\[f(p+h)\approx f(p)+hf&#39;(p)\]</span></p><p>1、如果<span class="math inline">\(|f&#39;(p)|&lt;1\)</span>，则<span class="math inline">\(p\)</span>是吸引固点Sink；</p><p>2、如果<span class="math inline">\(|f&#39;(p)|&gt;1\)</span>，则<span class="math inline">\(p\)</span>是排斥固点Source。</p><p>5、<strong>k周期点</strong></p>举一个例子：<span class="math inline">\(f(x)=3.3x(1-x)\)</span>，其图形如下：<center><img src="https://vivounicorn.github.io/images/ai_chapter_6/image_1ecm4fbi01ci51p1bgvj1opf1kag2a.png" width="450"></center><p>利用<span class="math inline">\(y=x\)</span>直线发现动态系统的固点有<span class="math inline">\(x_0=0\)</span>和<span class="math inline">\(x_1=\frac{23}{33}\)</span>两个点，而这两个点都是排斥固点，那么吸引固点去哪儿了呢？做一个简单模拟：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">g &lt;- function(x)&#123;</span><br><span class="line">  return(3.3*x*(1-x))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">gk &lt;- function(k, x)&#123;</span><br><span class="line">  for(i in 1:k)&#123;</span><br><span class="line">    x = g(x)</span><br><span class="line">    print(x)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">k=20</span><br><span class="line"></span><br><span class="line">for (i in 1:10)&#123;</span><br><span class="line">  x=runif(1)</span><br><span class="line">  gk(k, x)</span><br><span class="line">  print(&quot;=====&quot;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>部分结果如下：</p><table><thead><tr class="header"><th>t</th><th style="text-align:right">y(x=0.1156445)</th><th style="text-align:right">y(x=0.3317354)</th><th style="text-align:right">y(x=0.0.9131461)</th></tr></thead><tbody><tr class="odd"><td>1</td><td style="text-align:right">0.3374938</td><td style="text-align:right">0.7315672</td><td style="text-align:right">0.2617241</td></tr><tr class="even"><td>2</td><td style="text-align:right">0.7378527</td><td style="text-align:right">0.6480429</td><td style="text-align:right">0.6376411</td></tr><tr class="odd"><td>3</td><td style="text-align:right">0.6383061</td><td style="text-align:right">0.7526749</td><td style="text-align:right">0.7624812</td></tr><tr class="even"><td>4</td><td style="text-align:right">0.7618757</td><td style="text-align:right">0.6143128</td><td style="text-align:right">0.5976419</td></tr><tr class="odd"><td>5</td><td style="text-align:right">0.5986897</td><td style="text-align:right">0.7818775</td><td style="text-align:right">0.793538</td></tr><tr class="even"><td>6</td><td style="text-align:right">0.7928592</td><td style="text-align:right">0.5627987</td><td style="text-align:right">0.540657</td></tr><tr class="odd"><td>7</td><td style="text-align:right">0.5419706</td><td style="text-align:right">0.8119858</td><td style="text-align:right">0.8195451</td></tr><tr class="even"><td>8</td><td style="text-align:right">0.8191869</td><td style="text-align:right">0.5037939</td><td style="text-align:right">0.48804</td></tr><tr class="odd"><td>9</td><td style="text-align:right">0.488795</td><td style="text-align:right">0.8249525</td><td style="text-align:right">0.824528</td></tr><tr class="even"><td>10</td><td style="text-align:right">0.8245857</td><td style="text-align:right">0.4765394</td><td style="text-align:right">0.4774493</td></tr><tr class="odd"><td>11</td><td style="text-align:right">0.4773257</td><td style="text-align:right">0.8231837</td><td style="text-align:right">0.8233218</td></tr><tr class="even"><td>12</td><td style="text-align:right">0.8233034</td><td style="text-align:right">0.4803226</td><td style="text-align:right">0.4800279</td></tr><tr class="odd"><td>13</td><td style="text-align:right">0.4800672</td><td style="text-align:right">0.8237222</td><td style="text-align:right">0.8236837</td></tr><tr class="even"><td>14</td><td style="text-align:right">0.8236889</td><td style="text-align:right">0.4791729</td><td style="text-align:right">0.4792553</td></tr><tr class="odd"><td>15</td><td style="text-align:right">0.4792442</td><td style="text-align:right">0.8235686</td><td style="text-align:right">0.8235799</td></tr><tr class="even"><td>16</td><td style="text-align:right">0.8235784</td><td style="text-align:right">0.4795012</td><td style="text-align:right">0.479477</td></tr><tr class="odd"><td>17</td><td style="text-align:right">0.4794803</td><td style="text-align:right">0.8236133</td><td style="text-align:right">0.8236101</td></tr><tr class="even"><td>18</td><td style="text-align:right">0.8236105</td><td style="text-align:right">0.4794056</td><td style="text-align:right">0.4794125</td></tr><tr class="odd"><td>19</td><td style="text-align:right">0.4794116</td><td style="text-align:right">0.8236004</td><td style="text-align:right">0.8236013</td></tr><tr class="even"><td>20</td><td style="text-align:right">0.8236012</td><td style="text-align:right">0.4794332</td><td style="text-align:right">0.4794312</td></tr></tbody></table><p>一个有意思的现象出现，<span class="math inline">\(p_1=0.4694\)</span>和<span class="math inline">\(p_2=0.8236\)</span>交替出现且为吸引固点，换个角度就是：<span class="math inline">\(f(p_1)=p_2,f(p_2)=p_1;f^2(p_1)=p_1\)</span>,<span class="math inline">\(f^2(p_2)=p_2\)</span>，也就是吸引固点以2为周期出现，在两个点间循环往复。</p><p>形式化<strong>定义</strong>为：</p><p>假设动态系统有实数集上的映射<span class="math inline">\(f\)</span>，点<span class="math inline">\(p\)</span>为实数值，且满足<span class="math inline">\(f^k(p)=p\)</span>，<span class="math inline">\(k\)</span>为正整数，则<span class="math inline">\(p\)</span>被称为k周期点。</p><p>扩展下之前吸引固点的定义到k周期点：</p><p>如果<span class="math inline">\(f\)</span>是一个在实数集上的平滑映射，假设<span class="math inline">\(\{p_1,p_2,...p_k\}\)</span>构成了<span class="math inline">\(f\)</span>的<span class="math inline">\(k\)</span>周期点，则：</p><p>1、如果<span class="math inline">\(|f&#39;(p_k)...f&#39;(p_1)|&lt;1\)</span>，则<span class="math inline">\(\{p_1,p_2,...p_k\}\)</span>是吸引固点Sink；</p><p>2、如果<span class="math inline">\(|f&#39;(p_k)...f&#39;(p_1)|&gt;1\)</span>，则<span class="math inline">\(\{p_1,p_2,...p_k\}\)</span>是排斥固点Source。</p><p>还是上面的那个例子:</p><p><span class="math inline">\(f(x)=3.3x(1-x)\)</span></p><p>则有： <span class="math inline">\(f&#39;(x)=3.3-6.6x\)</span> k周期点为：<span class="math inline">\(\{0.4694,0.8236\}\)</span></p><p>因为<span class="math inline">\(|f&#39;(0.4694)f&#39;(0.8236)|=0.2904&lt;1\)</span>，所以它是吸引固点。</p><h3 id="二维映射">6.2.2 二维映射</h3><p>把一维映射扩展到多维映射，看看会出现什么有趣的现象，由于二维映射是多维映射的最简单形式且各种性质与多维映射一致，固以此为基础讨论。</p><p>1、<strong>邻域</strong></p><p>扩展一维映射时的邻域概念如下：</p><p>在欧式空间实数域下，向量<span class="math inline">\(v=(x_1,x_2,......x_m)\)</span>的范数定义为：</p><p><span class="math display">\[|v|=\sqrt{x_1^2+x_2^2+......+x_m^2}\]</span></p><p>对<span class="math inline">\(p=(p_1,p_2,......p_m)\)</span>定义它的邻域：</p><p><span class="math display">\[N_\epsilon(p)=\{v\in \mathbb{R}^m:|v-p|&lt;\epsilon\},0&lt;\epsilon&lt;&lt;1\]</span></p><p>有时候也叫<span class="math inline">\(p\)</span>的<span class="math inline">\(\epsilon\)</span>-<span class="math inline">\(disk\)</span>，举个例子。</p>二维下(<span class="math inline">\(p=(1,1)\)</span>)：<center><img src="https://vivounicorn.github.io/images/ai_chapter_6/2d" width="450"></center>三维下(<span class="math inline">\(p=(1,1,1)\)</span>)：<center><img src="https://vivounicorn.github.io/images/ai_chapter_6/3d" width="450"></center><p>2、<strong>固点</strong></p><p>其次，假设动态系统有实数域<span class="math inline">\(\mathbb{R}^m\)</span>的映射<span class="math inline">\(f\)</span>，<span class="math inline">\(p\)</span>为实数域<span class="math inline">\(\mathbb{R}^m\)</span>固点，即满足<span class="math inline">\(f(p)=p\)</span>，如果存在<span class="math inline">\(p\)</span>的邻域<span class="math inline">\(N_\epsilon(p)\)</span>，使得所有邻域内<span class="math inline">\(v\)</span>会被吸引到<span class="math inline">\(p\)</span>，即： <span class="math display">\[lim_{k-&gt;\infty}f^k(v)=p,v\in N_\epsilon(p)\]</span></p><p>则<span class="math inline">\(p\)</span>被称作<strong>Sink</strong>，反之如果邻域内的点会被排斥远离点<span class="math inline">\(p\)</span>，则点<span class="math inline">\(p\)</span>被称作<strong>Source</strong>。</p>在二维映射下还会出现一个一维映射时不会出现的固点，叫做鞍点(<strong>Saddle</strong>)，可以把它看做介于吸引固点和排斥固点间的一种状态，它拥有至少一个吸引方向和至少一个排斥方向。<center><img src="https://vivounicorn.github.io/images/ai_chapter_6/image_1ed8qo1fid0m1rmk1qupdb0s1311.png" width="600"></center><p>图中<span class="math inline">\(f\)</span>代表映射，<span class="math inline">\(N\)</span>代表<span class="math inline">\(p\)</span>点的邻域。<span class="math inline">\(a\)</span>图代表<span class="math inline">\(p\)</span>是一个吸引固点，进入其邻域的点会被吸引到<span class="math inline">\(p\)</span>点、<span class="math inline">\(b\)</span>图代表<span class="math inline">\(p\)</span>是一个排斥固点，进入其邻域的点会被排斥而远离<span class="math inline">\(p\)</span>点、<span class="math inline">\(c\)</span>图代表<span class="math inline">\(p\)</span>是一个鞍点，进入其邻域的点先会被吸引到<span class="math inline">\(p\)</span>点，然后会被排斥而远离<span class="math inline">\(p\)</span>点。来个更直观的图：</p><center><img src="https://vivounicorn.github.io/images/ai_chapter_6/saddle.gif" width="450"></center><p>在《<a href="https://vivounicorn.github.io/page/2021/09/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB-%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E6%9C%80%E4%BC%98%E5%8C%96%E5%8E%9F%E7%90%86/">最优化原理-梯度下降</a>》这一章我们曾经介绍过常用的一阶最优化方法，给定初始值后，不同的优化方法的优化轨迹不一样，但大的方向都是先被迭代吸引到鞍点，然后再从鞍点被排斥走，而因为待优化问题往往有很多局部最优点，所以我们希望优化算法能尽可能跳出当前点去寻找更优的局部最优点。</p><p>综上所述，显然排斥固点Source和鞍点Saddle的最大特点是：它们都是固点，都不是稳定固点，因为它们对初始条件很敏感，但对研究一个动态系统它们很重要。</p><h3 id="线性映射">6.2.3 线性映射</h3><p>1、<strong>线性映射</strong></p><p>所谓线性映射是指：</p><p>给定实数<span class="math inline">\(a,b\in \mathbb{R}\)</span>及实数向量<span class="math inline">\(v,w \in \mathbb{R}^m\)</span>，有<span class="math inline">\(\mathbb{R}^m\to \mathbb{R}^m\)</span>的映射<span class="math inline">\(A(v)\)</span>满足: <span class="math display">\[A(av+bw)=aA(v)+bA(w)\]</span> 显然原点(0,0)是所有线性映射的固点，且是稳定的，如果它邻域内的点在迭代映射时都趋向于接近固点，则该固点是一个<strong>吸引子</strong>，稍微正式点的定义如下：</p><p>在一个随时间演变的动态系统中，吸引子是一个代表某种稳定状态的数值集合，在给定动态系统初始状态后，系统有着朝该集合所表示的稳态演化的趋势，在吸引子的某个邻域(basin of attraction)范围内，即使系统受到扰动，也会趋向于该稳态。</p><p>后面会大量出现吸引子这个概念。</p><p>2、<strong>鞍点</strong></p><p>如果实数<span class="math inline">\(\lambda\)</span>和实数向量<span class="math inline">\(v\)</span>满足：</p><p><span class="math display">\[Av=\lambda v\]</span> 则它们分别被称为A的特征值和特征向量。</p><p>假设有以下向量关系：</p><p><span class="math display">\[v_{n+1}=Av_n\]</span></p><p>则有递推关系：</p><p><span class="math display">\[ \begin{align*} &amp; v_1=Av_0=\lambda v_0 \\ &amp; v_2=Av_1=\lambda v_1=\lambda^2v_0 \\ &amp; ...... \\ &amp; v_{n+1}=\lambda^n v_0\\ \end{align*} \]</span> 以<span class="math inline">\(\mathbb{R}^2\)</span>上的映射为例：</p><p><span class="math display">\[A(x,y)=(ax,by)\]</span></p><p>表示成矩阵形式： <span class="math display">\[ Av= \left[ \begin{matrix} a &amp; 0 \\ 0 &amp; b \end{matrix} \right] \left[ \begin{matrix} x\\ y \end{matrix} \right] \]</span> 以上过程迭代了<span class="math inline">\(n\)</span>次后得到： <span class="math display">\[ A^n= \left[ \begin{matrix} a^n &amp; 0 \\ 0 &amp; b^n \end{matrix} \right] \]</span></p><p>这里就有意思了，<span class="math inline">\(A\)</span>迭代了<span class="math inline">\(n\)</span>次后，把它映射在一个二维平面上，看上去应该是个椭圆形，其中横坐标长度为<span class="math inline">\(|a|^n\)</span>，纵坐标为<span class="math inline">\(|b|^n\)</span>，对于原点的某个邻域<span class="math inline">\(N_\epsilon(0,0)\)</span>同样也是个椭圆，横纵坐标长度分别为<span class="math inline">\(\epsilon|a|^n\)</span>和<span class="math inline">\(\epsilon|b|^n\)</span>，假设<span class="math inline">\(n\to \infty\)</span>，则会有三种情况：</p><p>1、如果<span class="math inline">\(|a|,|b|&lt;1\)</span>，则整个椭圆会收缩到原点(0,0)，原点是Sink；</p><p>2、如果<span class="math inline">\(|a|,|b|&gt;1\)</span>，则整个椭圆会无限过大并远离原点(0,0)，原点是Source；</p><p>3、如果<span class="math inline">\(|a|&gt;1&gt;|b|\)</span>，则整个椭圆的横坐标会无限扩大，而纵坐标会收缩到0，此时原点既不是Sink也不是Source，人们把它叫做Saddle(鞍点)。</p><center><img src="https://vivounicorn.github.io/images/ai_chapter_6/image_1edj5o3bpprf1te3jlc6c5qmb2v.png" width="600"></center><p>假设取：<span class="math inline">\(a=2,b=0.5\)</span>，则： <span class="math display">\[ A= \left[ \begin{matrix} 2 &amp; 0 \\ 0 &amp; 0.5 \end{matrix} \right] \]</span> 经过<span class="math inline">\(n\)</span>次迭代后，得到下图：</p><center><img src="https://vivounicorn.github.io/images/ai_chapter_6/image_1edj6hipk4o5138gi491htcbi93c.png" width="300"></center><center><img src="https://vivounicorn.github.io/images/ai_chapter_6/image_1edj6hu8vaf019av52n1d70h263p.png" width="300"></center><p>3、<strong>双曲(hyperbolic)</strong></p><p>假设A是实数域矩阵，基于它定义了<span class="math inline">\(\mathbb{R}^m\to \mathbb{R}^m\)</span>的线性映射<span class="math inline">\(A(v)\)</span>，则：</p><ul><li><p>如果<span class="math inline">\(A\)</span>的所有特征值的绝对值都小于1，则原点是一个吸引固点Sink;</p></li><li><p>如果<span class="math inline">\(A\)</span>的所有特征值的绝对值都大于1，则原点是一个排斥固点Source;</p></li><li><p>如果<span class="math inline">\(A\)</span>的所有特征值中至少有一个其绝对值大于1，且最少有一个其绝对值小于1，则原点是一个鞍点Saddle。</p></li></ul><p>如果一个映射<span class="math inline">\(A\)</span>，没有一个特征值的绝对值等于1，则我们把<span class="math inline">\(A\)</span>叫做是双曲的，显然有三类双曲映射：Sink、Source、Saddle。</p><h3 id="非线性映射">6.2.4 非线性映射</h3><p>真实世界中，非线性系统远远多于线性系统，而当非线性程度足够高时，系统将出现混沌状态，不过从概念和定义上与线性映射区别不大。前面说的吸引固点和k周期吸引固点都是运动状态可预测的，它们被叫做平庸吸引子，而运动状态不可预测的叫做奇异吸引子(Strange Attractor)。</p><p>同样利用泰勒展开式，在非线性高维空间，导数被扩展为雅克比矩阵(Jacobian matrix)：</p><p><span class="math display">\[f(p+h)\approx f(p)+Df(p)\cdot h\]</span> 其中：</p><p>1、<span class="math inline">\(f=(f_1,f_2,......f_m)\)</span>是<span class="math inline">\(\mathbb{R}^m\)</span>上的映射，<span class="math inline">\(p\in \mathbb{R}^m\)</span></p><p>2、雅可比矩阵<span class="math inline">\(Df(p)\)</span>为： <span class="math display">\[Df(p)= \left[ \begin{matrix} \frac{\partial f_1}{\partial x_1}(p) &amp; ...... &amp; \frac{\partial f_1}{\partial x_m}(p) \\ \frac{\partial f_m}{\partial x_1}(p) &amp; ...... &amp; \frac{\partial f_ms}{\partial x_m}(p) \end{matrix} \right]\]</span></p><p>假设<span class="math inline">\(p\)</span>为固点，满足<span class="math inline">\(f(p)=p\)</span>，则： <span class="math display">\[f(p+h)\approx f(p)+Df(p)\cdot h=p+Df(p)\cdot h\]</span> 即，在<span class="math inline">\(p\)</span>点邻域内对其做一个微小扰动，输出会有<span class="math inline">\(Df(p)\cdot h\)</span>的变化，显然类似线性映射，可以有下面结论：</p><p>假设：<span class="math inline">\(f\)</span>是<span class="math inline">\(\mathbb{R}^m\)</span>上的映射，且<span class="math inline">\(p\in \mathbb{R}^m\)</span>，满足<span class="math inline">\(f(p)=p\)</span>，则：</p><p>1、如果<span class="math inline">\(Df(p)\)</span>没有取值为1的特征值，则<span class="math inline">\(p\)</span>被称作<strong>双曲(hyperbolic)</strong>的，这个词很重要，会在后面多次出现，直观的也挺好理解，1的多少次方都还是1，只有大于1或小于1才会在某个方向上要么吸引要么排斥；</p><p>2、如果<span class="math inline">\(Df(p)\)</span>的每个特征值的绝对值都小于1，那么<span class="math inline">\(p\)</span>是一个吸引固点Sink，也有人叫做<strong>双曲吸引子(hyperbolic attractor)</strong>；</p><p>3、如果<span class="math inline">\(Df(p)\)</span>的每个特征值的绝对值都大于1，那么<span class="math inline">\(p\)</span>是一个排斥固点Source；</p><p>4、如果<span class="math inline">\(m\geq 1\)</span>且<span class="math inline">\(p\)</span>是双曲的，<span class="math inline">\(Df(p)\)</span>至少有一个特征值的绝对值大于1且至少有一个特征值的绝对值小于1，则<span class="math inline">\(p\)</span>是一个鞍点Saddle。</p><p><strong>举个例子</strong>：</p><p>有非线性映射：</p><p><span class="math display">\[f_{a,b}(x,y)=(-x^2+0.4y,x)\]</span> 因为<span class="math inline">\(f(x,y)=(x,y)\)</span>，则有<span class="math inline">\(-x^2+0.4y=x\)</span>且<span class="math inline">\(x=y\)</span> 所以<span class="math inline">\(f\)</span>有两个固点：<span class="math inline">\((0,0),(-0.6,-0.6)\)</span></p><p>其雅克比矩阵为：</p><p><span class="math display">\[Df(x,y)= \left[ \begin{matrix} -2x &amp; 0.4 \\ 1 &amp; 0 \end{matrix} \right] \]</span></p><p>于是：</p><p><span class="math display">\[Df(0,0)= \left[ \begin{matrix} 0 &amp; 0.4 \\ 1 &amp; 0 \end{matrix} \right] \]</span></p><p>特征值为：<span class="math inline">\(\pm\sqrt{4}\approx\pm0.632\)</span></p><p><span class="math display">\[Df(-0.6,-0.6)= \left[ \begin{matrix} 1.2 &amp; 0.4 \\ 1 &amp; 0 \end{matrix} \right] \]</span></p><p>特征值为：<span class="math inline">\(1.472\)</span>和<span class="math inline">\(-0.272\)</span>，显然，<span class="math inline">\((0,0)\)</span>为双曲吸引子，<span class="math inline">\((-0.6,-0.6)\)</span>为鞍点。</p><h3 id="混沌的演化及结构">6.2.5 混沌的演化及结构</h3><p>用一个简单的抛物线做说明： <span class="math display">\[y=1-rx^2 :0\leq r\leq 1;-1\leq x_n\leq 1\]</span></p><p>将其转化为迭代形式（一般来说，越复杂的非线性方程越无解析解，常常用数值计算中的迭代方法得到解）：</p><p><span class="math display">\[x_{n+1}=1-rx_n^2\]</span></p><p>程序模拟迭代过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 抛物线函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parabola</span>(<span class="params">r, x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> - r * x**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_bifu</span>(<span class="params">iterations, r, x0, last</span>):</span></span><br><span class="line">    ax = plt.subplot(<span class="number">111</span>)</span><br><span class="line">    x = x0</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iterations):</span><br><span class="line">        x = parabola(r, x)</span><br><span class="line">        <span class="keyword">if</span> i &gt;= (iterations - last):</span><br><span class="line">            ax.plot(r, x, <span class="string">&#x27;,k&#x27;</span>, alpha=<span class="number">.25</span>)</span><br><span class="line"></span><br><span class="line">    ax.set_xlim(<span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">    ax.set_title(<span class="string">&quot;Bifurcation: y=1-rx^2&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    n = <span class="number">10000</span></span><br><span class="line">    r = np.linspace(<span class="number">0</span>, <span class="number">2.0</span>, n)</span><br><span class="line">    iterations = <span class="number">1000</span> <span class="comment"># 迭代次数</span></span><br><span class="line">    last = <span class="number">200</span> <span class="comment"># 输出最后若干次迭代</span></span><br><span class="line">    x0 = <span class="number">0.1</span> * np.ones(n) <span class="comment"># 初始点</span></span><br><span class="line">    plot_bifu(iterations, r, x0, last)</span><br></pre></td></tr></table></figure><p></p>其分叉图如下，结构上按照<span class="math inline">\(2^n\)</span>指数级周期性分裂，当<span class="math inline">\(r\approx 1.40\)</span>时，系统进入混沌状态：<center><img src="https://vivounicorn.github.io/images/ai_chapter_6/image_1eei0s9k6138i1ms215b1vut19lc9.png" width="600"></center>对下图红框部分放大看，可以发现一个有趣的东西:<center><img src="https://vivounicorn.github.io/images/ai_chapter_6/image_1eei1f9j81t94tbv1g3g1dn0jsv19.png" width="600"></center>放大的部分其结构与开始时的整体结构相同，一般叫做分形，于是在混沌中再次出现周期性：<center><img src="https://vivounicorn.github.io/images/ai_chapter_6/image_1eei1hdcleacos17k51jt31td51m.png" width="600"></center><p>随着复杂度的提升，系统经历了：稳定态-&gt;周期态-&gt;类周期态-&gt;混沌态。</p><p>还可以再次放大类似的红框区域，但会发现一个普适的规律：</p><p><span class="math display">\[\delta=\lim\limits_{n-&gt;\infty}\frac{r_n-r_{n-1}}{r_{n+1}-r_n}=4.669201609109...\]</span></p><p>其中<span class="math inline">\(r\)</span>是发生混沌现象时的分界点。上面这个常数叫做Feigenbaum常数，它可能是比圆周率更神秘的常数，我没有做更深入的了解，详情可参见论文《<a target="_blank" rel="noopener" href="https://link.springer.com/article/10.1007/BF01020332">Quantitative universality for a class of nonlinear transformations</a>》，换句话说，混沌演化的过程中存在内部规律性，且这种演化过程存在某种“<strong>普适性</strong>”。</p><h3 id="rnn长依赖学习问题">6.2.6 RNN长依赖学习问题</h3><p>这一节主要基于对Yoshua Bengio《<a target="_blank" rel="noopener" href="https://pdfs.semanticscholar.org/d0be/39ee052d246ae99c082a565aba25b811be2d.pdf?_ga=2.228408725.909305177.1596354587-48890951.1581915581">Learning Long-Term Dependencies with Gradient Descent is Difficult</a>》一文的学习，个人认为它是少有的对长依赖学习做出精彩理论研究和证明的文章。</p><p>文章从实验和理论角度证明了：梯度下降算法无法有效学习长依赖（模型在时间t的输出依赖更早时间<span class="math inline">\(t^{&#39;}\ll t\)</span>时的系统状态）。</p><p>一个能学习长依赖的动态系统，至少应该满足以下几个条件：</p><p>1、系统能够存储任意时长的信息； 2、系统鲁棒性强，即使对系统输入做随机波动也不影响系统做出正确输出； 3、系统参数可在合理有限的时间内学习到。</p><ul><li>单节点RNN实验</li></ul><p>设计一个满足以上条件的简单的序列二分类问题：</p>给定任意序列：<span class="math inline">\(u_1,...,u_T\)</span>，二分类器<span class="math inline">\(C(u_1,...,u_T)\in \{0,1\}\)</span>，且分类结果只与序列的前<span class="math inline">\(L\)</span>（<span class="math inline">\(L\ll T\)</span>）个输入有关，即： <span class="math display">\[C(u_1,...,u_T)=C(u_1,...,u_L)\]</span> 显然，<span class="math inline">\(L\)</span>之后的信息都是噪声（文中实验采用高斯噪声），如果系统不能有效存储任意时长的信息则无法做正确分类。换句话说，分类器内置了一个latching subsystem（暂且翻译为锁存子系统），这个子系统可以提取分类的关键信息，并存储于子系统的状态变量中。<center><img src="https://vivounicorn.github.io/images/ai_chapter_6/image_1een7h4n513ku1b6uq8e9sjk0h9.png" width="600"></center><p>当<span class="math inline">\(t\leq L\)</span>时，<span class="math inline">\(h_t\)</span>为可学习调整的参数，当<span class="math inline">\(L&lt;t\leq T\)</span>时，<span class="math inline">\(h_t\)</span>为高斯噪声，损失函数为：</p><p><span class="math display">\[C=\frac{1}{2}\sum_{p}(x_T^p-d^p)^2\]</span> 其中<span class="math inline">\(p\)</span>是训练序列的索引，<span class="math inline">\(d^p\)</span>是目标输出，取值0.8代表分类1，取值-0.8代表分类0，$h_t (t = 1,. . . , L) <span class="math inline">\(代表抽取了分类关键信息后的计算结果，显然，直接学习\)</span>h_t$要比用原始输入学习 <span class="math inline">\(h_t(u_t,\theta)\)</span>来的容易，而且不管以上哪种方式，传播误差梯度<span class="math inline">\(\frac{\partial C}{\partial h_t}\)</span>的方法一样，如果因为梯度消失导致<span class="math inline">\(h_t\)</span>都学不出来，更别说<span class="math inline">\(h_t(u_t,\theta)\)</span>了。</p><p>以最简单的RNN为例：</p><center><img src="https://vivounicorn.github.io/images/ai_chapter_6/image_1eend2gj51ahg1c30e0id2113u9m.png" width="100"></center><p><span class="math display">\[ \begin{align*} &amp; y_t^k=f(x_t^k)=tanh(x_t^k) \\ &amp; x_t^k=wf(x_{t-1}^k)+h_t^k \quad t=1...T\\ &amp; x_0^0=x_0^1=1\\ &amp; k \in\{0,1\}\\ \end{align*} \]</span></p>如果<span class="math inline">\(w&gt;1\)</span>，则以上动态系统有两个双曲吸引子，即下图的<span class="math inline">\((\pm x^*,\pm y^*)\)</span>：<center><img src="https://vivounicorn.github.io/images/ai_chapter_6/image_1eengrro49501c9ur94d581t541g.png" width="600"></center><p>依据《<a target="_blank" rel="noopener" href="http://ai.dinfo.unifi.it/paolo/ps/tkde93.pdf">Unified integration of explicit rules and learning by example in recurrent networks</a>》的证明，假设初始状态是<span class="math inline">\((-x^*,-y^*)\)</span>，则存在<span class="math inline">\(h^*\)</span>使得：</p><p>1、如果<span class="math inline">\(|h_t|&lt;h^*\)</span>，<span class="math inline">\(\forall t\)</span>，<span class="math inline">\(y_t\)</span>的符号可以保持不变，即在负向<span class="math inline">\((-x^*,-y^*)\)</span>吸引子邻域的点会被吸引；</p><p>2、存在有限步数<span class="math inline">\(L_1\)</span>，如果<span class="math inline">\(h_t&gt;h^*\)</span>，<span class="math inline">\(\forall t \leq L_1\)</span>，使得<span class="math inline">\(y_{L1}&gt;y^*\)</span>，即超过<span class="math inline">\((-x^*,-y^*)\)</span>吸引子邻域的点会被吸引到正向吸引子<span class="math inline">\((x^*,y^*)\)</span>。</p><p>当<span class="math inline">\(w\)</span>取固定值时，<span class="math inline">\(L_1\)</span> 随着 <span class="math inline">\(|h_t|\)</span> 增加而减小。</p><p>总结如下：</p><p>1、上述简单的系统可以锁存1 bit信息（即输出的符号变化与否）；</p><p>2、系统通过对一个大的输入保持足够长的时间来存储信息（<span class="math inline">\(|h_t|&gt;h^*\)</span>）；</p><p>3、对输入做微小的噪声扰动，即使时间很长也不会改变激活函数输出的符号；</p><p>4、<span class="math inline">\(w\)</span>也是可学习的，当<span class="math inline">\(T\gg L\)</span>时，要求<span class="math inline">\(w&gt;1\)</span>，此时会生成正负向两个吸引子，且<span class="math inline">\(w\)</span>越大，相应的阈值<span class="math inline">\(h^*\)</span>越大，因此系统鲁棒性越强。</p><center><img src="https://vivounicorn.github.io/images/ai_chapter_6/image_1eeniuive1ehfgojdr31s7t4302d.png" width="450"></center>上图加粗的点展示了系统成功学出来的3个<span class="math inline">\(h_t\)</span>（<span class="math inline">\(h_1,...,h_L\)</span>）。 5、实验结果： 1)、下图a，取<span class="math inline">\(L=3\)</span>，<span class="math inline">\(T=20\)</span>做实验，发现随着高斯噪声的标准差增大，<span class="math inline">\(w_0\)</span>变小，系统收敛性越来越差。 2)、下图b，取高斯噪声<span class="math inline">\(s=0.2\)</span>，<span class="math inline">\(w_0=1.25\)</span>做实验，发现随着<span class="math inline">\(T\)</span> 的增加，系统收敛性越来越差，即在这么简单的系统中，梯度下降算法想长时间稳定的存储1 bit信息都很困难。<center><img src="https://vivounicorn.github.io/images/ai_chapter_6/image_1eenjfahi1kov1bf4igd13rd10pk37.png" width="600"></center><ul><li>混沌理论角度的解释</li></ul><center><img src="https://vivounicorn.github.io/images/ai_chapter_6/image_1ef3r2gohi481di0af21v9vn859.png" width="450"></center><p>回忆前几节关于映射及吸引子、双曲吸引子的说明，围绕着吸引子(attractor)有几个相关定义：</p><p>1、basin of attractor：其实就是吸引子的邻域：</p><p><span class="math display">\[\beta(X)=\{p\in \mathbb{R}^m :\forall \epsilon,\exists x \in X s.t.||f(p)-x||&lt;\epsilon \}\]</span></p><p>2、reduced attractor set：其实就是被<strong>双曲吸引子</strong>强吸引的点集合：</p><p><span class="math display">\[\Gamma(X)=\{p\in \mathbb{R}^m :f&#39;(p)的所有特征值都小于1\}\]</span> 直观展示它们的关系如上图,显然：</p><p><span class="math display">\[X \subset \Gamma(X) \subset \beta(X) \]</span> 如果任意时刻对一个锁存子系统的输入做微小扰动后都落在该系统双曲吸引子的reduced attractor set中，即图中<span class="math inline">\(\Gamma(X)\)</span>，则该锁存系统具有鲁棒性。</p><p>3、对于双曲吸引子<span class="math inline">\(X\)</span>邻域内的点<span class="math inline">\(a\)</span>，如果在<span class="math inline">\(\beta(X)\)</span>但不在<span class="math inline">\(\Gamma(X)\)</span>中，则不确定性会随着<span class="math inline">\(t\)</span>的增加而呈指数增加，最终微小的扰动会让<span class="math inline">\(a\)</span>远离<span class="math inline">\(x\)</span>而进入其他吸引子的邻域，如下图：</p><center><img src="https://vivounicorn.github.io/images/ai_chapter_6/image_1ef93p7r11rsmv6aop414ud1aqs19.png" width="500"></center><p>证明：</p><p>假设<span class="math inline">\(\exists u\)</span>，满足<span class="math inline">\(\left\|u\right\|=1\)</span>及<span class="math inline">\(\left\|f&#39;(x)u\right\|&gt;1\)</span>，则根据泰勒展开式，对一个很小的值<span class="math inline">\(\lambda\)</span>有：</p><p><span class="math display">\[f(x+\lambda u)=f(x)+f&#39;(x)\lambda u+O(\left\|\lambda u\right\|^2)\]</span></p><p>对一个开放集合<span class="math inline">\(U(x)\)</span>，存在一个很小的值<span class="math inline">\(\lambda\)</span>，使得<span class="math inline">\(x+\lambda u \in U(x)\)</span>，且<span class="math inline">\(O(\left\|\lambda u\right\|^2)&lt;\lambda\left\|f&#39;(x)u\right\|-\lambda\)</span>。另<span class="math inline">\(y=x+\lambda u\)</span>，则根据三角不等式有：</p><p><span class="math display">\[\left\|-f&#39;(x)\lambda u\right\|-\left\|f(y)-f(x)\right\|&lt;\left\|-f&#39;(x)\lambda u+f(y)-f(x)\right\|\]</span></p><p>而：</p><p><span class="math display">\[\left\|-f&#39;(x)\lambda u+f(y)-f(x)\right\|=\left\|O(\left\|\lambda u\right\|^2)\right\|&lt;\lambda\left\|f&#39;(x)u\right\|-\lambda\]</span></p><p>所以有：</p><p><span class="math display">\[\lambda \left\|f&#39;(x)u\right\|-\left\|f(y)-f(x)\right\|&lt;\lambda\left\|f&#39;(x)u\right\|-\lambda\]</span></p><p>从而得到：</p><p><span class="math display">\[\left\|f(y)-f(x)\right\|&gt;\lambda=\left\|y-x\right\|\]</span></p><p>即：对<span class="math inline">\(x\)</span>的微小扰动会使得<span class="math inline">\(f(x)\)</span>的变化“幅度”增大。</p><p>4、一个具有鲁棒性的锁存子系统特点是：即使对系统输入有微小扰动，只要每次迭代时<span class="math inline">\(a_t\)</span>都在双曲吸引子<span class="math inline">\(X\)</span>的<span class="math inline">\(\Gamma(X)\)</span>内，则最终会被吸引收敛到双曲吸引子<span class="math inline">\(X\)</span>附近。如下图：</p><center><img src="https://vivounicorn.github.io/images/ai_chapter_6/image_1ef97as8ol8kr301q571g01l012c.png" width="500"></center><p>5、动态系统要想做到鲁棒性的锁存信息，则会出现梯度消失现象(gradient vanishing)，鱼与熊掌不可兼得。 一个通用的动态系统可以表示为：</p><p><span class="math display">\[a_{t}=f(a_{t-1})+x_t,\{a,x\}\in \mathbb{R}^m\]</span></p><p>其中<span class="math inline">\(a_t\)</span>和<span class="math inline">\(x_t\)</span>分别是<span class="math inline">\(t\)</span>时刻系统状态向量和<span class="math inline">\(t\)</span>时刻外部输入向量，根据导数的定义和双曲吸引子的性质有：</p><p><span class="math display">\[|\frac{\partial a_t}{\partial_{a_{t-1}}}|=|f&#39;(a_{t-1})|&lt;1\]</span></p><p>显然，当<span class="math inline">\(t\rightarrow \infty\)</span>时，<span class="math inline">\(\frac{\partial a_t}{\partial_{a_{0}}}\rightarrow 0\)</span>，梯度消失！</p><h2 id="lstm">6.3 LSTM</h2><p>上面两节从原理角度说明了RNN为什么很难学到长依赖，而本节的LSTM是一个伟大的和具有里程碑的模型，最著名的论文是Sepp Hochreiter 与 Jurgen Schmidhuber的《<a target="_blank" rel="noopener" href="https://www.bioinf.jku.at/publications/older/2604.pdf">Long Short-Term Memory</a> 》（没错，就是那位怼天怼地怼各种权威的Schmidhuber），从原理上分析解决了RNN学习长依赖中的梯度爆炸(blow up)和梯度消失(vanish)问题，大部分文章只介绍了LSTM的结构，我希望通过本文能抛砖引玉，了解作者为什么这么设计结构。</p><h3 id="基本原理-1">6.3.1 基本原理</h3><p>回忆6.1节末的RNN任意两层隐藏层</p><p><span class="math display">\[ \delta_{pj}(t-1)=\sum_h^m\delta_{ph}(t)u_{hj}f^{&#39;}(s_{pj}(t-1)) \]</span></p><p>其中：<span class="math inline">\(h\)</span>是在<span class="math inline">\(t\)</span>时刻的任何一个隐层节点，<span class="math inline">\(j\)</span>是在<span class="math inline">\(t-1\)</span>时刻的任何一个隐层节点，高层的<span class="math inline">\(\delta\)</span>可以通过循环递归的计算出来，所有<span class="math inline">\(\delta\)</span>计算完毕后累加求和并应用在<span class="math inline">\(U\)</span>、<span class="math inline">\(V\)</span>的权重更新中。</p><p>误差从<span class="math inline">\(t\)</span>时刻的隐藏层<span class="math inline">\(h\)</span>节点经过任意<span class="math inline">\(q\)</span>步往<span class="math inline">\(t-q\)</span>时刻的隐藏层<span class="math inline">\(j\)</span>节点做反向传播的传播速度如下：</p><p><span class="math display">\[ \frac{\partial{\delta_{pj}(t-q)}}{\partial{\delta_{ph}(t)}}=\left\{ \begin{array}{**lr**} f^{&#39;}(s_{pj}(t-1))u_{hj} &amp; q=1 \\ f^{&#39;}(s_{pj}(t-1)) \sum_{i=1}^m \frac{\partial{\delta_{pi}(t-q+1)}}{\partial{\delta_{ph}(t)}}u_{hj} &amp; q&gt;1 \end{array} \right. \]</span></p><p>把上面式子完全展开后得到：</p><p><span class="math display">\[ \frac{\partial{\delta_{pj}(t-q)}}{\partial{\delta_{ph}(t)}}=\sum_{i_1=1}^m...\sum_{i_{q-1}=1}^m\prod_{m=1}^qf^{&#39;}(s_{i_m}(t-m))u_{i_mi_{m-1}} \]</span></p><p>大家会发现整个误差反向传播速度是由$<em>{m=1}<sup>qf</sup>{'}(s</em>{i_m}(t-m))u_{i_mi_{m-1}} $决定的：</p><p>1、如果<span class="math inline">\(|f^{&#39;}(s_{i_m}(t-m))u_{i_mi_{m-1}}|&gt;1\)</span>，则连乘的结果会随着<span class="math inline">\(q\)</span>的增加呈指数形式增大，误差反向传播出现梯度爆炸；</p><p>2、如果<span class="math inline">\(|f^{&#39;}(s_{i_m}(t-m))u_{i_mi_{m-1}}|&lt;1\)</span>，则连乘的结果会随着<span class="math inline">\(q\)</span>的增加呈指数形式减小，误差反向传播出现梯度消失。</p>假设以最简单的RNN为例，即：<center><img src="https://vivounicorn.github.io/images/ai_chapter_6/image_1eend2gj51ahg1c30e0id2113u9m.png" width="100"></center><p>在<span class="math inline">\(t\)</span>时刻的反向误差传播为：</p><p><span class="math display">\[\delta_{h}(t)=\delta_{h}(t+1)u_{hh}f^{&#39;}(s_{h}(t))\]</span> 其中：<span class="math inline">\(s_{h}(t)=f(net_h(t))\)</span>。要想不出现梯度爆炸或消失，只能满足：</p><p><span class="math display">\[u_{hh}f^{&#39;}(s_{h}(t))=1\]</span></p><p>对上式积分下，得到：</p><p><span class="math display">\[f(s_{h}(t))=\frac{s_h(t)}{u_{hh}}\]</span></p><p>这意味着，函数<span class="math inline">\(f\)</span>必须是线性的，显然，当<span class="math inline">\(u_{hh}=1\)</span>时，有恒等映射函数<span class="math inline">\(f(x)=x\)</span>，上述关系也叫constant error carrousel(CEC)，CEC在LSTM的结构设计中举足轻重。</p>以输入权重为例，由于实际场景中除了自连接节点外，还会有其他输入节点，为简单起见，我们只关注一个额外的输入权重<span class="math inline">\(w_{ji}\)</span> 。假设通过响应某个输入而开启神经网络单元<span class="math inline">\(j\)</span>，并为了减少总误差，希望它能被长时间激活。显然，对同一个输入权重一方面要存储某些输入范式，一方面又要忽略其他输入范式，而涉及<span class="math inline">\(j\)</span>节点的函数(上面的CEC)又是线性的，所以对<span class="math inline">\(w_{ji}\)</span>而言，这些信号会试图让它既得通过开启<span class="math inline">\(j\)</span>单元对输入做存储又需要防止<span class="math inline">\(j\)</span>单元被其他输入关闭，这种情况使得学习变得困难。<center><img src="https://vivounicorn.github.io/images/ai_chapter_6/lstm-focus%5B0%5D.png" width="450"></center><p>对于输出权重，也存在类似的输出冲突，这里就不在赘述。 为了解决上面的输入和输出冲突，LSTM抽象了1个记忆单元（Memory Cel l）、设计了1个基础结构——遗忘门（Forget Gate）和2个组合结构——输入门（Input Gate）和输出门（Output Gate）来解决冲突。</p><p>0、记忆单元是对包含CEC线性单元的抽象，如下图（以RNN作为对比），包含当前时刻输入、上个隐层节点的状态、当前时刻输出、当前时刻隐层节点状态。：</p><center>RNN <img src="https://vivounicorn.github.io/images/ai_chapter_6/LSTM3-SimpleRNN%5B0%5D..png" width="500"></center><center>LSTM <img src="https://vivounicorn.github.io/images/ai_chapter_6/LSTM3-chain%5B0%5D..png" width="500"></center>1、遗忘门的结构如下图：它将上一个隐层的状态<span class="math inline">\(h_{t-1}\)</span>和当前输入<span class="math inline">\(x_t\)</span>合并后送入logistic函数<span class="math inline">\(f(x)=\frac{1}{1+e^{-w^Tx}}\)</span>，由于该函数输出为0~1之间，输出接近1的被保留，接近0的被丢掉，也就是说，<strong>遗忘门决定了哪些历史信息要被保留</strong>。<center>Forget Gate <img src="https://vivounicorn.github.io/images/ai_chapter_6/forget-gate.png" width="450"></center><p>2、输入门的结构如下图：它将上一个隐层的状态<span class="math inline">\(h_{t-1}\)</span>和当前输入<span class="math inline">\(x_t\)</span>合并后送入Logistic函数，输出介于0～1之间，同样的，0表示信息不重要，1表示信息重要；同时，<span class="math inline">\(h_{t-1}\)</span>和<span class="math inline">\(x_t\)</span>合并后的输入被送入Tanh函数，输出介于-1～1之间，Logistic的输出与Tanh的输出相乘后决定哪些Tanh的输出信息需要保留，哪些要丢掉，也就是说，<strong>输入门决定了哪些新的信息要被加进来</strong>。</p><center>Input Gate <img src="https://vivounicorn.github.io/images/ai_chapter_6/input-gate.png" width="450"></center>前一个记忆单元的输出与遗忘门输出相乘后，可以选择性忘记不重要的信息，之后与输入门的结果相加，把新的输入信息纳入进来，最终得到当前记忆单元的输出，比较好解决了输入冲突，如下图：<center>Cell Output <img src="https://vivounicorn.github.io/images/ai_chapter_6/output.png" width="450"></center><p>3、输出门的结构如下图：它主要解决<strong>隐藏层</strong>状态的输出冲突问题，它将上一个隐层的状态<span class="math inline">\(h_{t-1}\)</span>和当前输入<span class="math inline">\(x_t\)</span>合并后送入Logistic函数，输出介于0～1之间，然后与当前记忆单元的输出通过Tanh函数变换后的结果相乘，得到当前隐藏层的状态，也就是说，<strong>输出门决定了当前隐藏层要携带哪些历史信息</strong>，比较好解决了输出冲突。</p><center>Output Gate <img src="https://vivounicorn.github.io/images/ai_chapter_6/output-gate.png" width="450"></center><p>以上图片来源于：《<a target="_blank" rel="noopener" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a>》一文，非常不错的一篇LSTM入门文章。后续也有各种各样对经典LSTM的改进（如GRU），但整体上不如LSTM经典（截止2020.10.10在Google Scholar上查寻到该论文已经被引用了37851次，成为20世纪“最火论文”）。</p><p>除了比较完美解决了输入输出冲突外，LSTM的计算和存储复杂度并不高，权重更新计算的复杂度为O(W)，即与权重总个数线性相关；存储方面也不像使用全流程BPTT的传统方法，要存储大量历史节点信息，LSTM只需要存储一定历史时间步的局部信息。</p><h3 id="代码实践-1">6.3.2 代码实践</h3><p>本节以经典的《古诗词生成》为例子，介绍下LSTM的一种应用，以下例子只供娱乐使用。</p><p>问题描述如下：</p><p>“给定五言绝句的首句，生成整首共4句的五言绝句。”</p><p>例如，输入：“月暗竹亭幽，”，输出“月暗竹亭幽，碧昏时尽黄。园春歌雪光，云落分白草。”。</p><p><strong>完整代码在：https://github.com/vivounicorn/LstmApp.git</strong>，其中，data文件夹里包含了训练好的word2vec模型和迭代了2k+次的模型，可以直接做fine-tune。</p><p><strong>1、算法步骤</strong></p><p>Step-1：爬取古诗词作为原始数据；</p><p>Step-2：清洗原始数据，去掉不符合五言绝句的诗词；</p><p>Step-3：准备训练数据和相应的标注；</p><p>Step-4：若使用word2vec生成的词向量，则需要生成相关模型；</p><p>Step-5：构建以LSTM层和全连接层为主的神经网络；</p><p>Step-6：训练和验证模型，并做应用。</p><p><strong>2、实现详情</strong></p><ul><li><p><strong>Step-1，爬取古诗词作为原始数据</strong></p><p>用开源工具爬取：https://www.gushiwen.org/上的诗句。解析结果的基本格式为：“诗词标题：诗词内容”。</p></li><li><strong>Step-2，数据清洗</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_build_base</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                file_path,</span></span></span><br><span class="line"><span class="params"><span class="function">                vocab=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                word2idx=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                idx2word=<span class="literal">None</span></span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    To scan the file and build vocabulary and so on.</span></span><br><span class="line"><span class="string">    :param file_path: the file path of poetic corpus, one poem per line.</span></span><br><span class="line"><span class="string">    :param vocab: the vocabulary.</span></span><br><span class="line"><span class="string">    :param word2idx: the mapping of word to index.</span></span><br><span class="line"><span class="string">    :param idx2word: the mapping of index to word</span></span><br><span class="line"><span class="string">    :return: None.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 去掉无关字符</span></span><br><span class="line">    pattern = re.<span class="built_in">compile</span>(<span class="string">u&quot;_|\(|（|《&quot;</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&#x27;UTF-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                line = line.strip(<span class="string">u&#x27;\n&#x27;</span>)</span><br><span class="line">                title, content = line.strip(SPACE).split(<span class="string">u&#x27;:&#x27;</span>)</span><br><span class="line">                content = content.replace(SPACE, <span class="string">u&#x27;&#x27;</span>)</span><br><span class="line">                idx = re.search(pattern, content)</span><br><span class="line">                <span class="keyword">if</span> idx <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    content = content[:idx.span()[<span class="number">0</span>]]</span><br><span class="line">                <span class="comment"># 把指定长度的诗词选出来，如：五言绝句。</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(content) &lt; self.embedding_input_length:  <span class="comment"># Filter data according to embedding input</span></span><br><span class="line">                    <span class="comment"># length to improve accuracy.</span></span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                words = []</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(content)):</span><br><span class="line">                    word = content[i:i + <span class="number">1</span>]</span><br><span class="line">                    <span class="keyword">if</span> (i + <span class="number">1</span>) % self.embedding_input_length == <span class="number">0</span> <span class="keyword">and</span> word <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;,&#x27;</span>, <span class="string">&#x27;,&#x27;</span>, <span class="string">&#x27;，&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;。&#x27;</span>]:</span><br><span class="line">                        words = []</span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">                    words.append(word)</span><br><span class="line">                    self.all_words.append(word)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(words) &gt; <span class="number">0</span>:</span><br><span class="line">                    self.poetrys.append(words)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                log.error(<span class="built_in">str</span>(e))</span><br><span class="line">    <span class="comment"># 生成词汇表，保留出现频次top n的字</span></span><br><span class="line">    <span class="keyword">if</span> vocab <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        top_n = Counter(self.all_words).most_common(self.vocab_size - <span class="number">1</span>)</span><br><span class="line">        top_n.append(SPACE)</span><br><span class="line">        self.vocab = <span class="built_in">sorted</span>(<span class="built_in">set</span>([i[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> top_n]))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        top_n = <span class="built_in">list</span>(vocab)[:self.vocab_size - <span class="number">1</span>]</span><br><span class="line">        top_n.append(SPACE)</span><br><span class="line">        self.vocab = <span class="built_in">sorted</span>(<span class="built_in">set</span>([i <span class="keyword">for</span> i <span class="keyword">in</span> top_n]))  <span class="comment"># cut vocab with threshold.</span></span><br><span class="line"></span><br><span class="line">    log.debug(self.vocab)</span><br><span class="line">    <span class="comment"># 生成“字”到“编号”的映射，把每个字做了唯一编号，“空格”也做编号</span></span><br><span class="line">    <span class="keyword">if</span> word2idx <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        self.word2idx = <span class="built_in">dict</span>((c, i) <span class="keyword">for</span> i, c <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.vocab))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.word2idx = word2idx</span><br><span class="line">    <span class="comment"># 生成“编号”到“字”的映射</span></span><br><span class="line">    <span class="keyword">if</span> idx2word <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        self.idx2word = <span class="built_in">dict</span>((i, c) <span class="keyword">for</span> i, c <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.vocab))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.idx2word = idx2word</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Function of mapping word to index.</span></span><br><span class="line">    <span class="comment"># 以 “字” 查找 “编号”的函数，没在词汇表的“字”用“空格”的编号代替</span></span><br><span class="line">    self.w2i = <span class="keyword">lambda</span> word: self.word2idx.get(<span class="built_in">str</span>(word)) <span class="keyword">if</span> self.word2idx.get(word) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> \</span><br><span class="line">        <span class="keyword">else</span> self.word2idx.get(SPACE)</span><br><span class="line">    <span class="comment"># Function of mapping index to word.</span></span><br><span class="line">    <span class="comment"># 以 “编号”查找 “字” 的函数，找不到的“字”用“空格”代替</span></span><br><span class="line">    self.i2w = <span class="keyword">lambda</span> idx: self.idx2word.get(<span class="built_in">int</span>(idx)) <span class="keyword">if</span> self.idx2word.get(<span class="built_in">int</span>(idx)) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> \</span><br><span class="line">        <span class="keyword">else</span> SPACE</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Full vectors.</span></span><br><span class="line">    <span class="comment"># 把文本表示的诗词变成由“编号”表示的向量，如：“床前明月光，”变成[1,2,3,4,5,6]</span></span><br><span class="line">    self.poetrys_vector = [<span class="built_in">list</span>(<span class="built_in">map</span>(self.w2i, poetry)) <span class="keyword">for</span> poetry <span class="keyword">in</span> self.poetrys]</span><br><span class="line">    self._data_size = <span class="built_in">len</span>(self.poetrys_vector)</span><br><span class="line">    self._data_index = np.arange(self._data_size)</span><br></pre></td></tr></table></figure></li><li><p><strong>Step-3：准备训练数据</strong></p><p>原理是：根据指定的输入长度(input length)截取序列并生成特征数据，指定这个序列的下一个字为“标注”。</p><p>例如：“菩提本无树，明镜亦非台。”，以五言绝句为例，输入长度为6（包括标点符号），可以生成以下样本：</p><table><thead><tr class="header"><th style="text-align:center">特征</th><th style="text-align:center">标注</th></tr></thead><tbody><tr class="odd"><td style="text-align:center">菩提本无树，</td><td style="text-align:center">明</td></tr><tr class="even"><td style="text-align:center">提本无树，明</td><td style="text-align:center">镜</td></tr><tr class="odd"><td style="text-align:center">本无树，明镜</td><td style="text-align:center">亦</td></tr><tr class="even"><td style="text-align:center">无树，明镜亦</td><td style="text-align:center">非</td></tr><tr class="odd"><td style="text-align:center">树，明镜亦非</td><td style="text-align:center">台</td></tr><tr class="even"><td style="text-align:center">，明镜亦非台</td><td style="text-align:center">。</td></tr></tbody></table><p>对每个字，支持两种编码方式：基于词汇表的one hot和基于语义distributed representation的word2vec。</p><p><strong>1、One-hot</strong></p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_one_hot_encoding</span>(<span class="params">self, sample</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    One-hot encoding for a sample, a sample will be split into multiple samples.</span></span><br><span class="line"><span class="string">    :param sample: a sample. [1257, 6219, 3946]</span></span><br><span class="line"><span class="string">    :return: feature and label. feature:[[0,0,0,1,0,0,......],</span></span><br><span class="line"><span class="string">                                        [0,0,0,0,0,1,......],</span></span><br><span class="line"><span class="string">                                        [1,0,0,0,0,0,......]];</span></span><br><span class="line"><span class="string">                                label:  [0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0......]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(sample) != <span class="built_in">list</span> <span class="keyword">or</span> <span class="number">0</span> == <span class="built_in">len</span>(sample):</span><br><span class="line">        log.error(<span class="string">&quot;type or length of sample is invalid.&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    feature_samples = []</span><br><span class="line">    label_samples = []</span><br><span class="line">    idx = <span class="number">0</span></span><br><span class="line">    <span class="comment"># embedding_input_length即为输入窗口长度，五言绝句为6，当然也可以取其他值，但会影响训练精度和时间。</span></span><br><span class="line">    <span class="keyword">while</span> idx &lt; <span class="built_in">len</span>(sample) - self.embedding_input_length:</span><br><span class="line">        feature = sample[idx: idx + self.embedding_input_length]</span><br><span class="line">        label = sample[idx + self.embedding_input_length]</span><br><span class="line"></span><br><span class="line">        label_vector = np.zeros(</span><br><span class="line">            shape=(<span class="number">1</span>, self.vocab_size),</span><br><span class="line">            dtype=np.<span class="built_in">float</span></span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 序列的下一个字为标注</span></span><br><span class="line">        label_vector[<span class="number">0</span>, label] = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">        feature_vector = np.zeros(</span><br><span class="line">            shape=(<span class="number">1</span>, self.embedding_input_length, self.vocab_size),</span><br><span class="line">            dtype=np.<span class="built_in">float</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 根据词汇表，相应的编号赋值为1，其余都是0.</span></span><br><span class="line">        <span class="keyword">for</span> i, f <span class="keyword">in</span> <span class="built_in">enumerate</span>(feature):</span><br><span class="line">            feature_vector[<span class="number">0</span>, i, f] = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">        idx += <span class="number">1</span></span><br><span class="line">        feature_samples.append(feature_vector)</span><br><span class="line">        label_samples.append(label_vector)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> feature_samples, label_samples</span><br></pre></td></tr></table></figure>假设输入长度为6，词汇表维度为8000，则，对于一个样本有：<p></p>特征矩阵为：1×6<em>8000 标注向量为：1</em>8000<center><img src="https://vivounicorn.github.io/images/ai_chapter_6/image_1emdpl8qlgg6q6l18gt1udq177i9.png" width="600"></center><p><strong>2、Word2vec</strong></p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_word2vec_encoding</span>(<span class="params">self, sample</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    word2vec encoding for sample, a sample will be split into multiple samples.</span></span><br><span class="line"><span class="string">    :param sample: a sample. [1257, 6219, 3946]</span></span><br><span class="line"><span class="string">    :return: feature and label.feature:[[0.01,0.23,0.05,0.1,0.33,0.25,......],</span></span><br><span class="line"><span class="string">                                        [0.23,0.45,0.66,0.32,0.11,1.03,......],</span></span><br><span class="line"><span class="string">                                        [1.22,0.99,0.68,0.7,0.8,0.001,......]];</span></span><br><span class="line"><span class="string">                                label:  [0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0......]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(sample) != <span class="built_in">list</span> <span class="keyword">or</span> <span class="number">0</span> == <span class="built_in">len</span>(sample):</span><br><span class="line">        log.error(<span class="string">&quot;type or length of sample is invalid.&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    feature_samples = []</span><br><span class="line">    label_samples = []</span><br><span class="line">    idx = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> idx &lt; <span class="built_in">len</span>(sample) - self.embedding_input_length:</span><br><span class="line">        feature = sample[idx: idx + self.embedding_input_length]</span><br><span class="line">        label = sample[idx + self.embedding_input_length]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.w2v_model <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            log.error(<span class="string">&quot;word2vec model is none.&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        label_vector = np.zeros(</span><br><span class="line">            shape=(<span class="number">1</span>, self.vocab_size),</span><br><span class="line">            dtype=np.<span class="built_in">float</span></span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 序列的下一个字为标注</span></span><br><span class="line">        label_vector[<span class="number">0</span>, label] = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">        feature_vector = np.zeros(</span><br><span class="line">            shape=(<span class="number">1</span>, self.embedding_input_length, self.w2v_model.size),</span><br><span class="line">            dtype=np.<span class="built_in">float</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 用训练好的word2vec模型获取相应“字”的语义向量</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.embedding_input_length):</span><br><span class="line">            feature_vector[<span class="number">0</span>, i] = self.w2v_model.get_vector(feature[i])</span><br><span class="line"></span><br><span class="line">        idx += <span class="number">1</span></span><br><span class="line">        feature_samples.append(feature_vector)</span><br><span class="line">        label_samples.append(label_vector)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> feature_samples, label_samples</span><br></pre></td></tr></table></figure>假设输入长度为6，词的语义向量维度为200，则，对于一个样本有：<p></p>特征矩阵为：1×6<em>200 标注向量为：1</em>8000<center><img src="https://vivounicorn.github.io/images/ai_chapter_6/image_1emdqc2sgvspaoa18uk1bqs1t87m.png" width="600"></center></li><li><p><strong>Step-4：基于word2vec训练词向量</strong></p><p>使用dump函数将训练数据相关数据结构dump下来，其中poetrys_words.dat文件可直接作为word2vec的训练数据（注：要训练的是<strong>字粒度</strong>的语义向量），文件内容类似这样，一行一首诗，字与字空格分割：</p><p>寒 随 穷 律 变 ， 春 逐 鸟 声 开 。 初 风 飘 带 柳 ， 晚 雪 间 花 梅 。 碧 林 青 旧 竹 ， 绿 沼 翠 新 苔 。 芝 田 初 雁 去 ， 绮 树 巧 莺 来 。 晚 霞 聊 自 怡 ， 初 晴 弥 可 喜 。 日 晃 百 花 色 ， 风 动 千 林 翠 。 池 鱼 跃 不 同 ， 园 鸟 声 还 异 。 寄 言 博 通 者 ， 知 予 物 外 志 。</p><p></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dump_data</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    To dump: poetry&#x27;s words list, poetry&#x27;s words vectors, poetry&#x27;s words vectors for training,</span></span><br><span class="line"><span class="string">             poetry&#x27;s words vectors for testing, poetry&#x27;s words vectors for validation,</span></span><br><span class="line"><span class="string">             poetry&#x27;s words vocabulary, poetry&#x27;s word to index mapping,poetry&#x27;s index to word mapping.</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    org_filename = self.dump_dir + <span class="string">&#x27;poetrys_words.dat&#x27;</span></span><br><span class="line">    self._dump_list(org_filename, self.poetrys)</span><br><span class="line"></span><br><span class="line">    vec_filename = self.dump_dir + <span class="string">&#x27;poetrys_words_vector.dat&#x27;</span></span><br><span class="line">    self._dump_list(vec_filename, self.poetrys_vector)</span><br><span class="line"></span><br><span class="line">    train_vec_filename = self.dump_dir + <span class="string">&#x27;poetrys_words_train_vector.dat&#x27;</span></span><br><span class="line">    self._dump_list(train_vec_filename, self.poetrys_vector_train)</span><br><span class="line"></span><br><span class="line">    valid_vec_filename = self.dump_dir + <span class="string">&#x27;poetrys_words_valid_vector.dat&#x27;</span></span><br><span class="line">    self._dump_list(valid_vec_filename, self.poetrys_vector_valid)</span><br><span class="line"></span><br><span class="line">    test_vec_filename = self.dump_dir + <span class="string">&#x27;poetrys_words_test_vector.dat&#x27;</span></span><br><span class="line">    self._dump_list(test_vec_filename, self.poetrys_vector_test)</span><br><span class="line"></span><br><span class="line">    vocab_filename = self.dump_dir + <span class="string">&#x27;poetrys_vocab.dat&#x27;</span></span><br><span class="line">    self._dump_list(vocab_filename, <span class="built_in">list</span>(self.vocab))</span><br><span class="line"></span><br><span class="line">    w2i_filename = self.dump_dir + <span class="string">&#x27;poetrys_word2index.dat&#x27;</span></span><br><span class="line">    self._dump_dict(w2i_filename, self.word2idx)</span><br><span class="line"></span><br><span class="line">    i2w_filename = self.dump_dir + <span class="string">&#x27;poetrys_index2word.dat&#x27;</span></span><br><span class="line">    self._dump_dict(i2w_filename, self.idx2word)</span><br></pre></td></tr></table></figure><p></p><p>模型方面直接使用gensim包，定义如下，根据参数不同，可以训练得到基于CBOW或SkipGram的语义向量，我们这种规模下，本质上没有太大差别，我们这里使用SkipGram。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> gensim</span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec</span><br><span class="line"><span class="keyword">from</span> gensim.models.word2vec <span class="keyword">import</span> LineSentence</span><br><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> src.config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> src.utils <span class="keyword">import</span> Logger</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Word2vecModel</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Word2vec model class.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                 cfg_path=<span class="string">&#x27;/home/zhanglei/Gitlab/LstmApp/config/cfg.ini&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 is_ns=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        To initialize model.</span></span><br><span class="line"><span class="string">        :param cfg_path: he path of configration file.</span></span><br><span class="line"><span class="string">        :param model_type:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        cfg = Config(cfg_path)</span><br><span class="line">        <span class="keyword">global</span> log</span><br><span class="line">        log = Logger(cfg.model_log_path())</span><br><span class="line">        self.model = <span class="literal">None</span></span><br><span class="line">        self.is_ns = is_ns</span><br><span class="line">        self.vec_out = cfg.vec_out()</span><br><span class="line">        self.corpus_file = cfg.corpus_file()</span><br><span class="line">        self.window = cfg.window()</span><br><span class="line">        self.size = cfg.size()</span><br><span class="line">        self.sg = cfg.sg()</span><br><span class="line">        self.hs = cfg.hs()</span><br><span class="line">        self.negative = cfg.negative()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_vec</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        To train a word2vec model.</span></span><br><span class="line"><span class="string">        :return: None</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        output_model = self.vec_out + <span class="string">&#x27;w2v_size&#123;0&#125;_sg&#123;1&#125;_hs&#123;2&#125;_ns&#123;3&#125;.model&#x27;</span>.<span class="built_in">format</span>(self.size,</span><br><span class="line">                                                                                   self.sg,</span><br><span class="line">                                                                                   self.hs,</span><br><span class="line">                                                                                   self.negative)</span><br><span class="line"></span><br><span class="line">        output_vector = self.vec_out + <span class="string">&#x27;w2v_size&#123;0&#125;_sg&#123;1&#125;_hs&#123;2&#125;_ns&#123;3&#125;.vector&#x27;</span>.<span class="built_in">format</span>(self.size,</span><br><span class="line">                                                                                     self.sg,</span><br><span class="line">                                                                                     self.hs,</span><br><span class="line">                                                                                     self.negative)</span><br><span class="line">        <span class="comment"># 是否做负采样</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.is_ns:</span><br><span class="line">            self.model = Word2Vec(LineSentence(self.corpus_file),</span><br><span class="line">                                  size=self.size,</span><br><span class="line">                                  window=self.window,</span><br><span class="line">                                  sg=self.sg,</span><br><span class="line">                                  hs=self.hs,</span><br><span class="line">                                  workers=multiprocessing.cpu_count())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.model = Word2Vec(LineSentence(self.corpus_file),</span><br><span class="line">                                  size=self.size,</span><br><span class="line">                                  window=self.window,</span><br><span class="line">                                  sg=self.sg,</span><br><span class="line">                                  hs=self.hs,</span><br><span class="line">                                  negative=self.negative,</span><br><span class="line">                                  workers=multiprocessing.cpu_count())</span><br><span class="line"></span><br><span class="line">        self.model.save(output_model)</span><br><span class="line">        self.model.wv.save_word2vec_format(output_vector, binary=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load</span>(<span class="params">self, path</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        To load a word2vec model.</span></span><br><span class="line"><span class="string">        :param path: the model file path.</span></span><br><span class="line"><span class="string">        :return: success True otherwise False.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            self.model = gensim.models.Word2Vec.load(path)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">most_similar</span>(<span class="params">self, word</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Return the most similar words.</span></span><br><span class="line"><span class="string">        :param word: a word.</span></span><br><span class="line"><span class="string">        :return: similar word list.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        word = self.model.most_similar(word)</span><br><span class="line">        <span class="keyword">for</span> text <span class="keyword">in</span> word:</span><br><span class="line">            log.info(<span class="string">&quot;word:&#123;0&#125; similar:&#123;1&#125;&quot;</span>.<span class="built_in">format</span>(text[<span class="number">0</span>], text[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> word</span><br><span class="line">    <span class="comment"># 获取某个字 的语义向量</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_vector</span>(<span class="params">self, word</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        To get a word&#x27;s vector.</span></span><br><span class="line"><span class="string">        :param word: a word.</span></span><br><span class="line"><span class="string">        :return: word&#x27;s word2vec vector.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">return</span> self.model.wv.get_vector(<span class="built_in">str</span>(word))</span><br><span class="line">        <span class="keyword">except</span> KeyError:</span><br><span class="line">            <span class="keyword">return</span> np.zeros(</span><br><span class="line">                shape=(self.size,),</span><br><span class="line">                dtype=np.<span class="built_in">float</span></span><br><span class="line">            )</span><br><span class="line">    <span class="comment"># 也可以直接把keras的embedding层给拿出来，</span></span><br><span class="line">    <span class="comment"># 为了直观，我这里没有直接用它，如果要用，记着把语义向量的权重冻结下。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_embedding_layer</span>(<span class="params">self, train_embeddings=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        To get keras embedding layer from model.</span></span><br><span class="line"><span class="string">        :param train_embeddings: if frozen the layer.</span></span><br><span class="line"><span class="string">        :return: embedding layer.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">return</span> self.model.wv.get_keras_embedding(train_embeddings)</span><br><span class="line">        <span class="keyword">except</span> KeyError:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p><strong>Step-5：构建LSTM模型</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_build</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">           lstm_layers_num,</span></span></span><br><span class="line"><span class="params"><span class="function">           dense_layers_num</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    To build a lstm model with lstm layers and densse layers.</span></span><br><span class="line"><span class="string">    :param lstm_layers_num: The number of lstm layers.</span></span><br><span class="line"><span class="string">    :param dense_layers_num:The number of dense layers.</span></span><br><span class="line"><span class="string">    :return: model.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    units = <span class="number">256</span></span><br><span class="line">    model = Sequential()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 样本特征向量的维度，onehot为词汇表大小，word2vec为语义向量维度</span></span><br><span class="line">    <span class="keyword">if</span> self.mode == WORD2VEC:</span><br><span class="line">        dim = self.data_sets.w2v_model.size</span><br><span class="line">    <span class="keyword">elif</span> self.mode == ONE_HOT:</span><br><span class="line">        dim = self.vocab_size</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;mode must be word2vec or one-hot.&quot;</span>)</span><br><span class="line">    <span class="comment"># embedding_input_length为输入序列窗口大小，如：五言绝句取为6</span></span><br><span class="line">    model.add(Input(shape=(self.embedding_input_length, dim)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 可以加多个LSTM层提取序列特征，这里会把之前每隔时刻的隐层都输出出来</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(lstm_layers_num - <span class="number">1</span>):</span><br><span class="line">        model.add(LSTM(units=units * (i + <span class="number">1</span>),</span><br><span class="line">                       return_sequences=<span class="literal">True</span>))</span><br><span class="line">        model.add(Dropout(<span class="number">0.6</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 注意这里我只要最后一个隐层的输出</span></span><br><span class="line">    model.add(LSTM(units=units * lstm_layers_num,</span><br><span class="line">                   return_sequences=<span class="literal">False</span>))</span><br><span class="line">    model.add(Dropout(<span class="number">0.6</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 可以加多个稠密层，用于对之前提取出来特征的组合</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(dense_layers_num - <span class="number">1</span>):</span><br><span class="line">        model.add(Dense(units=units * (i + <span class="number">1</span>)))</span><br><span class="line">        model.add(Dropout(<span class="number">0.6</span>))</span><br><span class="line">    <span class="comment"># 最后一层，用softmax做分类</span></span><br><span class="line">    model.add(Dense(units=self.vocab_size,</span><br><span class="line">                    activation=<span class="string">&#x27;softmax&#x27;</span>))</span><br><span class="line">    <span class="comment"># 使用交叉熵损失函数，优化器选择默认参数的adam（ps：随便选的，没做调参）</span></span><br><span class="line">    model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>, optimizer=<span class="string">&#x27;adam&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    model.summary()</span><br><span class="line">    <span class="comment"># 可视化输出模型结构</span></span><br><span class="line">    plot_model(model, to_file=<span class="string">&#x27;../model.png&#x27;</span>, show_shapes=<span class="literal">True</span>, expand_nested=<span class="literal">True</span>)</span><br><span class="line">    self.model = model</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><p>例如：使用200维语义向量、输入长度6、词汇量8000、两层LSTM，一层Dense的模型结构如下：</p><center><img src="https://vivounicorn.github.io/images/ai_chapter_6/image_1emdtm03k1r4l1mq342e14enu6t13.png" width="300"></center></li><li><p><strong>Step-6：模型训练和应用</strong></p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> src.lstm_model <span class="keyword">import</span> LstmModel</span><br><span class="line"><span class="keyword">from</span> src.data_processing <span class="keyword">import</span> PoetrysDataSet</span><br><span class="line"><span class="keyword">from</span> src.word2vec <span class="keyword">import</span> Word2vecModel</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_word2vec</span>(<span class="params">base_data</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">    w2v = Word2vecModel()</span><br><span class="line">    w2v.train_vec()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># test.</span></span><br><span class="line">    a = w2v.most_similar(<span class="built_in">str</span>(base_data.w2i(<span class="string">&#x27;床&#x27;</span>)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(a)):</span><br><span class="line">        <span class="built_in">print</span>(base_data.i2w(a[i][<span class="number">0</span>]), a[i][<span class="number">11</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_lstm</span>(<span class="params">base_data, finetune=<span class="literal">None</span>, mode=<span class="string">&#x27;word2vec&#x27;</span></span>):</span></span><br><span class="line"></span><br><span class="line">    model = LstmModel(cfg_file_path, base_data, mode)</span><br><span class="line">    <span class="comment"># fine tune.</span></span><br><span class="line">    <span class="keyword">if</span> finetune <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        model.load(finetune)</span><br><span class="line"></span><br><span class="line">    model.train_batch(mode=mode)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_lstm</span>(<span class="params">base_data, sentence, model_path=<span class="literal">None</span>, mode=<span class="string">&#x27;word2vec&#x27;</span></span>):</span></span><br><span class="line"></span><br><span class="line">    model = LstmModel(cfg_file_path, base_data, mode)</span><br><span class="line">    <span class="keyword">if</span> model_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        model.load(model_path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model.generate_poetry(sentence, mode=mode)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    cfg_file_path = <span class="string">&#x27;/home/zhanglei/Gitlab/LstmApp/config/cfg.ini&#x27;</span></span><br><span class="line">    w2vmodel_path = <span class="string">&#x27;/home/zhanglei/Gitlab/LstmApp/data/w2v_models/w2v_size200_sg1_hs0_ns3.model&#x27;</span></span><br><span class="line">    model_path = <span class="string">&#x27;/home/zhanglei/Gitlab/LstmApp/data/models/model-2117.hdf5&#x27;</span></span><br><span class="line"></span><br><span class="line">    base_data = PoetrysDataSet(cfg_file_path)</span><br><span class="line">    train_word2vec(base_data)</span><br><span class="line">    base_data.load_word2vec_model(w2vmodel_path)</span><br><span class="line"></span><br><span class="line">    train_lstm(base_data=base_data, finetune=model_path)</span><br><span class="line"></span><br><span class="line">    sentence = <span class="string">&#x27;惜彼落日暮，&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(test_lstm(base_data=base_data, sentence=sentence, model_path=model_path))</span><br></pre></td></tr></table></figure><pre><code>在model.log里会看到训练时的中间信息，如下，随着迭代次数变多，效果会越来越好，包括标点符号的规律也会学进去：

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[2020-11-05 12:58:48,723] - lstm_model.py [Line:127] - [DEBUG]-[thread:140045784893248]-[process:29513] - begin training</span><br><span class="line">[2020-11-05 12:58:48,723] - lstm_model.py [Line:132] - [DEBUG]-[thread:140045784893248]-[process:29513] - batch_size:32,steps_per_epoch:355,epochs:5000,validation_steps152</span><br><span class="line">[2020-11-05 12:59:10,260] - lstm_model.py [Line:194] - [INFO]-[thread:140045784893248]-[process:29513] - ==================Epoch 0, Loss 7.93123197555542=====================</span><br><span class="line">[2020-11-05 12:59:11,968] - lstm_model.py [Line:197] - [INFO]-[thread:140045784893248]-[process:29513] - 欲别牵郎衣，粳酗蓦釱北，鈒静槃遍衫。恸阳日搦蛆，</span><br><span class="line">[2020-11-05 12:59:12,816] - lstm_model.py [Line:197] - [INFO]-[thread:140045784893248]-[process:29513] - 金庭仙树枝，莨行查娇乂。具撅日霈韂，帝鸟 维。。</span><br><span class="line">[2020-11-05 12:59:13,659] - lstm_model.py [Line:197] - [INFO]-[thread:140045784893248]-[process:29513] - 素艳拥行舟，母 佶翕何，藁澡   。一 钺辗。，</span><br><span class="line">[2020-11-05 12:59:14,494] - lstm_model.py [Line:197] - [INFO]-[thread:140045784893248]-[process:29513] - 白鹭拳一足，芾 乡诏秩，启窑 展赢，酪溜劫騊 ，</span><br><span class="line">[2020-11-05 12:59:15,385] - lstm_model.py [Line:197] - [INFO]-[thread:140045784893248]-[process:29513] - 恩酬期必报，闾瞢，颾钏。啾，。耴望，薖，州耒朿。</span><br><span class="line">[2020-11-05 12:59:16,277] - lstm_model.py [Line:197] - [INFO]-[thread:140045784893248]-[process:29513] - 君去方为宰，沈乡看一帷，柳跂 仁柳，营空长日韍。</span><br><span class="line">[2020-11-05 12:59:16,278] - lstm_model.py [Line:198] - [INFO]-[thread:140045784893248]-[process:29513] - ==================End=====================</span><br><span class="line">......</span><br><span class="line">[2020-11-06 02:12:12,971] - lstm_model.py [Line:194] - [INFO]-[thread:140348029687616]-[process:31309] - ==================Epoch 2106, Loss 7.458868980407715=====================</span><br><span class="line">[2020-11-06 02:12:13,732] - lstm_model.py [Line:197] - [INFO]-[thread:140348029687616]-[process:31309] - 新开窗犹偏，回雨草花天。谁因家群应，人年功日未。</span><br><span class="line">[2020-11-06 02:12:14,498] - lstm_model.py [Line:197] - [INFO]-[thread:140348029687616]-[process:31309] - 此心非一事，白物郡期旧。相爱含将回，更相日见光。</span><br><span class="line">[2020-11-06 02:12:15,274] - lstm_model.py [Line:197] - [INFO]-[thread:140348029687616]-[process:31309] - 刻舟寻已化，有恨多两开。去闻难乱东，地中当如来。</span><br><span class="line">[2020-11-06 02:12:16,069] - lstm_model.py [Line:197] - [INFO]-[thread:140348029687616]-[process:31309] - 带水摘禾穗，鸟独光冥客。拂船不自已，远年必年非。</span><br><span class="line">[2020-11-06 02:12:16,846] - lstm_model.py [Line:197] - [INFO]-[thread:140348029687616]-[process:31309] - 茕茕孤思逼，此前路去地。如事别自以，闻阳近高酒。</span><br><span class="line">[2020-11-06 02:12:17,613] - lstm_model.py [Line:197] - [INFO]-[thread:140348029687616]-[process:31309] - 西陆蝉声唱，日衣烟东云。春出不饥家，马白贵风御。</span><br><span class="line">[2020-11-06 02:12:17,614] - lstm_model.py [Line:198] - [INFO]-[thread:140348029687616]-[process:31309] - ==================End=====================</span><br><span class="line">[2020-11-06 02:13:56,069] - lstm_model.py [Line:194] - [INFO]-[thread:140348029687616]-[process:31309] - ==================Epoch 2112, Loss 7.728175163269043=====================</span><br><span class="line">[2020-11-06 02:13:56,946] - lstm_model.py [Line:197] - [INFO]-[thread:140348029687616]-[process:31309] - 旅泊多年岁，发知期东今。君自舟岁未，应当折君新。</span><br><span class="line">[2020-11-06 02:13:57,731] - lstm_model.py [Line:197] - [INFO]-[thread:140348029687616]-[process:31309] - 吾师师子儿，花其重前相。鸟千人身一，清相无道因。</span><br><span class="line">[2020-11-06 02:13:58,532] - lstm_model.py [Line:197] - [INFO]-[thread:140348029687616]-[process:31309] - 和吹度穹旻，此外更高可。来闻人成独，故去深看春。</span><br><span class="line">[2020-11-06 02:13:59,317] - lstm_model.py [Line:197] - [INFO]-[thread:140348029687616]-[process:31309] - 下直遇春日，独与时相飞。江君贤犹名，清清曲河人。</span><br><span class="line">[2020-11-06 02:14:00,089] - lstm_model.py [Line:197] - [INFO]-[thread:140348029687616]-[process:31309] - 月暗竹亭幽，碧昏时尽黄。园春歌雪光，云落分白草。</span><br><span class="line">[2020-11-06 02:14:00,861] - lstm_model.py [Line:197] - [INFO]-[thread:140348029687616]-[process:31309] - 睢阳陷虏日，远平然多岩。公水三共朝，月看同出人。</span><br><span class="line">[2020-11-06 02:14:00,861] - lstm_model.py [Line:198] - [INFO]-[thread:140348029687616]-[process:31309] - ==================End=====================</span><br><span class="line">[2020-11-06 02:15:22,836] - lstm_model.py [Line:148] - [DEBUG]-[thread:140348029687616]-[process:31309] - end training</span><br></pre></td></tr></table></figure></code></pre><h2 id="attention机制">6.4 Attention机制</h2><h3 id="什么是attention机制">6.4.1 什么是Attention机制</h3><p>在人类的认知过程中，Attention(注意力)是所有感知和认知操作的一种核心属性。由于人类对多来源信息的处理能力有限，Attention机制可以帮助我们选择、调整并聚焦在那些与当前目标行为最相关的的信息上。就像小学生上课，主要行为需要集中在黑板和老师身上，但周围的同学、环境等信息也会涌入大脑并参与竞争，所以集中注意力是成为好学生的必要条件之一。</p>再比如计算机视觉应用中，Attention机制和感受野类似，都是对人类真实行为的抽象模拟，人类观察事物时，依据自己当时的兴趣点，“天然”会关注“重点”同时忽略无关点，而重点也一定是一些局部信息，不同的人如果关注的“重点”相同，一定程度上就是“臭味相投”。<center><img src="https://vivounicorn.github.io/images/ai_chapter_6/image_1fbislg3d5m5221g4d15kl1rbc9.png" width="450"></center><p>当然，Attention机制也可以看作是一种通用框架，逻辑类似<a href="https://vivounicorn.github.io/page/2021/09/05/机器学习与人工智能技术分享-第三章-机器学习中的统一框架">第三章</a>中讲的的统一框架，夸张点说它也是某种“上帝视角”，目前它在：<strong><em>计算机视觉</em></strong>(如：图像检测、识别、跟踪等)、<strong><em>自然语言处理</em></strong>(如：翻译、问答、摘要等)、<strong><em>计算机视觉与自然语言处理交叉领域</em></strong>(如：根据图像生成文本描述等)、<strong><em>算法任务</em></strong>(如：神经图灵机)、<strong><em>机器人应用</em></strong>(如：人机交互、控制、导航)等领域的研究和应用遍地开花。</p><h3 id="从seq2seq模型说起">6.4.2 从Seq2Seq模型说起</h3><p>Sequence-to-sequence模型是一种典型的DNN模型，被广泛应用于处理序列问题，如：机器翻译、内容生成等，最早在《<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.3215">Sequence to Sequence Learning with Neural Networks</a>》一文中被提出，以及更早一些具有类似思想，基于RNN Encoder-Decoder做机器翻译的《<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1406.1078">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</a>》。 seq2seq问题处理过程类似这样：</p><center><img src="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-企业微信截图_16337719682456.png" width=""></center><p>以机器翻译：“我是中国人”<span class="math inline">\(\Rightarrow\)</span>“I am from china”为例，如果Seq2seq模型采用Encoder-Decoder框架，那么处理过程类似这样：</p><center><img src="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-企业微信截图_16337732972491.png" width=""></center><p>其中：</p><p>1、<span class="math inline">\(x\)</span>为源输入，像这里的“我是中国人”，进入Encoder模块前一般会被表示为一个语义向量&lt;<span class="math inline">\(x_1,x_2,...,x_n\)</span>&gt;（如：利用word2vec做word embedding进而生成隐语义向量）；</p><p>2、接着，Encoder模块负责接收输入序列并将其抽象/压缩/提取/编码输出为一个上下文向量，这个向量隐含了源输入的序列/语义等各种信息，即执行：<span class="math inline">\(Context=E(x_1,x_2,...,x_n)\)</span>；</p><p>3、上下文向量做为输入进入Decoder模块，并由Decoder模块生成最终的目标序列<span class="math inline">\(y\)</span>，即执行：<span class="math inline">\(y_i=D(Context,y_1,y_2,...,y_{i-1})\)</span>。</p><p>假设： 1、“我是中国人”被表示为以下向量：</p><center><img src="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-企业微信截图_16337793705067.png" width="260" height=""></center>2、Encoder和Decoder都采用经典的RNN结构：<center><img src="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-企业微信截图_16337791134694.png" width="390" height=""></center>Encoder阶段展开后如下：<center><img src="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-企业微信截图_16337800556564.png" width="" height=""></center>最终生成的上下文向量为：“<strong><em>隐藏状态#5</em></strong>” 向量，之后将该向量传入Decoder，逐字生成<span class="math inline">\(y\)</span>，上面的非Attention的Seq2Seq模型执行过程如下：<center><img src="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-企业微信截图_16337807036222.png" width="600" height=""></center><ceneter><img src="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-企业微信截图_16338713082289.png" width="" height=""><p>即：</p><p><span class="math inline">\(y_1=D(隐藏状态\#5)\)</span> = I</p><p><span class="math inline">\(y_2=D(隐藏状态\#5,y_1)\)</span> = am</p><p><span class="math inline">\(y_3=D(隐藏状态\#5,y_1,y_2)\)</span> = from</p><p><span class="math inline">\(y_4=D(隐藏状态\#5,y_1,y_2,y_3)\)</span> = China</p>在没有Attention机制作用时，生成过程使用的是 <strong><em>相同</em></strong> 的上下文向量：“<strong><em>隐藏状态#5</em></strong>”，即，输入序列“我是中国人”中的每个字对翻译结果为“I am from China”的每个单词的贡献是一样的，这显然是不合理的（比如：“是”对China的贡献要远低于“中”和“国”）。为解决此缺点，引入Attention机制，如下：<center><img src="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-attention.jpg" width="" height=""></center><ul><li><ol type="a"><li>Encoder阶段，从原来只保留最后一次迭代的隐层向量，改为保留每次迭代的隐层向量，每个隐层向量隐含了相应输入向量的语义信息；</li></ol></li><li><ol start="2" type="a"><li>以上所有隐层向量传入Decoder阶段；</li></ol></li><li><ol start="3" type="a"><li>以Decoder阶段RNN的第一次迭代为例:</li></ol><p>1、输入为一个“初始向量”和embedding后的“EOS”（输入结束标识符）向量;</p><p>2、第一次迭代产生隐层向量#a；</p><p>3、依据函数<span class="math inline">\(f\)</span>，每个Encoder阶段传来的隐层向量(#1,#2,...,#5)分别和隐层向量#a计算得分： <span class="math display">\[s=f(h_{\#i},h_{\#a})\]</span></p><p>4、计算出的得分利用Softmax函数做归一化： <span class="math display">\[w_{ia}=\frac{f(h_{\#i},h_{\#a})}{\sum_{i=1}^{n=5}f(h_{\#i},h_{\#a})}\]</span></p><p>5、用上面计算出的权重对：隐层向量(#1,#2,...,#5)做加权求和，得到当前迭代的上下文向量： <span class="math display">\[c_{a}=\sum_{i=1}^{n=5}w_{ia}*h_{\#i}\]</span></p><p>6、拼接新的上下文向量<span class="math inline">\(c_a\)</span>与当前隐层向量<span class="math inline">\(h_{\#a}\)</span>，得到： <span class="math display">\[v_a=[c_a,h_{\#a}]\]</span></p><p>7、将上述向量送入一个前馈神经网络，注意，训练时此网络和整个RNN网络是联合训练的；</p><p>8、以上网络的输出向量即为目标内容：<span class="math inline">\(y_1=I\)</span>，其他迭代过程类似就不再赘述。</p></li></ul>这里看一个来源于tensoflow官网的机器翻译例子： 项目目录结构如下：<center><img src="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-企业微信截图_16348962307501.png" width="350" height=""></center><p><strong>1、训练数据</strong></p><p><strong>1)、下载数据集</strong></p><p>下载：http://www.manythings.org/anki/cmn-eng.zip 数据集，内容类似这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">I received an invitation.	我收到了一张请帖。	CC-BY <span class="number">2.0</span> (France) Attribution: tatoeba.org <span class="comment">#258705 (CK) &amp; #332420 (fucongcong)</span></span><br><span class="line">I saw her clean the room.	我看見了她打掃房間。	CC-BY <span class="number">2.0</span> (France) Attribution: tatoeba.org <span class="comment">#261072 (CK) &amp; #862735 (Martha)</span></span><br><span class="line">I saw her enter the room.	我看見她進了房間。	CC-BY <span class="number">2.0</span> (France) Attribution:</span><br></pre></td></tr></table></figure><p></p><p><strong>2)、安装繁简转换器</strong></p><p>由于训练集是繁体的，所以需要做下繁简转换，我这里使用OpenCC库：https://github.com/BYVoid/OpenCC 通常情况使用以下命令可安装：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install opencc</span><br></pre></td></tr></table></figure><p>使用方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> opencc</span><br><span class="line">converter = opencc.OpenCC(<span class="string">&#x27;t2s.json&#x27;</span>)</span><br><span class="line">converter.convert(<span class="string">&#x27;漢字&#x27;</span>)  <span class="comment"># 汉字</span></span><br></pre></td></tr></table></figure><p></p><p>但如果无法成功，尤其在ubuntu 18.04下，也可以尝试源码安装，不过编译过程很痛苦，你可能会遇到各种各样稀奇古怪的错误，最简单的方案是使用以下项目：https://pypi.org/project/opencc-python-reimplemented/</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install opencc-python-reimplemented</span><br></pre></td></tr></table></figure><p></p><p>使用方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> opencc</span><br><span class="line">converter = opencc.OpenCC(<span class="string">&#x27;t2s&#x27;</span>)</span><br><span class="line">converter.convert(<span class="string">&#x27;漢字&#x27;</span>)  <span class="comment"># 汉字</span></span><br></pre></td></tr></table></figure><p></p><p><strong>3)、安装matplotlib字体</strong> 由于默认matplotlib不支持显示中文，会报错：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RuntimeWarning: Glyph ***** missing <span class="keyword">from</span> current font. font.set_text(s, <span class="number">0.0</span>, flags=flags)</span><br></pre></td></tr></table></figure><p></p><p>网上有一堆所谓解决办法，但真正靠谱的没几种，不想尝试的可以用我下面的方法，该方法适用于：deepin及ubuntu 18.04。</p><ul><li><p>下载中文字体，如：SimHei(https://github.com/StellarCN/scp_zh/blob/master/fonts/SimHei.ttf)</p></li><li><p>在终端查看matplotlib路径</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="built_in">print</span>(matplotlib.matplotlib_fname())</span><br></pre></td></tr></table></figure><p>例如输出：</p><blockquote><p>/home/leon/.local/lib/python3.8/site-packages/matplotlib/mpl-data/matplotlibrc</p></blockquote><p>进入目录：</p><blockquote><p>/home/leon/.local/lib/python3.8/site-packages/matplotlib/mpl-data/fonts/ttf</p></blockquote><p>把上面的字体放入该目录后，删除缓存文件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">matplotlib.get_cachedir()</span><br></pre></td></tr></table></figure><p>例如输出：</p><blockquote><p>/home/leon/.cache/matplotlib</p></blockquote><p>执行：</p><blockquote><p>rm -rf /home/leon/.cache/matplotlib</p></blockquote><ul><li>修改matplotlibrc配置文件</li></ul><p>例如路径：</p><blockquote><p>/home/leon/.local/lib/python3.8/site-packages/matplotlib/mpl-data/matplotlibrc 改三个部分如下：</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#small, medium, large, x-large, xx-large, larger, or smaller</span></span><br><span class="line">font.family:  sans-serif</span><br><span class="line"><span class="comment">#font.style:   normal</span></span><br><span class="line"><span class="comment">#font.variant: normal</span></span><br><span class="line"><span class="comment">#font.weight:  normal</span></span><br><span class="line"><span class="comment">#font.stretch: normal</span></span><br><span class="line"><span class="comment">#font.size:    10.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#font.serif:     SimHei,  DejaVu Serif, Bitstream Vera Serif, Computer Modern Roman, New Century Schoolbook, Century Schoolbook L, Utopia, ITC Bookman, Bookman, Nimbus Roman No9 L, Times New Roman, Times, Palatino, Charter, serif</span></span><br><span class="line">font.sans-serif: SimHei, DejaVu Sans, Bitstream Vera Sans, Computer Modern Sans Serif, Lucida Grande, Verdana, Geneva, Lucid, Arial, Helvetica, Avant Garde, sans-serif</span><br><span class="line"><span class="comment">#font.cursive:    Apple Chancery, Textile, Zapf Chancery, Sand, Script MT, Felipa, Comic Neue, Comic Sans MS, cursive</span></span><br><span class="line"><span class="comment">#axes.spines.right:  True</span></span><br><span class="line"></span><br><span class="line">axes.unicode_minus: <span class="literal">False</span>  <span class="comment"># use Unicode for the minus symbol rather than hyphen.  See</span></span><br><span class="line">                           <span class="comment"># https://en.wikipedia.org/wiki/Plus_and_minus_signs#Character_codes</span></span><br><span class="line"><span class="comment">#axes.prop_cycle: cycler(&#x27;color&#x27;, [&#x27;1f77b4&#x27;, &#x27;ff7f0e&#x27;, &#x27;2ca02c&#x27;, &#x27;d62728&#x27;, &#x27;9467bd&#x27;, &#x27;8c564b&#x27;, &#x27;e377c2&#x27;, &#x27;7f7f7f&#x27;, &#x27;bcbd22&#x27;, &#x27;17becf&#x27;])</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>2、数据预处理</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> opencc</span><br><span class="line"><span class="keyword">import</span> unicodedata</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line"></span><br><span class="line"><span class="comment"># 繁体汉字转为简体汉字</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">t2s_converter</span>(<span class="params">sentence</span>):</span></span><br><span class="line">    cvt = opencc.OpenCC(<span class="string">&#x27;t2s&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> cvt.convert(sentence)</span><br><span class="line"></span><br><span class="line"><span class="comment"># unicode转ascii</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unicode_to_ascii</span>(<span class="params">s</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join(c <span class="keyword">for</span> c <span class="keyword">in</span> unicodedata.normalize(<span class="string">&#x27;NFD&#x27;</span>, s)</span><br><span class="line">                   <span class="keyword">if</span> unicodedata.category(c) != <span class="string">&#x27;Mn&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查找文本中第一个汉字</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_first_chinese</span>(<span class="params">s</span>):</span></span><br><span class="line">    han_str = re.<span class="built_in">compile</span>(<span class="string">&#x27;[\u4e00-\u9fff]+&#x27;</span>).findall(s)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(han_str) &lt;= <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> han_str[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取处理文本中的英文句子</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_ch_en</span>(<span class="params">sen</span>):</span></span><br><span class="line">    idx = sen.find(<span class="string">&#x27;CC-BY&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> idx == -<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    w = unicode_to_ascii(sen[:idx].lower().strip())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在单词与跟在其后的标点符号之间插入一个空格</span></span><br><span class="line">    <span class="comment"># 例如： &quot;he is a boy.&quot; =&gt; &quot;he is a boy .&quot;</span></span><br><span class="line">    <span class="comment"># 参考：https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation</span></span><br><span class="line">    w = re.sub(<span class="string">r&quot;([?.!])&quot;</span>, <span class="string">r&quot; \1 &quot;</span>, w)</span><br><span class="line">    w = re.sub(<span class="string">r&#x27;[&quot; &quot;]+&#x27;</span>, <span class="string">&quot; &quot;</span>, w)</span><br><span class="line"></span><br><span class="line">    w = w.rstrip().strip()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定位汉语和英语句子所在位置</span></span><br><span class="line">    han = find_first_chinese(w)</span><br><span class="line">    idx = w.find(han)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> idx == -<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    han_str = w[idx:]</span><br><span class="line">		<span class="comment"># 给句子加上开始和结束标记</span></span><br><span class="line">    <span class="comment"># 以便模型知道何时开始和结束预测</span></span><br><span class="line">    en_str = <span class="string">&#x27;&lt;start&gt; &#x27;</span> + w[:idx].strip() + <span class="string">&#x27; &lt;end&gt;&#x27;</span></span><br><span class="line">		<span class="comment"># 返回一个英文句子在前，汉语句子在后的list</span></span><br><span class="line">    w = [en_str, <span class="string">&#x27;&lt;start&gt; &#x27;</span> + chinese_words_cut(t2s_converter(han_str)) + <span class="string">&#x27; &lt;end&gt;&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> w</span><br><span class="line"></span><br><span class="line"><span class="comment"># 中文分词，这里采用结巴分词。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chinese_words_cut</span>(<span class="params">sentence</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot; &quot;</span>.join(jieba.cut(sentence, cut_all=<span class="literal">False</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 去除重音符号</span></span><br><span class="line"><span class="comment"># 2. 清理句子</span></span><br><span class="line"><span class="comment"># 3. 返回这样格式的单词对：[CHINESE, ENGLISH]</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_dataset</span>(<span class="params">path, num_examples</span>):</span></span><br><span class="line">    lines = io.<span class="built_in">open</span>(path, encoding=<span class="string">&#x27;UTF-8&#x27;</span>).read().strip().split(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    word_pairs = [extract_ch_en(l) <span class="keyword">for</span> l <span class="keyword">in</span> lines[:num_examples]]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">zip</span>(*word_pairs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回最大长度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_length</span>(<span class="params">tensor</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">max</span>(<span class="built_in">len</span>(t) <span class="keyword">for</span> t <span class="keyword">in</span> tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 文本预处理、整数序列化：https://keras.io/zh/preprocessing/text/</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize</span>(<span class="params">lang</span>):</span></span><br><span class="line">    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(</span><br><span class="line">        filters=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    lang_tokenizer.fit_on_texts(lang)</span><br><span class="line"></span><br><span class="line">    tensor = lang_tokenizer.texts_to_sequences(lang)</span><br><span class="line"></span><br><span class="line">    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,</span><br><span class="line">                                                           padding=<span class="string">&#x27;post&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tensor, lang_tokenizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span>(<span class="params">path, num_examples=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="comment"># 创建清理过的输入输出对</span></span><br><span class="line">    targ_lang, inp_lang = create_dataset(path, num_examples)</span><br><span class="line"></span><br><span class="line">    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)</span><br><span class="line">    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回整数编号和文本的对应关系</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert</span>(<span class="params">lang, tensor</span>):</span></span><br><span class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> tensor:</span><br><span class="line">    <span class="keyword">if</span> t!=<span class="number">0</span>:</span><br><span class="line">      <span class="built_in">print</span> (<span class="string">&quot;%d ----&gt; %s&quot;</span> % (t, lang.index_word[t]))</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>3、实现Encoder编码器</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>(<span class="params">tf.keras.Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embedding_dim, enc_units, batch_sz</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.batch_sz = batch_sz</span><br><span class="line">        self.enc_units = enc_units</span><br><span class="line">        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.gru = tf.keras.layers.GRU(self.enc_units,</span><br><span class="line">                                       return_sequences=<span class="literal">True</span>,</span><br><span class="line">                                       return_state=<span class="literal">True</span>,</span><br><span class="line">                                       recurrent_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 很简单的结构，embedding+GRU单元</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x, hidden</span>):</span></span><br><span class="line">        x = self.embedding(x)</span><br><span class="line">        output, state = self.gru(x, initial_state=hidden)</span><br><span class="line">        <span class="keyword">return</span> output, state</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 随机初始化隐层权重</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize_hidden_state</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> tf.zeros((self.batch_sz, self.enc_units))</span><br></pre></td></tr></table></figure><p><strong>4、实现Decoder解码器</strong></p><ul><li><p>BahdanauAttention</p><p>Bahdanau在《Neural Machine Translation by Jointly Learning to Align and Translate》这篇论文提出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BahdanauAttention</span>(<span class="params">tf.keras.layers.Layer</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, units</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(BahdanauAttention, self).__init__()</span><br><span class="line">        self.W1 = tf.keras.layers.Dense(units)</span><br><span class="line">        self.W2 = tf.keras.layers.Dense(units)</span><br><span class="line">        self.V = tf.keras.layers.Dense(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, query, values</span>):</span></span><br><span class="line">        <span class="comment"># 隐藏层的形状 == （批大小，隐藏层大小）</span></span><br><span class="line">        <span class="comment"># hidden_with_time_axis 的形状 == （批大小，1，隐藏层大小）</span></span><br><span class="line">        <span class="comment"># 这样做是为了执行加法以计算分数</span></span><br><span class="line">        hidden_with_time_axis = tf.expand_dims(query, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 分数的形状 == （批大小，最大长度，1）</span></span><br><span class="line">        <span class="comment"># 我们在最后一个轴上得到 1， 因为我们把分数应用于 self.V</span></span><br><span class="line">        <span class="comment"># 在应用 self.V 之前，张量的形状是（批大小，最大长度，单位）</span></span><br><span class="line">        score = self.V(tf.nn.tanh(</span><br><span class="line">            self.W1(values) + self.W2(hidden_with_time_axis)))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 注意力权重 （attention_weights） 的形状 == （批大小，最大长度，1）</span></span><br><span class="line">        attention_weights = tf.nn.softmax(score, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 上下文向量 （context_vector） 求和之后的形状 == （批大小，隐藏层大小）</span></span><br><span class="line">        context_vector = attention_weights * values</span><br><span class="line">        context_vector = tf.reduce_sum(context_vector, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> context_vector, attention_weights</span><br></pre></td></tr></table></figure><p></p></li><li><p>Decoder</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> BahdanauAttention <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span>(<span class="params">tf.keras.Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embedding_dim, dec_units, batch_sz</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.batch_sz = batch_sz</span><br><span class="line">        self.dec_units = dec_units</span><br><span class="line">        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.gru = tf.keras.layers.GRU(self.dec_units,</span><br><span class="line">                                       return_sequences=<span class="literal">True</span>,</span><br><span class="line">                                       return_state=<span class="literal">True</span>,</span><br><span class="line">                                       recurrent_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>)</span><br><span class="line">        self.fc = tf.keras.layers.Dense(vocab_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 用于注意力</span></span><br><span class="line">        self.attention = BahdanauAttention(self.dec_units)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x, hidden, enc_output</span>):</span></span><br><span class="line">        <span class="comment"># 编码器输出 （enc_output） 的形状 == （批大小，最大长度，隐藏层大小）</span></span><br><span class="line">        context_vector, attention_weights = self.attention.call(hidden, enc_output)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># x 在通过嵌入层后的形状 == （批大小，1，嵌入维度）</span></span><br><span class="line">        x = self.embedding(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># x 在拼接 （concatenation） 后的形状 == （批大小，1，嵌入维度 + 隐藏层大小）</span></span><br><span class="line">				<span class="comment"># 注意这里是“拼接”操作，可以对比前面的大图理解</span></span><br><span class="line">        x = tf.concat([tf.expand_dims(context_vector, <span class="number">1</span>), x], axis=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将合并后的向量传送到 GRU</span></span><br><span class="line">        output, state = self.gru(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输出的形状 == （批大小 * 1，隐藏层大小）</span></span><br><span class="line">        output = tf.reshape(output, (-<span class="number">1</span>, output.shape[<span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输出的形状 == （批大小，vocab）</span></span><br><span class="line">        x = self.fc(output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x, state, attention_weights</span><br></pre></td></tr></table></figure></li></ul><p><strong>5、模型构建和训练</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.ticker <span class="keyword">as</span> ticker</span><br><span class="line"><span class="keyword">import</span> utils.preprocessor <span class="keyword">as</span> up</span><br><span class="line"><span class="keyword">from</span> Decoder <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> Encoder <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DatasetParams</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.inp_lang = <span class="literal">None</span></span><br><span class="line">        self.targ_lang = <span class="literal">None</span></span><br><span class="line">        self.max_length_targ = <span class="number">0</span></span><br><span class="line">        self.max_length_inp = <span class="number">0</span></span><br><span class="line">        self.BATCH_SIZE = <span class="number">64</span></span><br><span class="line">        self.BUFFER_SIZE = <span class="number">0</span></span><br><span class="line">        self.steps_per_epoch = <span class="number">0</span></span><br><span class="line">        self.embedding_dim = <span class="number">256</span></span><br><span class="line">        self.units = <span class="number">1024</span></span><br><span class="line">        self.vocab_inp_size = <span class="number">0</span></span><br><span class="line">        self.vocab_tar_size = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ModelParams</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.encoder = <span class="literal">None</span></span><br><span class="line">        self.decoder = <span class="literal">None</span></span><br><span class="line">        self.optimizer = <span class="literal">None</span></span><br><span class="line">        self.optimizer = <span class="literal">None</span></span><br><span class="line">        self.loss_object = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CheckPointsParams</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.checkpoint_prefix = <span class="string">&#x27;&#x27;</span></span><br><span class="line">        self.checkpoint = <span class="literal">None</span></span><br><span class="line">        self.checkpoint_dir = <span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_preparing</span>(<span class="params">path_to_file</span>):</span></span><br><span class="line">    params = DatasetParams()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 尝试实验不同大小的数据集</span></span><br><span class="line">    num_examples = <span class="number">30000</span></span><br><span class="line">    input_tensor, target_tensor, params.inp_lang, params.targ_lang = up.load_dataset(path_to_file, num_examples)</span><br><span class="line">    <span class="comment"># 计算目标张量的最大长度 （max_length）</span></span><br><span class="line">    params.max_length_targ, params.max_length_inp = up.max_length(target_tensor), up.max_length(input_tensor)</span><br><span class="line">    <span class="comment"># 采用 80 - 20 的比例切分训练集和验证集</span></span><br><span class="line">    input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor,</span><br><span class="line">                                                                                                    target_tensor,</span><br><span class="line">                                                                                                    test_size=<span class="number">0.2</span>)</span><br><span class="line">    <span class="comment"># 显示长度</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(input_tensor_train), <span class="built_in">len</span>(target_tensor_train), <span class="built_in">len</span>(input_tensor_val), <span class="built_in">len</span>(target_tensor_val))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Input Language; index to word mapping&quot;</span>)</span><br><span class="line">    up.convert(params.inp_lang, input_tensor_train[<span class="number">0</span>])</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Target Language; index to word mapping&quot;</span>)</span><br><span class="line">    up.convert(params.targ_lang, target_tensor_train[<span class="number">0</span>])</span><br><span class="line">    params.BUFFER_SIZE = <span class="built_in">len</span>(input_tensor_train)</span><br><span class="line"></span><br><span class="line">    params.steps_per_epoch = <span class="built_in">len</span>(input_tensor_train) // params.BATCH_SIZE</span><br><span class="line"></span><br><span class="line">    params.vocab_inp_size = <span class="built_in">len</span>(params.inp_lang.word_index) + <span class="number">1</span></span><br><span class="line">    params.vocab_tar_size = <span class="built_in">len</span>(params.targ_lang.word_index) + <span class="number">1</span></span><br><span class="line">    dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(params.BUFFER_SIZE)</span><br><span class="line">    dataset = dataset.batch(params.BATCH_SIZE, drop_remainder=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> params, dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inference_model_building</span>(<span class="params">dataset_params</span>):</span></span><br><span class="line"></span><br><span class="line">    params = ModelParams()</span><br><span class="line"></span><br><span class="line">    params.encoder = Encoder(dataset_params.vocab_inp_size, dataset_params.embedding_dim, dataset_params.units,</span><br><span class="line">                             dataset_params.BATCH_SIZE)</span><br><span class="line"></span><br><span class="line">    params.decoder = Decoder(dataset_params.vocab_tar_size, dataset_params.embedding_dim, dataset_params.units,</span><br><span class="line">                             dataset_params.BATCH_SIZE)</span><br><span class="line"></span><br><span class="line">    params.optimizer = tf.keras.optimizers.Adam()</span><br><span class="line">    params.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(</span><br><span class="line">        from_logits=<span class="literal">True</span>, reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> params</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_building</span>(<span class="params">dataset_params, dataset</span>):</span></span><br><span class="line">	  <span class="comment"># 返回一个batch的数据</span></span><br><span class="line">    example_input_batch, example_target_batch = <span class="built_in">next</span>(<span class="built_in">iter</span>(dataset))</span><br><span class="line">    <span class="built_in">print</span>(example_input_batch.shape, example_target_batch.shape)</span><br><span class="line"></span><br><span class="line">    params = ModelParams()</span><br><span class="line"></span><br><span class="line">    params.encoder = Encoder(dataset_params.vocab_inp_size, dataset_params.embedding_dim, dataset_params.units,</span><br><span class="line">                             dataset_params.BATCH_SIZE)</span><br><span class="line">    <span class="comment"># 样本输入</span></span><br><span class="line">    sample_hidden = params.encoder.initialize_hidden_state()</span><br><span class="line">    sample_output, sample_hidden = params.encoder.call(example_input_batch, sample_hidden)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Encoder output shape: (batch size, sequence length, units) &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(sample_output.shape))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Encoder Hidden state shape: (batch size, units) &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(sample_hidden.shape))</span><br><span class="line">    attention_layer = BahdanauAttention(<span class="number">10</span>)</span><br><span class="line">    attention_result, attention_weights = attention_layer.call(sample_hidden, sample_output)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Attention result shape: (batch size, units) &#123;&#125;&quot;</span>.<span class="built_in">format</span>(attention_result.shape))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Attention weights shape: (batch_size, sequence_length, 1) &#123;&#125;&quot;</span>.<span class="built_in">format</span>(attention_weights.shape))</span><br><span class="line">    params.decoder = Decoder(dataset_params.vocab_tar_size, dataset_params.embedding_dim, dataset_params.units,</span><br><span class="line">                             dataset_params.BATCH_SIZE)</span><br><span class="line">    sample_decoder_output, _, _ = params.decoder.call(tf.random.uniform((<span class="number">64</span>, <span class="number">1</span>)),</span><br><span class="line">                                                      sample_hidden, sample_output)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Decoder output shape: (batch_size, vocab size) &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(sample_decoder_output.shape))</span><br><span class="line">    params.optimizer = tf.keras.optimizers.Adam()</span><br><span class="line">    params.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(</span><br><span class="line">        from_logits=<span class="literal">True</span>, reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> params</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_function</span>(<span class="params">model_params, real, pred</span>):</span></span><br><span class="line">	  <span class="comment"># 逻辑非</span></span><br><span class="line">    mask = tf.math.logical_not(tf.math.equal(real, <span class="number">0</span>))</span><br><span class="line">    loss_ = model_params.loss_object(real, pred)</span><br><span class="line"></span><br><span class="line">    mask = tf.cast(mask, dtype=loss_.dtype)</span><br><span class="line">    loss_ *= mask</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tf.reduce_mean(loss_)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chk_settings</span>(<span class="params">model_params</span>):</span></span><br><span class="line">    params = CheckPointsParams()</span><br><span class="line">    params.checkpoint_dir = <span class="string">&#x27;./training_checkpoints&#x27;</span></span><br><span class="line">    params.checkpoint_prefix = os.path.join(params.checkpoint_dir, <span class="string">&quot;ckpt&quot;</span>)</span><br><span class="line">    params.checkpoint = tf.train.Checkpoint(optimizer=model_params.optimizer,</span><br><span class="line">                                            encoder=model_params.encoder,</span><br><span class="line">                                            decoder=model_params.decoder)</span><br><span class="line">    <span class="keyword">return</span> params</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span>(<span class="params">dataset_params, model_params, inp, targ, enc_hidden</span>):</span></span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        enc_output, enc_hidden = model_params.encoder.call(inp, enc_hidden)</span><br><span class="line"></span><br><span class="line">        dec_hidden = enc_hidden</span><br><span class="line"></span><br><span class="line">        dec_input = tf.expand_dims([dataset_params.targ_lang.word_index[<span class="string">&#x27;&lt;start&gt;&#x27;</span>]] * dataset_params.BATCH_SIZE, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 教师强制 - 将目标词作为下一个输入，每个目标词来一下。</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, targ.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="comment"># 将编码器输出 （enc_output） 传送至解码器</span></span><br><span class="line">            predictions, dec_hidden, _ = model_params.decoder.call(dec_input, dec_hidden, enc_output)</span><br><span class="line"></span><br><span class="line">            loss += loss_function(model_params, targ[:, t], predictions)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 使用教师强制</span></span><br><span class="line">            dec_input = tf.expand_dims(targ[:, t], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    batch_loss = (loss / <span class="built_in">int</span>(targ.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    variables = model_params.encoder.trainable_variables + model_params.decoder.trainable_variables</span><br><span class="line"></span><br><span class="line">    gradients = tape.gradient(loss, variables)</span><br><span class="line"></span><br><span class="line">    model_params.optimizer.apply_gradients(<span class="built_in">zip</span>(gradients, variables))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> batch_loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainer</span>(<span class="params">dataset_params, model_params, chk_params, dataset, epochs=<span class="number">100</span></span>):</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        start = time.time()</span><br><span class="line"></span><br><span class="line">        enc_hidden = model_params.encoder.initialize_hidden_state()</span><br><span class="line">        total_loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (batch, (inp, targ)) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataset.take(dataset_params.steps_per_epoch)):</span><br><span class="line">            batch_loss = train_step(dataset_params, model_params, inp, targ, enc_hidden)</span><br><span class="line">            total_loss += batch_loss</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> batch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;Epoch &#123;&#125; Batch &#123;&#125; Loss &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch + <span class="number">1</span>,</span><br><span class="line">                                                             batch,</span><br><span class="line">                                                             batch_loss.numpy()))</span><br><span class="line">        <span class="comment"># 每 2 个周期（epoch），保存（检查点）一次模型</span></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">            chk_params.checkpoint.save(file_prefix=chk_params.checkpoint_prefix)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Epoch &#123;&#125; Loss &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch + <span class="number">1</span>,</span><br><span class="line">                                            total_loss / dataset_params.steps_per_epoch))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Time taken for 1 epoch &#123;&#125; sec\n&#x27;</span>.<span class="built_in">format</span>(time.time() - start))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span>(<span class="params">dataset_params, model_params, sentence</span>):</span></span><br><span class="line">    attention_plot = np.zeros((dataset_params.max_length_targ, dataset_params.max_length_inp))</span><br><span class="line"></span><br><span class="line">    sentence = up.extract_ch(sentence)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;============================&quot;</span>, sentence)</span><br><span class="line"></span><br><span class="line">    inputs = [dataset_params.inp_lang.word_index[i] <span class="keyword">for</span> i <span class="keyword">in</span> sentence.split(<span class="string">&#x27; &#x27;</span>)]</span><br><span class="line">    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],</span><br><span class="line">                                                           maxlen=dataset_params.max_length_inp,</span><br><span class="line">                                                           padding=<span class="string">&#x27;post&#x27;</span>)</span><br><span class="line">    inputs = tf.convert_to_tensor(inputs)</span><br><span class="line"></span><br><span class="line">    result = <span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    hidden = [tf.zeros((<span class="number">1</span>, dataset_params.units))]</span><br><span class="line">    enc_out, enc_hidden = model_params.encoder.call(inputs, hidden)</span><br><span class="line"></span><br><span class="line">    dec_hidden = enc_hidden</span><br><span class="line">    dec_input = tf.expand_dims([dataset_params.targ_lang.word_index[<span class="string">&#x27;&lt;start&gt;&#x27;</span>]], <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(dataset_params.max_length_targ):</span><br><span class="line">        predictions, dec_hidden, attention_weights = model_params.decoder.call(dec_input,</span><br><span class="line">                                                                               dec_hidden,</span><br><span class="line">                                                                               enc_out)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 存储注意力权重以便后面制图</span></span><br><span class="line">        attention_weights = tf.reshape(attention_weights, (-<span class="number">1</span>,))</span><br><span class="line">        attention_plot[t] = attention_weights.numpy()</span><br><span class="line"></span><br><span class="line">        predicted_id = tf.argmax(predictions[<span class="number">0</span>]).numpy()</span><br><span class="line"></span><br><span class="line">        result += dataset_params.targ_lang.index_word[predicted_id] + <span class="string">&#x27; &#x27;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> dataset_params.targ_lang.index_word[predicted_id] == <span class="string">&#x27;&lt;end&gt;&#x27;</span>:</span><br><span class="line">            <span class="keyword">return</span> result, sentence, attention_plot</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 预测的 ID 被输送回模型</span></span><br><span class="line">        dec_input = tf.expand_dims([predicted_id], <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result, sentence, attention_plot</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意力权重制图函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_attention</span>(<span class="params">attention, sentence, predicted_sentence</span>):</span></span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">    ax = fig.add_subplot(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    ax.matshow(attention, cmap=<span class="string">&#x27;viridis&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    fontdict = &#123;<span class="string">&#x27;fontsize&#x27;</span>: <span class="number">15</span>&#125;</span><br><span class="line"></span><br><span class="line">    ax.set_xticklabels([<span class="string">&#x27;&#x27;</span>] + sentence, fontdict=fontdict, rotation=<span class="number">90</span>)</span><br><span class="line">    ax.set_yticklabels([<span class="string">&#x27;&#x27;</span>] + predicted_sentence, fontdict=fontdict)</span><br><span class="line"></span><br><span class="line">    ax.xaxis.set_major_locator(ticker.MultipleLocator(<span class="number">1</span>))</span><br><span class="line">    ax.yaxis.set_major_locator(ticker.MultipleLocator(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    plt.savefig(<span class="string">&quot;latest.png&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">restore_model</span>(<span class="params">chk_params</span>):</span></span><br><span class="line">    chk_params.checkpoint.restore(tf.train.latest_checkpoint(chk_params.checkpoint_dir))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">translate</span>(<span class="params">dataset_params, model_params, sentence</span>):</span></span><br><span class="line">    result, sentence, attention_plot = evaluate(dataset_params, model_params, sentence)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Input: %s&#x27;</span> % sentence)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Predicted translation: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(result))</span><br><span class="line"></span><br><span class="line">    attention_plot = attention_plot[:<span class="built_in">len</span>(result.split(<span class="string">&#x27; &#x27;</span>)), :<span class="built_in">len</span>(sentence.split(<span class="string">&#x27; &#x27;</span>))]</span><br><span class="line">    plot_attention(attention_plot, sentence.split(<span class="string">&#x27; &#x27;</span>), result.split(<span class="string">&#x27; &#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 序列化存储文件</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">params_dump</span>(<span class="params">params, params_path</span>):</span></span><br><span class="line">    <span class="keyword">if</span> params <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(params_path, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            pickle.dump(params, f)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 反序列化读取文件</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">params_load</span>(<span class="params">params_path</span>):</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(params_path, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">return</span> pickle.load(f)</span><br></pre></td></tr></table></figure><p><strong>6、跑一下</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> utils.preprocessor <span class="keyword">as</span> up</span><br><span class="line"><span class="keyword">import</span> utils.modelling <span class="keyword">as</span> md</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 是否加载序列化文件</span></span><br><span class="line">    is_load = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据实际情况写</span></span><br><span class="line">    dataset_dump_path = <span class="string">&#x27;/home/leon/PycharmProjects/attention-seq2seq/dump/dataset_params.dat&#x27;</span></span><br><span class="line">    model_dump_path = <span class="string">&#x27;/home/leon/PycharmProjects/attention-seq2seq/dump/model_params.dat&#x27;</span></span><br><span class="line">    chk_dump_path = <span class="string">&#x27;/home/leon/PycharmProjects/attention-seq2seq/dump/chk_params.dat&#x27;</span></span><br><span class="line"></span><br><span class="line">    path_to_file = <span class="string">&#x27;/home/leon/PycharmProjects/attention-seq2seq/cmn-eng/cmn.txt&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> is_load:</span><br><span class="line">        dataset_params = md.params_load(dataset_dump_path)</span><br><span class="line">        model_params = md.inference_model_building(dataset_params)</span><br><span class="line"></span><br><span class="line">        chk_params = md.chk_settings(model_params)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        en, sp = up.create_dataset(path_to_file, <span class="literal">None</span>)</span><br><span class="line">        <span class="built_in">print</span>(en[-<span class="number">1</span>])</span><br><span class="line">        <span class="built_in">print</span>(sp[-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        dataset_params, dataset = md.data_preparing(path_to_file)</span><br><span class="line"></span><br><span class="line">        model_params = md.model_building(dataset_params, dataset)</span><br><span class="line"></span><br><span class="line">        chk_params = md.chk_settings(model_params)</span><br><span class="line"></span><br><span class="line">        md.params_dump(dataset_params, dataset_dump_path)</span><br><span class="line"></span><br><span class="line">        md.trainer(dataset_params, model_params, chk_params, dataset)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 恢复检查点目录 checkpoint_dir）中最新的检查点</span></span><br><span class="line">    md.restore_model(chk_params)</span><br><span class="line"></span><br><span class="line">    md.translate(dataset_params, model_params, <span class="string">u&#x27;我认为我们准备好了。&#x27;</span>)</span><br></pre></td></tr></table></figure>结果：<center><img src="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-企业微信截图_16348992418770.png" width="600" height=""></center><p>明显可以看到对于某个目标词，输入里的不同词贡献度很不一样，尤其在对角线上有明显对应关系的两个词，例如：(认为，think)、(准备，ready)等等。</p><h3 id="attention机制的基本结构">6.4.3 Attention机制的基本结构</h3><center><img src="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-2021-11-05 15-32-45 的屏幕截图.png" width="300" height=""></center>回头再看生成 <strong><em>Attention Value</em></strong> 的整个过程，会发现可以抽象成以下过程及公式：<center><img src="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-2021-11-05 16-34-38 的屏幕截图.png" width="500" height=""></center><p>整个过程就像是用Query去数据库中扫描Key，并取出Key所对应的Value，通过计算Query与Key的相似度，来决定这些Value的最终贡献，<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1706.03762.pdf">公式</a>表示为： <span class="math display">\[ Attention\_Value(Q,K,V)=Normalization(Similarity(Q,K))\cdot V \]</span> 其中，<span class="math inline">\(Similarity\)</span>为相似度函数，<span class="math inline">\(Normalization\)</span>为归一化函数。 例如： 当<span class="math inline">\(Similarity\)</span>函数为做了尺度变换的点积函数<span class="math inline">\(QK^T\)</span>（反映了两个向量的夹角和向量的范数），<span class="math inline">\(Normalization\)</span>为<span class="math inline">\(Softmax\)</span>函数时，上述公式变为： <span class="math display">\[ Attention\_Value(Q,K,V)=Softmax(\frac{QK^T}{\sqrt{d_k}})\cdot V \]</span> 其中：<span class="math inline">\(d_k\)</span>为Query向量和Key向量的空间维度，<span class="math inline">\(\frac{1}{\sqrt{d_k}}\)</span>为尺度缩放因子。</p><p><strong>一、Scaled Dot-Product Attention</strong></p><p>上面的公式在文中被叫做：Scaled Dot-Product Attention，基本结构为：</p><center><img src="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-2021-11-10 13-53-21 的屏幕截图.png" width="300" height=""></center><p>这里唯一需要强调的是Mask函数，其作用是：</p><p><strong><em>1、Padding Masked</em></strong></p><p>是在Encoder和Decoder都可能使用的Mask操作，因为NLP问题中很大概率会遇到每句话长度不一样，为了平衡效率与效果，我们又经常使用batch的方式训练模型，所以利用padding填充可以让每个batch中的句子长度一样，带来的问题是这些padding的内容不应该被Attention到，所以利用padding masked给padding位置一个非常大的负数值，这样通过Softmax运算后，这些位置上的概率就是0，相当于把padding位置的信息用掩码给遮蔽了。例如pytorch下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">attention = torch.bmm(q, k.transpose(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">attention = attention.masked_fill(mask.<span class="built_in">bool</span>(), -np.inf)</span><br></pre></td></tr></table></figure><p><strong><em>2、Sequence Masked</em></strong></p><p>是在Decoder使用的masked操作，目的是为了保证以下事实：</p>待预测的位置<span class="math inline">\(i\)</span>只依赖<span class="math inline">\(i,i-1,i-2,...,0\)</span>时刻的输出，而不能看到<strong><em>未来</em></strong>的信息(<span class="math inline">\(i+1,i+2,...,i+n\)</span>时刻的输出)，例如：<center><img src="https://vivounicorn.github.io/images/ai_chapter_6/2021-11-10 15-36-57 的屏幕截图.png" width="500" height=""></center><p>其中黄色部分为有效部分，紫色部分为masked部分，pytorch下的实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ScaledDotProductAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; Scaled Dot-Product Attention &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_k, attn_dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 缩放因子</span></span><br><span class="line">        self.scalar = <span class="number">1</span> / np.power(d_k, <span class="number">0.5</span>)</span><br><span class="line">        self.dropout = nn.Dropout(attn_dropout)</span><br><span class="line">        self.softmax = nn.Softmax(dim=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, q, k, v, mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># 计算q∙k</span></span><br><span class="line">        attn = torch.bmm(q, k.transpose(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># 计算q∙k/sqr(d_k)</span></span><br><span class="line">        attn = attn * self.scalar</span><br><span class="line"></span><br><span class="line">        <span class="comment"># attention masked</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attn = attn.masked_fill(mask.<span class="built_in">bool</span>(), -np.inf)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算softmax(q∙k/sqr(d_k))</span></span><br><span class="line">        attn = self.softmax(attn)</span><br><span class="line">        attn = self.dropout(attn)</span><br><span class="line">        <span class="comment"># 计算softmax(q∙k/sqr(d_k))∙v</span></span><br><span class="line">        output = torch.bmm(attn, v)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output, attn</span><br></pre></td></tr></table></figure><p>相似度函数可以随大家定义，比如，可以是Cosine函数、欧氏距离、甚至是一个神经网络。</p><p>上面机器翻译的例子是这个公式的特殊情况，即5个隐藏状态向量即是Key又是Value，还以机器翻译为例子，如果输入和输出是一样的，就会产生所谓Self Attention的效果，可以看做是通过Attention机制捕获句子本身的内在关系。 举个例子：</p><blockquote><p>许多自然环境保护主义者担心持续屠杀鲸鱼正推动这些动物走向灭绝。</p></blockquote><center><img src="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-2021-11-05 19-12-34 的屏幕截图.png" width="600" height=""></center><p>从上图可以看到“正”、“推动”、“这些”、“灭绝”对于“推动”这个词的贡献是最大的，这里既包括了距离因素也包括了语义因素，所以引入Self Attention后会更容易捕获句子中长距离依赖特征，而且更容易做并行计算。</p><p><strong>二、Multi-Head Attention</strong></p><center><img src="https://vivounicorn.github.io/images/ai_chapter_6/2021-11-16%2011-54-31%20%E7%9A%84%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE.png" width="350" height=""></center><p>Multi-Head机制是把Q、K、V向量通过<span class="math inline">\(f(x)=x \cdot w^T+b\)</span>做仿射映射，使得模型可以“关注”来自不同位置的、不同表示子空间的隐含信息，且由于这些向量是相互独立的，所以可以做并行计算，又因为所有“Head”维度之和等于变换前“Head”维度，所以总的计算成本没变。Multi-Head Attention由以下四部分组成：</p><ul><li>仿射映射层，将向量映射拆分成若干“Multi-Head” <span class="math display">\[f(X)=X \cdot W^T+B\]</span></li><li>对每个“Multi-Head”做Scaled Dot-Product Attention <span class="math display">\[head_i = Scaled\_Dot\_Product\_Attention(QW_i^Q,KW_i^K,VW_i^V)\]</span></li><li>把上述输出结果做横向拼接 <span class="math display">\[Multi\_Head\_Vector= Concat(head_1, ..., head_h)\]</span></li><li>把拼接结果做线性映射得到最终Attention输出 <span class="math display">\[MultiHead(Q, K, V ) = Multi\_Head\_Vector \cdot W^O\]</span></li></ul><p>Multi-Head的pytorch实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; Multi-Head Attention module &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_of_heads, dim_of_model, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 拆分出的attention head数量.</span></span><br><span class="line">        self.num_of_heads = num_of_heads</span><br><span class="line">        <span class="comment"># 模型维度，例如：embedding层为词向量维度.</span></span><br><span class="line">        self.dim_of_model = dim_of_model</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 模型维度的设置要保证能使拆分出来的所有head维度相同</span></span><br><span class="line">        <span class="keyword">if</span> self.dim_of_model % self.num_of_heads != <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">raise</span> RuntimeError(<span class="string">&#x27;Dimensions of the model must be divisible by number of attention heads.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 拆分出来的每个head向量的维度</span></span><br><span class="line">        self.depth = self.dim_of_model // self.num_of_heads</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 保持输入输出维度的仿射变换</span></span><br><span class="line">        self.w_qs = nn.Linear(self.dim_of_model, self.dim_of_model)</span><br><span class="line">        self.w_ks = nn.Linear(self.dim_of_model, self.dim_of_model)</span><br><span class="line">        self.w_vs = nn.Linear(self.dim_of_model, self.dim_of_model)</span><br><span class="line"></span><br><span class="line">        self.attention = ScaledDotProductAttention(temperature=np.power(self.depth, <span class="number">0.5</span>),</span><br><span class="line">                                                    attn_dropout=dropout)</span><br><span class="line"></span><br><span class="line">        self.layer_norm = nn.LayerNorm(self.dim_of_model)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最终输出层</span></span><br><span class="line">        self.fc = nn.Linear(self.dim_of_model, self.dim_of_model)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, q, k, v, mask=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># q.shape=(batch_size, sequence_len_q, dim_of_model)，其中dim_of_model = num_of_heads * depth</span></span><br><span class="line">        batch_size, sequence_len_q, _ = q.size()</span><br><span class="line">        batch_size, sequence_len_k, _ = k.size()</span><br><span class="line">        batch_size, sequence_len_v, _ = v.size()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 类似ResNet，对query保留输入信息</span></span><br><span class="line">        residual = q</span><br><span class="line"></span><br><span class="line">        <span class="comment"># q.shape=(batch_size, num_of_heads, sequence_len_q, depth)</span></span><br><span class="line">        q = self.w_qs(q).view(batch_size, -<span class="number">1</span>, self.num_of_heads, self.depth).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">        k = self.w_qs(k).view(batch_size, -<span class="number">1</span>, self.num_of_heads, self.depth).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">        v = self.w_qs(v).view(batch_size, -<span class="number">1</span>, self.num_of_heads, self.depth).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># q.shape=(batch_size * num_of_heads, sequence_len_q, depth)</span></span><br><span class="line">        q = q.reshape(batch_size * self.num_of_heads, -<span class="number">1</span>, self.depth)</span><br><span class="line">        k = k.reshape(batch_size * self.num_of_heads, -<span class="number">1</span>, self.depth)</span><br><span class="line">        v = v.reshape(batch_size * self.num_of_heads, -<span class="number">1</span>, self.depth)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># mask操作</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            mask = mask.repeat(self.num_of_heads, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        scaled_attention, attention_weights = self.attention(q, k, v, mask=mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># scaled_attention.shape=(batch_size, sequence_len_q, num_of_heads, depth)</span></span><br><span class="line">        scaled_attention = scaled_attention.view(batch_size, self.num_of_heads, sequence_len_q, self.depth).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">        <span class="comment"># attention_weights.shape=(batch_size, num_of_heads, sequence_len_q, sequence_len_k)</span></span><br><span class="line">        attention_weights = attention_weights.view(batch_size, self.num_of_heads, sequence_len_q, sequence_len_k)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 拼接所有head</span></span><br><span class="line">        <span class="comment"># concat_attention.shape=(batch_size, sequence_len_q, dim_of_model)，其中dim_of_model = num_of_heads * depth</span></span><br><span class="line">        concat_attention = scaled_attention.reshape(batch_size, sequence_len_q, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 全连接层做线性输出</span></span><br><span class="line">        linear_output = self.fc(concat_attention)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 加入query残差信息并做Layer Normalization归一化，见论文：https://arxiv.org/pdf/1607.06450.pdf</span></span><br><span class="line">        output = self.layer_norm(linear_output + residual)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output, attention_weights</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># (num_of_heads, dim_of_model)</span></span><br><span class="line">mha = MultiHeadAttention(<span class="number">8</span>, <span class="number">512</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># (batch_size, sequence_len_q, dim_of_model)</span></span><br><span class="line">q = torch.Tensor(<span class="number">16</span>, <span class="number">60</span>, <span class="number">512</span>)</span><br><span class="line"></span><br><span class="line">output, attention_weights = mha.forward(q, k=q, v=q, mask=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;shape of output:&quot;&#123;0&#125;&quot;, shape of attention weight:&quot;&#123;1&#125;&quot;&#x27;</span>.<span class="built_in">format</span>(output.shape, attention_weights.shape))</span><br></pre></td></tr></table></figure><p></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">q.shape:  torch.Size([16, 60, 512])</span><br><span class="line">scaled_attention shape: torch.Size([16, 60, 8, 64])</span><br><span class="line">attention_weights shape: torch.Size([16, 8, 60, 60])</span><br><span class="line">concat_attention: torch.Size([16, 60, 512])</span><br><span class="line">shape of output:&quot;torch.Size([16, 60, 512])&quot;, shape of attention weight:&quot;torch.Size([16, 8, 60, 60])&quot;</span><br></pre></td></tr></table></figure><p><strong><em>三、Position-wise Feed-Forward Networks</em></strong></p><p>主要用来做Attention信息的非线性组合，类似于使用了1×1卷积操作，输入序列上每个位置的词生成的Attention信息都会有一个前馈网络做这种非线性组合，注意是每个“位置”都会做但不同位置使用相同的线性变换，所以被叫做“position-wise”或“point-wise”，同时也意味着位置间可以做并行计算，公式如下： <span class="math display">\[ FFN(x) = max(0, xW_1 + b_1)W_2 + b_2 \]</span> 两个子层之间采用relu做激活，同样为了更好的保留输入信息和有利于梯度传播，也会增加一个残差连接，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim_of_model, d_ff</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        <span class="comment"># 类似做1×1卷积，self.w_1 = nn.Conv1d(dim_of_model, d_ff, 1)</span></span><br><span class="line">        self.w_1 = nn.Linear(dim_of_model, d_ff)</span><br><span class="line">        <span class="comment"># 类似做1×1卷积，self.w_2 = nn.Conv1d(dim_of_model, d_ff, 1)</span></span><br><span class="line">        self.w_2 = nn.Linear(d_ff, dim_of_model)</span><br><span class="line">        self.layer_norm = nn.LayerNorm(dim_of_model)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        residual = x</span><br><span class="line">        output = self.w_2(F.relu(self.w_1(x)))</span><br><span class="line">        output = self.layer_norm(output + residual)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p></p><p><strong><em>五、Positional Encoding &amp; Positional Embedding</em></strong></p><p>我们知道，RNN类神经网络在结构上天然具有序列学习能力，从而将词与词之间的位置关系可以很好的学到，而CNN利用zero-padding及通道编码位置信息，使得空间信息可以被捕获到，相关论文：《<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2108.07884.pdf">Global Pooling, More than Meets the Eye:Position Information is Encoded Channel-Wise in CNNs</a>》，对没有使用这两种结构的网络就需要某种机制帮助考虑词与词在序列中的相对或绝对位置信息。这种机制应该既能隐含位置信息从而捕捉长依赖，又能做并行计算加快训练速度。通过类似word2vec这种机制能够把词的语义压缩在某个低维空间中（即Embedding的维度），且语义相近的词在空间中距离相近，同样的，除了语义距离外，词与词之间的绝对或相对距离也可以通过某种编码或Embedding的方法引入到模型中，这样既考虑了词与词之间的语义相关性又考虑了它们的距离相关性，例如：下面两个句子，分词后的“词”完全一样，语义向量也一样，但词放在不同的位置则意思南辕北辙：</p><blockquote><p>我喜欢苗苗，因为她从不做作。</p></blockquote><blockquote><p>我从不喜欢苗苗，因为她做作。</p></blockquote><p>假设，语义向量为:<span class="math inline">\(S_n\)</span>，位置向量为<span class="math inline">\(P_m\)</span>，两种做法：</p><p>1、如果<span class="math inline">\(m=n\)</span>，则<span class="math inline">\(S_n+P_n\)</span>相当于把词的位置信息和语义信息压缩到了一个<span class="math inline">\(n\)</span>维空间下，随后通过网络学习权重<span class="math inline">\(W\)</span>；</p><p>2、如果<span class="math inline">\(m\neq n\)</span>，则可以通过向量拼接方式实现：<span class="math inline">\([S_n;P_m]\)</span>，随后通过网络学习权重<span class="math inline">\(W_s\)</span>和<span class="math inline">\(W_p\)</span>。</p><p>关于<span class="math inline">\(P_m\)</span>的生成同样也是两类方法：编码方式或者学习Embedding方式：</p><ul><li>Positional Encoding 可以采用固定公式生成位置的绝对编码或者相对编码： 1、按照自然数递增编码，如：1、2、3...：位置间的关系本应“平等”，但顺序编码会使得数值大小严重影响这种“平等”性，产生数据bias； 2、类似one-hot或者dummy方式编码：如：10000，01000，...：编码空间和文本长度有关系，且不能编码到任意空间维度； 3、直觉上词与词局部的相对位置信息会比全局的绝对位置信息来的重要些，所以采用有界周期函数编码，能够做到一定范围内编码与文本长度无关、位置数据无bias、可以体现出词的偏序关系，例如，sine或cosine函数，一种做法是（ps：可以有无数种做法）： 假设，编码维度为<span class="math inline">\(d_{model}\)</span>，则在不同维度上使用不同编码函数如下： <span class="math display">\[ \begin{align*} PE_{(pos,2i)} &amp;= sin(pos/10000^{2i/d_{model}})\\ PE_{(pos,2i+1)} &amp;= cos(pos/10000^{2i/d_{model}}) \end{align*} \]</span> 其中<span class="math inline">\(pos\)</span>是位置，<span class="math inline">\(i\)</span>是维度。位置编码的每个维度都对应于一个正弦曲线, 其波长形成从<span class="math inline">\(2\pi\)</span>到<span class="math inline">\(10000 \cdot 2 \pi\)</span>的等比级数。 代码如下：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, max_len=<span class="number">5000</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line"></span><br><span class="line">        pe = torch.zeros(max_len, d_model, requires_grad=<span class="literal">False</span>)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>).<span class="built_in">float</span>()</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() *</span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line">        length = <span class="built_in">input</span>.size(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> self.pe[:, :length]</span><br></pre></td></tr></table></figure></li><li><p>Positional Embedding 不使用固定函数生成位置信息隐向量，类似word embedding，通过学习方式获得，相关论文见：《<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1705.03122.pdf">Convolutional Sequence to Sequence Learning</a>》，虽然从效果上看两者没什么区别，但个人感觉embedding方式更自然，本身被“原生集成”在了模型中，且一旦与输入序列的embedding向量做“加法”操作，计算开销可以忽略不计。</p><p>关于位置编码的一些深入研究，可以看以下文章：</p><p>1、《<a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=09-528y2Fgf">Rethinking Positional Encoding in Language Pre-training</a>》</p><p>2、《<a target="_blank" rel="noopener" href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/">Transformer Architecture:The Positional Encoding</a>》</p><p>3、《<a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=onxoVA9FxMw">On Position Embeddings in BERT</a>》</p></li></ul><p>至此，Transformer里会用到的一些基本结构介绍完毕。</p><h2 id="transformer">6.5. Transformer</h2><p>与标准Sequence2Sequence模型类似，Transformer版本的模型也是由Encoder和Decoder两大部分组成：</p><p>1、输入序列经过若干个Encoder，为序列中每个词生成一个输出。</p><p>2、Decoder利用Encoder的输出以及Attention信息预测下一个词。</p>大的框架如下：<center><img src="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-2021-11-25 10-36-09 的屏幕截图.png" width="500" height=""></center><p>Encoder之间相互独立，每个Encoder又由两大部分组成：自身的Multi-Head Attention模块和前馈神经网络模块组成：</p><center><img src="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-2021-11-25 10-58-44 的屏幕截图.png" width="330" height=""></center><p>Decoder之间相互独立，每个Decoder又由三大部分组成：自身的Masked Multi-Head Attention模块、由Encoder产生并输入的Multi-Head Attention信息和前馈神经网络模块组成：</p><center><img src="https://vivounicorn.github.io/images/ai_chapter_6/2021-11-25%2011-12-43%20的屏幕截图.png" width="600" height=""></center><p>完整的标准Transformer结构如下：</p><center><img src="https://vivounicorn.github.io/images/ai_chapter_6/2021-09-18-机器学习与人工智能技术分享-第六章-循环神经网络与Transformers-the transformer" width="390" height=""></center><p>需要注意几个地方：</p><ul><li>Encoder的输入部分除了Word Embedding外还有描述词序的Positional Embedding，这两个向量维度相同，且执行“加法”操作后输入到Encoder；</li><li>Encoder的Multi-Head Attention做了Padding Masked；</li><li>Encoder每个子层都有一个残差连接（类似ResNet）帮助梯度更好更深的传播；</li><li>Encoder的Position-wise前馈神经网络后面接了归一化层，用于降低模型方差；</li><li>Transformer有多个Encoder串联；</li><li>Decoder的Masked Multi-Head Attention既做了Padding Masked又做了Sequence Masked；</li><li>Decoder的Multi-Head Attention做了Padding Masked；</li><li>Decoder除了使用Word Embedding外还有描述词序的Positional Embedding；</li><li>Decoder的Position-wise前馈神经网络后面接了归一化层，用于降低模型方差；</li><li>Transformer有多个Decoder串联。</li></ul><p>代码示例如下：</p><ul><li><p>Encoder Layer &amp; Encoder</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># padding填充，为公共方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_attn_pad_mask</span>(<span class="params">seq_q, seq_k</span>):</span></span><br><span class="line">    batch_size, len_q = seq_q.size()</span><br><span class="line">    batch_size, len_k = seq_k.size()</span><br><span class="line">    pad_attn_mask = seq_k.data.eq(<span class="number">0</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> pad_attn_mask.expand(batch_size, len_q, len_k)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;每个Encoder层由两部分组成.</span></span><br><span class="line"><span class="string">      1. multi-head self-attention.</span></span><br><span class="line"><span class="string">      2. position-wise fully connected feed-forward network.</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_of_heads, dim_of_model, d_ff, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">      <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line">      self.mul_attn = MultiHeadAttention(num_of_heads, dim_of_model, dropout)</span><br><span class="line">      self.pos_ffn = PositionwiseFeedForward(dim_of_model, d_ff)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, query, padding_mask=<span class="literal">None</span></span>):</span></span><br><span class="line">      output, attention_weights = self.mul_attn.forward(</span><br><span class="line">          query, query, query, mask=padding_mask)</span><br><span class="line"></span><br><span class="line">      output = self.pos_ffn(output)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">return</span> output, attention_weights</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;Transformer有多个Encoder串联.</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, num_of_layers=<span class="number">6</span>, num_of_heads=<span class="number">8</span>, dim_of_model=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">               d_ff=<span class="number">512</span>, dropout=<span class="number">0.1</span>, max_length=<span class="number">2000</span></span>):</span></span><br><span class="line">      <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line"></span><br><span class="line">      <span class="comment"># vocab_size 词典大小</span></span><br><span class="line">      <span class="comment"># Encoder个数</span></span><br><span class="line">      self.n_layers = num_of_layers</span><br><span class="line">      <span class="comment"># Multi-head数</span></span><br><span class="line">      self.num_of_heads = num_of_heads</span><br><span class="line">      <span class="comment"># 模型维度</span></span><br><span class="line">      self.dim_of_model = dim_of_model</span><br><span class="line">      <span class="comment"># 仿射变换输出维度</span></span><br><span class="line">      self.d_ff = d_ff</span><br><span class="line">      self.dropout_rate = dropout</span><br><span class="line">      <span class="comment"># 序列最大长度</span></span><br><span class="line">      self.max_length = max_length</span><br><span class="line"></span><br><span class="line">      <span class="comment"># 输入的Embedding层</span></span><br><span class="line">      self.input_emb = nn.Embedding(vocab_size, dim_of_model)</span><br><span class="line">      <span class="comment"># 输入的词序embedding层</span></span><br><span class="line">      self.pos_emb = PositionalEncoding(dim_of_model, max_len=max_length)</span><br><span class="line">      self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Encoder层</span></span><br><span class="line">      self.layers = nn.ModuleList([</span><br><span class="line">          EncoderLayer(num_of_heads, dim_of_model, d_ff, dropout=dropout)</span><br><span class="line">          <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_of_layers)])</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, padded_input</span>):</span></span><br><span class="line">      enc_slf_attn_list = []</span><br><span class="line"></span><br><span class="line">      <span class="comment"># 词向量Embedding</span></span><br><span class="line">      enc_outputs = self.input_emb(padded_input)</span><br><span class="line">      <span class="comment"># 词向量Embedding + 词序Embedding</span></span><br><span class="line">      enc_outputs += self.pos_emb(enc_outputs)</span><br><span class="line">      enc_output = self.dropout(enc_outputs)</span><br><span class="line"></span><br><span class="line">      slf_attn_mask = get_attn_pad_mask(padded_input, padded_input)</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Encoder层的多级串联</span></span><br><span class="line">      <span class="keyword">for</span> enc_layer <span class="keyword">in</span> self.layers:</span><br><span class="line">          enc_output, enc_slf_attn = enc_layer(enc_output, slf_attn_mask)</span><br><span class="line">          enc_slf_attn_list += [enc_slf_attn]</span><br><span class="line"></span><br><span class="line">      <span class="keyword">return</span> enc_output, enc_slf_attn_list</span><br><span class="line"></span><br><span class="line">sample_encoder = Encoder(<span class="number">200</span>)</span><br><span class="line">q = torch.randint(<span class="number">1</span>, <span class="number">10</span>, (<span class="number">64</span>, <span class="number">62</span>))</span><br><span class="line">sample_encoder_output, enc_slf_attn_list = sample_encoder.forward(q)</span><br><span class="line"><span class="built_in">print</span>(sample_encoder_output.shape)</span><br></pre></td></tr></table></figure><p></p></li><li>Decoder Layer &amp; Decoder<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_subsequent_mask</span>(<span class="params">seq</span>):</span></span><br><span class="line">  sz_b, len_s = seq.size()</span><br><span class="line">  subsequent_mask = torch.triu(</span><br><span class="line">      torch.ones((len_s, len_s), device=seq.device, dtype=torch.uint8), diagonal=<span class="number">1</span>)</span><br><span class="line">  subsequent_mask = subsequent_mask.unsqueeze(<span class="number">0</span>).expand(sz_b, -<span class="number">1</span>, -<span class="number">1</span>)  <span class="comment"># b x ls x ls</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> subsequent_mask</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;每个Decoder层由两部分组成.</span></span><br><span class="line"><span class="string">      1. multi-head self-attention.</span></span><br><span class="line"><span class="string">      2. encoder-decoder multi-head self-attention.</span></span><br><span class="line"><span class="string">      3. position-wise fully connected feed-forward network.</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_of_heads, dim_of_model, d_ff, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">      <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">      self.mul_attn = MultiHeadAttention(num_of_heads, dim_of_model, dropout)</span><br><span class="line">      self.enc_attn = MultiHeadAttention(num_of_heads, dim_of_model, dropout)</span><br><span class="line">      self.pos_ffn = PositionwiseFeedForward(dim_of_model, d_ff)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, dec_input, enc_output, dec_self_attn_mask=<span class="literal">None</span>, dec_enc_attn_mask=<span class="literal">None</span></span>):</span></span><br><span class="line">      dec_output, dec_slf_attn = self.mul_attn(</span><br><span class="line">          dec_input, dec_input, dec_input, mask=dec_self_attn_mask)</span><br><span class="line"></span><br><span class="line">      dec_output, dec_enc_attn = self.enc_attn(</span><br><span class="line">          dec_output, enc_output, enc_output, mask=dec_enc_attn_mask)</span><br><span class="line"></span><br><span class="line">      dec_output = self.pos_ffn(dec_output)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">return</span> dec_output, dec_slf_attn, dec_enc_attn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Transformer有多个Decoder串联.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, tgt_vocab_size, num_of_layers=<span class="number">6</span>, num_of_heads=<span class="number">8</span>, dim_of_model=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 d_ff=<span class="number">512</span>, dropout=<span class="number">0.1</span>, max_length=<span class="number">2000</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># vocab_size 词典大小</span></span><br><span class="line">        <span class="comment"># Encoder个数</span></span><br><span class="line">        self.n_layers = num_of_layers</span><br><span class="line">        <span class="comment"># Multi-head数</span></span><br><span class="line">        self.num_of_heads = num_of_heads</span><br><span class="line">        <span class="comment"># 模型维度</span></span><br><span class="line">        self.dim_of_model = dim_of_model</span><br><span class="line">        <span class="comment"># 仿射变换输出维度</span></span><br><span class="line">        self.d_ff = d_ff</span><br><span class="line">        self.dropout_rate = dropout</span><br><span class="line">        <span class="comment"># 序列最大长度</span></span><br><span class="line">        self.max_length = max_length</span><br><span class="line"></span><br><span class="line">        self.tgt_emb = nn.Embedding(tgt_vocab_size, dim_of_model)</span><br><span class="line">        self.positional_encoding = PositionalEncoding(dim_of_model, max_len=max_length)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Decoder层串行级联</span></span><br><span class="line">        self.layers = nn.ModuleList([</span><br><span class="line">            DecoderLayer(num_of_heads, dim_of_model, d_ff, dropout=dropout)</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_of_layers)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 映射目标词维度空间到词典表维度空间</span></span><br><span class="line">        self.tgt_word_prj = nn.Linear(dim_of_model, tgt_vocab_size, bias=<span class="literal">False</span>)</span><br><span class="line">        nn.init.xavier_normal_(self.tgt_word_prj.weight)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, padded_input, encoder_padded_outputs, dec_enc_attn_mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># multi-head self-attention 和 encoder-decoder multi-head self-attention的attention信息</span></span><br><span class="line">        dec_slf_attn_list, dec_enc_attn_list = [], []</span><br><span class="line"></span><br><span class="line">        dec_self_attn_subsequence_mask = get_subsequent_mask(padded_input)</span><br><span class="line">        dec_self_attn_pad_mask = get_attn_pad_mask(padded_input,</span><br><span class="line">                                                   padded_input)</span><br><span class="line">        dec_self_attn_mask = (dec_self_attn_pad_mask + dec_self_attn_subsequence_mask).gt(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        dec_output = self.dropout(self.tgt_emb(padded_input) +</span><br><span class="line">                                  self.positional_encoding(padded_input))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 多个Decoder层串行执行.</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            dec_output, dec_slf_attn, dec_enc_attn = layer(</span><br><span class="line">                dec_output, encoder_padded_outputs,</span><br><span class="line">                dec_self_attn_mask=dec_self_attn_mask,</span><br><span class="line">                dec_enc_attn_mask=dec_enc_attn_mask)</span><br><span class="line"></span><br><span class="line">            dec_slf_attn_list += [dec_slf_attn]</span><br><span class="line">            dec_enc_attn_list += [dec_enc_attn]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 把目标词维度映射到词典维度</span></span><br><span class="line">        seq_logit = self.tgt_word_prj(dec_output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> seq_logit, dec_slf_attn_list, dec_enc_attn_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sample_decoder = Decoder(tgt_vocab_size=<span class="number">8000</span>)</span><br><span class="line">q = torch.randint(<span class="number">1</span>, <span class="number">10</span>, (<span class="number">64</span>, <span class="number">26</span>))</span><br><span class="line"></span><br><span class="line">output, dec_slf_attn_list, dec_enc_attn_list = sample_decoder.forward(padded_input=q,</span><br><span class="line">                                                                      encoder_padded_outputs=sample_encoder_output)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br></pre></td></tr></table></figure></li><li><p>Transformer</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Transformer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Transformer构建.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size=<span class="number">8000</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Transformer, self).__init__()</span><br><span class="line">        self.encoder = Encoder(vocab_size)</span><br><span class="line">        self.decoder = Decoder(vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, padded_input, padded_target</span>):</span></span><br><span class="line"></span><br><span class="line">        encoder_padded_outputs, enc_slf_attn_list = self.encoder(padded_input)</span><br><span class="line"></span><br><span class="line">        pred, dec_slf_attn_list, dec_enc_attn_list = self.decoder(padded_target, encoder_padded_outputs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> pred, enc_slf_attn_list, dec_slf_attn_list, dec_enc_attn_list</span><br><span class="line"></span><br><span class="line">model = Transformer()</span><br><span class="line">q = torch.randint(<span class="number">1</span>, <span class="number">10</span>, (<span class="number">64</span>, <span class="number">26</span>))</span><br><span class="line">t = torch.randint(<span class="number">1</span>, <span class="number">10</span>, (<span class="number">64</span>, <span class="number">26</span>))</span><br><span class="line">pred, *_ = model.forward(q, t)</span><br><span class="line"><span class="built_in">print</span>(pred.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p></p></li></ul></ceneter></div><div class="popular-posts-header">相关文章推荐</div><ul class="popular-posts"><li class="popular-posts-item"><div class="popular-posts-title"><a href="/article/c0a9d987.html" rel="bookmark">机器学习与人工智能技术分享-第十章-Vision Transformers (ViT)</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/article/d677b2e0.html" rel="bookmark">机器学习与人工智能技术分享-第三章 机器学习中的统一框架</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/article/ad2261a2.html" rel="bookmark">机器学习与人工智能技术分享-第四章 最优化原理</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/article/b12a240.html" rel="bookmark">机器学习与人工智能技术分享-第九章 语义分割</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"><a href="/article/fb9cd06d.html" rel="bookmark">机器学习与人工智能技术分享-第七章 金融风控</a></div></li></ul><footer class="post-footer"><div class="post-tags"><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a> <a href="/tags/transformer/" rel="tag"># transformer</a> <a href="/tags/attention/" rel="tag"># attention</a> <a href="/tags/RNN/" rel="tag"># RNN</a> <a href="/tags/LSTM/" rel="tag"># LSTM</a> <a href="/tags/%E7%AC%AC%E5%85%AD%E7%AB%A0/" rel="tag"># 第六章</a></div><div class="post-nav"><div class="post-nav-item"><a href="/article/783e74f9.html" rel="prev" title="机器学习与人工智能技术分享-第五章 深度神经网络"><i class="fa fa-chevron-left"></i> 机器学习与人工智能技术分享-第五章 深度神经网络</a></div><div class="post-nav-item"><a href="/article/fb9cd06d.html" rel="next" title="机器学习与人工智能技术分享-第七章 金融风控">机器学习与人工智能技术分享-第七章 金融风控 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener("tabs:register",()=>{let e=CONFIG.comments["activeClass"];if(CONFIG.comments.storage&&(e=localStorage.getItem("comments_active")||e),e){let t=document.querySelector(`a[href="#comment-${e}"]`);t&&t.click()}}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{t.target.matches(".tabs-comment .tab-content .tab-pane")&&(t=t.target.classList[1],localStorage.setItem("comments_active",t))})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8Etransformers"><span class="nav-text">6. 循环神经网络与Transformers</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#rnn"><span class="nav-text">6.1 RNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86"><span class="nav-text">6.1.1 基本原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bptt-%E5%8E%9F%E7%90%86"><span class="nav-text">6.1.2 BPTT 原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5"><span class="nav-text">6.1.3 代码实践</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B7%E6%B2%8C%E7%90%86%E8%AE%BA"><span class="nav-text">6.2 混沌理论</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E7%BB%B4%E6%98%A0%E5%B0%84"><span class="nav-text">6.2.1 一维映射</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%8C%E7%BB%B4%E6%98%A0%E5%B0%84"><span class="nav-text">6.2.2 二维映射</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E6%98%A0%E5%B0%84"><span class="nav-text">6.2.3 线性映射</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%98%A0%E5%B0%84"><span class="nav-text">6.2.4 非线性映射</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B7%B7%E6%B2%8C%E7%9A%84%E6%BC%94%E5%8C%96%E5%8F%8A%E7%BB%93%E6%9E%84"><span class="nav-text">6.2.5 混沌的演化及结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rnn%E9%95%BF%E4%BE%9D%E8%B5%96%E5%AD%A6%E4%B9%A0%E9%97%AE%E9%A2%98"><span class="nav-text">6.2.6 RNN长依赖学习问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lstm"><span class="nav-text">6.3 LSTM</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86-1"><span class="nav-text">6.3.1 基本原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5-1"><span class="nav-text">6.3.2 代码实践</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#attention%E6%9C%BA%E5%88%B6"><span class="nav-text">6.4 Attention机制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFattention%E6%9C%BA%E5%88%B6"><span class="nav-text">6.4.1 什么是Attention机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%8Eseq2seq%E6%A8%A1%E5%9E%8B%E8%AF%B4%E8%B5%B7"><span class="nav-text">6.4.2 从Seq2Seq模型说起</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#attention%E6%9C%BA%E5%88%B6%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84"><span class="nav-text">6.4.3 Attention机制的基本结构</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#transformer"><span class="nav-text">6.5. Transformer</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="张磊" src="https://vivounicorn.github.io/images/wali.png"><p class="site-author-name" itemprop="name">张磊</p><div class="site-description" itemprop="description"></div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">14</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">2</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">44</span> <span class="site-state-item-name">标签</span></a></div></nav></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2021</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">张磊</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span title="站点总字数">477k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span title="站点阅读时长">7:13</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动</div></div></footer></div><script src="//cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js"></script><script src="//cdn.bootcdn.net/ajax/libs/jquery/3.6.0/jquery.min.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script>!function(){var e,t,o,n,r=document.getElementsByTagName("link");if(0<r.length)for(i=0;i<r.length;i++)"canonical"==r[i].rel.toLowerCase()&&r[i].href&&(e=r[i].href);t=(e||window.location.protocol).split(":")[0],e=e||window.location.href,window,o=e,n=document.referrer,/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi.test(o)||(t="https"===String(t).toLowerCase()?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif",n?(t+="?r="+encodeURIComponent(document.referrer),o&&(t+="&l="+o)):o&&(t+="?l="+o),(new Image).src=t)}()</script><script src="/js/local-search.js"></script><script>"undefined"==typeof MathJax?(window.MathJax={loader:{load:["[tex]/mhchem"],source:{"[tex]/amsCd":"[tex]/amscd","[tex]/AMScd":"[tex]/amscd"}},tex:{inlineMath:{"[+]":[["$","$"]]},packages:{"[+]":["mhchem"]},tags:"ams"},options:{renderActions:{findScript:[10,d=>{document.querySelectorAll('script[type^="math/tex"]').forEach(e=>{var t=!!e.type.match(/; *mode=display/);const a=new d.options.MathItem(e.textContent,d.inputJax[0],t);t=document.createTextNode("");e.parentNode.replaceChild(t,e),a.start={node:t,delim:"",n:0},a.end={node:t,delim:"",n:0},d.math.push(a)})},"",!1],insertedScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(e=>{let t=e.parentNode;"li"===t.nodeName.toLowerCase()&&t.parentNode.classList.add("has-jax")})},"",!1]}}},function(){var e=document.createElement("script");e.src="//cdnjs.cloudflare.com/ajax/libs/mathjax/3.0.5/es5/tex-mml-chtml.js",e.defer=!0,document.head.appendChild(e)}()):(MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset())</script><script>NexT.utils.loadComments(document.querySelector("#valine-comments"),()=>{NexT.utils.getScript("//cdnjs.cloudflare.com/ajax/libs/valine/1.3.10/Valine.min.js",()=>{var i=["nick","mail","link"],e="nick,mail,link".split(",").filter(e=>i.includes(e));new Valine({el:"#valine-comments",verify:!1,notify:!0,appId:"m8FPP0CqMpxyTuUvaVOX9qVV-gzGzoHsz",appKey:"Ori6X9PXqQyURvwgl7HT5TJj",placeholder:"赠人玫瑰，手有余香",avatar:"mm",meta:e,pageSize:"10",visitor:!0,lang:"zh-cn",path:location.pathname,recordIP:!1,serverURLs:""})},window.Valine)})</script></body></html>